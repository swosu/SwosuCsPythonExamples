{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Sort Lab üß†üß∞\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Implement merge sort (recursive) and the merge step.\n",
    "2. Count *comparisons* made during sorting.\n",
    "3. Experimentally confirm the book‚Äôs big claim: merge sort uses about **O(n log n)** comparisons.\n",
    "\n",
    "### What the book gives us\n",
    "- A recursive picture of merge sort splitting and merging a list (see the diagram on page 3).\n",
    "- **Algorithm 9**: recursive merge sort (page 4).\n",
    "- **Algorithm 10**: merging two sorted lists (page 5).\n",
    "- A key bound: merge sort comparisons are **O(n log n)** (page 6).\n",
    "\n",
    "We'll turn those ideas into runnable code and evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04d75f",
   "metadata": {},
   "source": [
    "## Quick intuition\n",
    "Merge sort does two repeated actions:\n",
    "\n",
    "### 1) Split\n",
    "Keep splitting the list into halves until each piece has size 1.\n",
    "\n",
    "### 2) Merge\n",
    "Merge sorted halves back together.\n",
    "The merge step is where comparisons happen.\n",
    "\n",
    "A key fact from the text:\n",
    "If you merge two sorted lists with lengths `m` and `n`,\n",
    "the merge needs at most **m + n ‚àí 1** comparisons.\n",
    "(That‚Äôs the engine behind the O(n log n) result.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class SortStats:\n",
    "    comparisons: int = 0\n",
    "    writes: int = 0   # how many items we append into merged output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02445bde",
   "metadata": {},
   "source": [
    "## Tracking ‚Äúwhat happened‚Äù during sorting (Stats object)\n",
    "\n",
    "This cell defines a tiny data container called `SortStats`. It‚Äôs not part of merge sort *itself*, it‚Äôs a **scoreboard** we carry along while the algorithm runs.\n",
    "\n",
    "### What `@dataclass` does for us\n",
    "`@dataclass` tells Python: ‚ÄúThis is mostly just data.‚Äù\n",
    "So Python automatically creates helpful boilerplate like:\n",
    "- an `__init__` method (so we can do `SortStats()` easily),\n",
    "- a nice printable representation (so we can see the values cleanly),\n",
    "- and consistent field handling.\n",
    "\n",
    "That means we can focus on the *algorithm*, not on writing boring setup code.\n",
    "\n",
    "### What the fields mean\n",
    "- `comparisons`: how many times we compared two items (like `left[i] <= right[j]`)\n",
    "- `writes`: how many items we placed into the merged output list\n",
    "\n",
    "These numbers give us a concrete way to test ideas from the book, like:\n",
    "- ‚ÄúMerging two sorted lists of sizes m and n takes at most m + n ‚àí 1 comparisons.‚Äù\n",
    "- ‚ÄúMerge sort has about O(n log n) comparisons overall.‚Äù\n",
    "\n",
    "### Why the ‚Äúbottom half‚Äù matters (the `= 0` defaults)\n",
    "The lines:\n",
    "```python\n",
    "comparisons: int = 0\n",
    "writes: int = 0\n",
    "are default starting values.\n",
    "\n",
    "Without them, Python would require you to provide values every time:\n",
    "\n",
    "SortStats(comparisons=0, writes=0)  # annoying and easy to forget\n",
    "With defaults, you can just do:\n",
    "\n",
    "stats = SortStats()\n",
    "That‚Äôs the whole point: we want a fresh, empty scoreboard every time we run the sort,\n",
    "so our measurements start at zero and the results are trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c221238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sorted_lists(left, right, stats: SortStats):\n",
    "    \"\"\"\n",
    "    Merge two already-sorted lists into one sorted list.\n",
    "\n",
    "    We count:\n",
    "      - stats.comparisons: element-to-element comparisons (left[i] <= right[j])\n",
    "      - stats.writes: how many values we write into the output list\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    # While both lists still have items left to compare...\n",
    "    while i < len(left) and j < len(right):\n",
    "        # This is the \"real\" comparison merge sort analysis cares about:\n",
    "        stats.comparisons += 1\n",
    "\n",
    "        if left[i] <= right[j]:\n",
    "            merged.append(left[i])\n",
    "            stats.writes += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            stats.writes += 1\n",
    "            j += 1\n",
    "\n",
    "    # One of the lists ran out. Copy the leftovers from the other list.\n",
    "    # No more element-to-element comparisons are needed here.\n",
    "    if i < len(left):\n",
    "        merged.extend(left[i:])\n",
    "        stats.writes += (len(left) - i)\n",
    "\n",
    "    if j < len(right):\n",
    "        merged.extend(right[j:])\n",
    "        stats.writes += (len(right) - j)\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af85539",
   "metadata": {},
   "source": [
    "## Why we only add 1 comparison inside the loop\n",
    "\n",
    "The book‚Äôs claim about merging two lists (like ‚Äúat most m + n ‚àí 1 comparisons‚Äù) is counting\n",
    "**comparisons between the actual data values**, not bookkeeping checks.\n",
    "\n",
    "In our code, the only place we compare *values* from the lists is here:\n",
    "\n",
    "```python\n",
    "if left[i] <= right[j]:\n",
    "So that‚Äôs what stats.comparisons measures.\n",
    "```\n",
    "\n",
    "The while i < len(left) and j < len(right) line does evaluate two conditions,\n",
    "but those are index/bounds checks, not comparisons of elements.\n",
    "They matter for runtime in real machines, but they are not what the classic merge sort math is tracking.\n",
    "\n",
    "\n",
    "```markdown\n",
    "## Step-by-step: what the merge function does\n",
    "\n",
    "We are given two lists:\n",
    "- `left` is already sorted\n",
    "- `right` is already sorted\n",
    "\n",
    "Our goal: build `merged`, a new sorted list containing everything from both.\n",
    "\n",
    "### 1) Start with empty output and two pointers\n",
    "- `merged = []` will hold the final sorted result\n",
    "- `i` points into `left`\n",
    "- `j` points into `right`\n",
    "\n",
    "### 2) Repeatedly pick the smaller ‚Äúfront‚Äù element\n",
    "While both lists still have unprocessed items:\n",
    "1. Compare the current front items: `left[i]` and `right[j]`\n",
    "2. Whichever is smaller gets appended into `merged`\n",
    "3. Move that list‚Äôs pointer forward (`i += 1` or `j += 1`)\n",
    "4. Each append is a ‚Äúwrite‚Äù into the output list\n",
    "5. Each element-to-element decision uses exactly **one** comparison\n",
    "\n",
    "### 3) Copy leftovers after one list runs out\n",
    "Eventually, either:\n",
    "- `i == len(left)` (we used all of `left`), or\n",
    "- `j == len(right)` (we used all of `right`)\n",
    "\n",
    "At that moment, the remaining items in the other list are already sorted,\n",
    "and they are all larger than everything we already placed into `merged`.\n",
    "\n",
    "So we can copy them straight in with `extend(...)`:\n",
    "- no more element comparisons needed\n",
    "- but we still count the writes because we are still adding items to `merged`\n",
    "\n",
    "### 4) Return the merged sorted list\n",
    "`merged` is sorted and contains all items from both inputs.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f3f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr, stats: SortStats):\n",
    "    \"\"\"\n",
    "    Recursive merge sort.\n",
    "    Splits list, sorts halves recursively, then merges.\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    mid = len(arr) // 2\n",
    "    left_sorted = merge_sort(arr[:mid], stats)\n",
    "    right_sorted = merge_sort(arr[mid:], stats)\n",
    "\n",
    "    return merge_sorted_lists(left_sorted, right_sorted, stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72807b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], SortStats(comparisons=22, writes=34))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [8, 2, 4, 6, 9, 7, 10, 1, 5, 3]  # similar to the page-3 example list\n",
    "stats = SortStats()\n",
    "sorted_data = merge_sort(data, stats)\n",
    "\n",
    "sorted_data, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c432ec0",
   "metadata": {},
   "source": [
    "## What should we expect?\n",
    "If `n = 10`, merge sort splits into halves until size 1, then merges back.\n",
    "\n",
    "The book‚Äôs storyline:\n",
    "- merge step comparisons are bounded (‚â§ m + n ‚àí 1)\n",
    "- total merges happen across about log2(n) \"levels\"\n",
    "- so total comparisons scale like **n log2(n)**\n",
    "\n",
    "Next: we‚Äôll measure comparisons for different `n` and compare to `n log2(n)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701803a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {'avg_comparisons': 15.675,\n",
       "  'min_comparisons': 13,\n",
       "  'max_comparisons': 17,\n",
       "  'n_log2_n': 24.0},\n",
       " 16: {'avg_comparisons': 45.3,\n",
       "  'min_comparisons': 39,\n",
       "  'max_comparisons': 48,\n",
       "  'n_log2_n': 64.0},\n",
       " 32: {'avg_comparisons': 121.075,\n",
       "  'min_comparisons': 115,\n",
       "  'max_comparisons': 129,\n",
       "  'n_log2_n': 160.0},\n",
       " 64: {'avg_comparisons': 304.875,\n",
       "  'min_comparisons': 294,\n",
       "  'max_comparisons': 314,\n",
       "  'n_log2_n': 384.0},\n",
       " 128: {'avg_comparisons': 738.375,\n",
       "  'min_comparisons': 724,\n",
       "  'max_comparisons': 749,\n",
       "  'n_log2_n': 896.0},\n",
       " 256: {'avg_comparisons': 1728.075,\n",
       "  'min_comparisons': 1703,\n",
       "  'max_comparisons': 1744,\n",
       "  'n_log2_n': 2048.0},\n",
       " 512: {'avg_comparisons': 3957.075,\n",
       "  'min_comparisons': 3931,\n",
       "  'max_comparisons': 3990,\n",
       "  'n_log2_n': 4608.0},\n",
       " 1024: {'avg_comparisons': 8952.8,\n",
       "  'min_comparisons': 8914,\n",
       "  'max_comparisons': 8982,\n",
       "  'n_log2_n': 10240.0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_one_trial(n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    arr = [random.randint(0, 10**9) for _ in range(n)]\n",
    "    stats = SortStats()\n",
    "    out = merge_sort(arr, stats)\n",
    "    assert out == sorted(arr), \"Sort failed!\"\n",
    "    return stats.comparisons\n",
    "\n",
    "def run_experiment(ns, trials=30):\n",
    "    results = {}\n",
    "    for n in ns:\n",
    "        comps = [run_one_trial(n) for _ in range(trials)]\n",
    "        results[n] = {\n",
    "            \"avg_comparisons\": sum(comps) / len(comps),\n",
    "            \"min_comparisons\": min(comps),\n",
    "            \"max_comparisons\": max(comps),\n",
    "            \"n_log2_n\": n * math.log2(n) if n > 1 else 0\n",
    "        }\n",
    "    return results\n",
    "\n",
    "ns = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "results = run_experiment(ns, trials=40)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc68796",
   "metadata": {},
   "source": [
    "## Reading the results\n",
    "We computed:\n",
    "- average comparisons actually used\n",
    "- the value `n log2(n)` as a reference scale\n",
    "\n",
    "We do **not** expect comparisons to equal `n log2(n)` exactly.\n",
    "We *do* expect the comparisons to grow proportionally to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16c6f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     n |    avg comps |     n log2 n | avg/(n log2 n)\n",
      "-------------------------------------------------------\n",
      "     8 |        15.68 |        24.00 |         0.6531\n",
      "    16 |        45.30 |        64.00 |         0.7078\n",
      "    32 |       121.08 |       160.00 |         0.7567\n",
      "    64 |       304.88 |       384.00 |         0.7939\n",
      "   128 |       738.38 |       896.00 |         0.8241\n",
      "   256 |      1728.08 |      2048.00 |         0.8438\n",
      "   512 |      3957.07 |      4608.00 |         0.8587\n",
      "  1024 |      8952.80 |     10240.00 |         0.8743\n"
     ]
    }
   ],
   "source": [
    "def print_table(results):\n",
    "    print(f\"{'n':>6} | {'avg comps':>12} | {'n log2 n':>12} | {'avg/(n log2 n)':>14}\")\n",
    "    print(\"-\" * 55)\n",
    "    for n in sorted(results.keys()):\n",
    "        avg_c = results[n][\"avg_comparisons\"]\n",
    "        ref = results[n][\"n_log2_n\"]\n",
    "        ratio = (avg_c / ref) if ref else float('nan')\n",
    "        print(f\"{n:>6} | {avg_c:>12.2f} | {ref:>12.2f} | {ratio:>14.4f}\")\n",
    "\n",
    "print_table(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995d537",
   "metadata": {},
   "source": [
    "### What does the ratio mean?\n",
    "If merge sort is O(n log n), then:\n",
    "\n",
    "avg_comparisons ‚âà C * (n log2 n)\n",
    "\n",
    "So the ratio avg_comparisons / (n log2 n) should hover around a constant C\n",
    "as n grows (it might wiggle a bit, but it shouldn‚Äôt explode).\n",
    "\n",
    "If the ratio grows without bound, we'd be in trouble.\n",
    "If it stabilizes, that‚Äôs empirical evidence for O(n log n).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d713fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=  2000  ours=0.002565s  builtin=0.000116s  speedup=22.15x\n",
      "n=  4000  ours=0.005627s  builtin=0.000271s  speedup=20.80x\n",
      "n=  8000  ours=0.012242s  builtin=0.000590s  speedup=20.74x\n",
      "n= 16000  ours=0.026073s  builtin=0.001295s  speedup=20.13x\n",
      "n= 32000  ours=0.057087s  builtin=0.002820s  speedup=20.24x\n",
      "n= 64000  ours=0.121247s  builtin=0.006296s  speedup=19.26x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'n': 2000,\n",
       "  'our_best_avg_s': 0.0025646599009633065,\n",
       "  'builtin_best_avg_s': 0.00011579999700188637,\n",
       "  'speedup_builtin_over_ours': 22.147322688803943},\n",
       " {'n': 4000,\n",
       "  'our_best_avg_s': 0.005627410067245364,\n",
       "  'builtin_best_avg_s': 0.00027058999985456466,\n",
       "  'speedup_builtin_over_ours': 20.796814628293564},\n",
       " {'n': 8000,\n",
       "  'our_best_avg_s': 0.012242049910128117,\n",
       "  'builtin_best_avg_s': 0.0005903200712054968,\n",
       "  'speedup_builtin_over_ours': 20.737986911284484},\n",
       " {'n': 16000,\n",
       "  'our_best_avg_s': 0.026072539947927,\n",
       "  'builtin_best_avg_s': 0.0012951799668371677,\n",
       "  'speedup_builtin_over_ours': 20.130437943381875},\n",
       " {'n': 32000,\n",
       "  'our_best_avg_s': 0.05708746993914247,\n",
       "  'builtin_best_avg_s': 0.002820130018517375,\n",
       "  'speedup_builtin_over_ours': 20.2428503523944},\n",
       " {'n': 64000,\n",
       "  'our_best_avg_s': 0.12124685002490879,\n",
       "  'builtin_best_avg_s': 0.006296280026435852,\n",
       "  'speedup_builtin_over_ours': 19.25690241155669}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "def time_one(func, data, repeats=3):\n",
    "    \"\"\"\n",
    "    Time `func(data)` using a small number of repeats and return the best time.\n",
    "    We use best-of-N to reduce noise from background processes.\n",
    "    \"\"\"\n",
    "    best = float(\"inf\")\n",
    "    for _ in range(repeats):\n",
    "        start = time.perf_counter()\n",
    "        func(data)\n",
    "        end = time.perf_counter()\n",
    "        best = min(best, end - start)\n",
    "    return best\n",
    "\n",
    "def our_mergesort_wrapper(data):\n",
    "    # We copy the data so each algorithm gets the same input\n",
    "    stats = SortStats()\n",
    "    out = merge_sort(list(data), stats)\n",
    "    return out\n",
    "\n",
    "def builtin_sorted_wrapper(data):\n",
    "    return sorted(data)\n",
    "\n",
    "def benchmark_sorts(\n",
    "    sizes,\n",
    "    trials_per_size=8,\n",
    "    repeats_per_trial=2,\n",
    "    seed=12345\n",
    "):\n",
    "    \"\"\"\n",
    "    For each n in sizes:\n",
    "      - generate trials_per_size random lists of length n\n",
    "      - time merge_sort vs built-in sorted on the same lists\n",
    "      - return aggregated results\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    results = []  # list of dict rows\n",
    "    for n in sizes:\n",
    "        our_times = []\n",
    "        builtin_times = []\n",
    "\n",
    "        for _ in range(trials_per_size):\n",
    "            data = [random.randint(0, 10**9) for _ in range(n)]\n",
    "\n",
    "            # Warm correctness check (optional but nice)\n",
    "            out1 = our_mergesort_wrapper(data)\n",
    "            out2 = builtin_sorted_wrapper(data)\n",
    "            assert out1 == out2, \"Mismatch! Our merge sort result differs from built-in sorted().\"\n",
    "\n",
    "            # Timing\n",
    "            our_t = time_one(our_mergesort_wrapper, data, repeats=repeats_per_trial)\n",
    "            bi_t  = time_one(builtin_sorted_wrapper, data, repeats=repeats_per_trial)\n",
    "\n",
    "            our_times.append(our_t)\n",
    "            builtin_times.append(bi_t)\n",
    "\n",
    "        results.append({\n",
    "            \"n\": n,\n",
    "            \"our_best_avg_s\": sum(our_times) / len(our_times),\n",
    "            \"builtin_best_avg_s\": sum(builtin_times) / len(builtin_times),\n",
    "            \"speedup_builtin_over_ours\": (sum(our_times) / len(our_times)) / (sum(builtin_times) / len(builtin_times))\n",
    "        })\n",
    "\n",
    "        print(f\"n={n:>6}  ours={results[-1]['our_best_avg_s']:.6f}s  \"\n",
    "              f\"builtin={results[-1]['builtin_best_avg_s']:.6f}s  \"\n",
    "              f\"speedup={results[-1]['speedup_builtin_over_ours']:.2f}x\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Let out a little magic smoke: bigger sizes than before\n",
    "sizes = [2_000, 4_000, 8_000, 16_000, 32_000, 64_000]\n",
    "\n",
    "bench = benchmark_sorts(\n",
    "    sizes=sizes,\n",
    "    trials_per_size=10,       # more trials = smoother signal\n",
    "    repeats_per_trial=2,      # best-of-2 each trial\n",
    "    seed=2026\n",
    ")\n",
    "\n",
    "bench\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f079a8c",
   "metadata": {},
   "source": [
    "## Benchmarking: our merge sort vs Python‚Äôs built-in sort\n",
    "\n",
    "Now we‚Äôre doing a fair fight between two sorting approaches:\n",
    "\n",
    "### 1) Our merge sort (educational version)\n",
    "- Written in Python\n",
    "- Uses recursion and creates new lists during splitting/merging\n",
    "- Great for learning the algorithm and the math\n",
    "- Not optimized for raw speed\n",
    "\n",
    "### 2) Python‚Äôs built-in `sorted()` (industrial version)\n",
    "- Uses **Timsort** (a highly engineered hybrid sort)\n",
    "- Written in **C** under the hood (fast loops, low overhead)\n",
    "- Takes advantage of real-world patterns in data\n",
    "- This is what you actually want in production most of the time\n",
    "\n",
    "### How we benchmark fairly\n",
    "For each list size `n`:\n",
    "1. We generate the same random list of length `n`\n",
    "2. We sort it with both methods\n",
    "3. We confirm the results match (correctness check)\n",
    "4. We time each method multiple times and keep the best timing per trial\n",
    "   (this reduces noise from the OS doing random background stuff)\n",
    "\n",
    "### What we expect\n",
    "Both methods are **O(n log n)** in big-O terms, but:\n",
    "- `sorted()` should be dramatically faster because it‚Äôs optimized and implemented in C\n",
    "- Our Python merge sort will get slower faster due to Python-level overhead and extra allocations\n",
    "\n",
    "So: same asymptotic family, different ‚Äúconstant factors‚Äù‚Ä¶ and those constants can be *giant*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ns = [row[\"n\"] for row in bench]\n",
    "our = [row[\"our_best_avg_s\"] for row in bench]\n",
    "bi  = [row[\"builtin_best_avg_s\"] for row in bench]\n",
    "speedup = [row[\"speedup_builtin_over_ours\"] for row in bench]\n",
    "\n",
    "# 1) Runtime plot\n",
    "plt.figure()\n",
    "plt.plot(ns, our, marker=\"o\", label=\"Our merge_sort (Python)\")\n",
    "plt.plot(ns, bi, marker=\"o\", label=\"built-in sorted() (Timsort, C)\")\n",
    "plt.xlabel(\"List size (n)\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Runtime: Our Merge Sort vs Python built-in sorted()\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2) Speedup plot (how many times faster built-in is)\n",
    "plt.figure()\n",
    "plt.plot(ns, speedup, marker=\"o\")\n",
    "plt.xlabel(\"List size (n)\")\n",
    "plt.ylabel(\"Speedup factor (ours / built-in)\")\n",
    "plt.title(\"How many times faster is built-in sorted()?\")\n",
    "plt.show()\n",
    "\n",
    "# 3) Print a neat summary table\n",
    "print(f\"{'n':>8} | {'ours (s)':>12} | {'built-in (s)':>12} | {'speedup':>9}\")\n",
    "print(\"-\" * 53)\n",
    "for row in bench:\n",
    "    print(f\"{row['n']:>8} | {row['our_best_avg_s']:>12.6f} | {row['builtin_best_avg_s']:>12.6f} | {row['speedup_builtin_over_ours']:>9.2f}x\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
