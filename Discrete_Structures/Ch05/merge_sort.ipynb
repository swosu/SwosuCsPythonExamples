{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Sort Lab ðŸ§ ðŸ§°\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Implement merge sort (recursive) and the merge step.\n",
    "2. Count *comparisons* made during sorting.\n",
    "3. Experimentally confirm the bookâ€™s big claim: merge sort uses about **O(n log n)** comparisons.\n",
    "\n",
    "### What the book gives us\n",
    "- A recursive picture of merge sort splitting and merging a list (see the diagram on page 3).\n",
    "- **Algorithm 9**: recursive merge sort (page 4).\n",
    "- **Algorithm 10**: merging two sorted lists (page 5).\n",
    "- A key bound: merge sort comparisons are **O(n log n)** (page 6).\n",
    "\n",
    "We'll turn those ideas into runnable code and evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04d75f",
   "metadata": {},
   "source": [
    "## Quick intuition\n",
    "Merge sort does two repeated actions:\n",
    "\n",
    "### 1) Split\n",
    "Keep splitting the list into halves until each piece has size 1.\n",
    "\n",
    "### 2) Merge\n",
    "Merge sorted halves back together.\n",
    "The merge step is where comparisons happen.\n",
    "\n",
    "A key fact from the text:\n",
    "If you merge two sorted lists with lengths `m` and `n`,\n",
    "the merge needs at most **m + n âˆ’ 1** comparisons.\n",
    "(Thatâ€™s the engine behind the O(n log n) result.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evertj\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe\n",
      "3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class SortStats:\n",
    "    comparisons: int = 0\n",
    "    writes: int = 0   # how many items we append into merged output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "def merge_sorted_lists(left, right, stats: SortStats):\n",
    "    \"\"\"\n",
    "    Merge two already-sorted lists into one sorted list.\n",
    "    Counts comparisons and writes (append operations).\n",
    "\n",
    "    Mirrors the bookâ€™s \"merge two lists\" idea (Algorithm 10).\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = j = 0\n",
    "\n",
    "    while i < len(left) and j < len(right):\n",
    "        stats.comparisons += 1\n",
    "        if left[i] <= right[j]:\n",
    "            merged.append(left[i])\n",
    "            stats.writes += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            stats.writes += 1\n",
    "            j += 1\n",
    "\n",
    "    # Append leftovers (no comparisons needed here)\n",
    "    if i < len(left):\n",
    "        merged.extend(left[i:])\n",
    "        stats.writes += (len(left) - i)\n",
    "    if j < len(right):\n",
    "        merged.extend(right[j:])\n",
    "        stats.writes += (len(right) - j)\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr, stats: SortStats):\n",
    "    \"\"\"\n",
    "    Recursive merge sort.\n",
    "    Splits list, sorts halves recursively, then merges.\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    mid = len(arr) // 2\n",
    "    left_sorted = merge_sort(arr[:mid], stats)\n",
    "    right_sorted = merge_sort(arr[mid:], stats)\n",
    "\n",
    "    return merge_sorted_lists(left_sorted, right_sorted, stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72807b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [8, 2, 4, 6, 9, 7, 10, 1, 5, 3]  # similar to the page-3 example list\n",
    "stats = SortStats()\n",
    "sorted_data = merge_sort(data, stats)\n",
    "\n",
    "sorted_data, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c432ec0",
   "metadata": {},
   "source": [
    "## What should we expect?\n",
    "If `n = 10`, merge sort splits into halves until size 1, then merges back.\n",
    "\n",
    "The bookâ€™s storyline:\n",
    "- merge step comparisons are bounded (â‰¤ m + n âˆ’ 1)\n",
    "- total merges happen across about log2(n) \"levels\"\n",
    "- so total comparisons scale like **n log2(n)**\n",
    "\n",
    "Next: weâ€™ll measure comparisons for different `n` and compare to `n log2(n)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701803a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_trial(n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    arr = [random.randint(0, 10**9) for _ in range(n)]\n",
    "    stats = SortStats()\n",
    "    out = merge_sort(arr, stats)\n",
    "    assert out == sorted(arr), \"Sort failed!\"\n",
    "    return stats.comparisons\n",
    "\n",
    "def run_experiment(ns, trials=30):\n",
    "    results = {}\n",
    "    for n in ns:\n",
    "        comps = [run_one_trial(n) for _ in range(trials)]\n",
    "        results[n] = {\n",
    "            \"avg_comparisons\": sum(comps) / len(comps),\n",
    "            \"min_comparisons\": min(comps),\n",
    "            \"max_comparisons\": max(comps),\n",
    "            \"n_log2_n\": n * math.log2(n) if n > 1 else 0\n",
    "        }\n",
    "    return results\n",
    "\n",
    "ns = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "results = run_experiment(ns, trials=40)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc68796",
   "metadata": {},
   "source": [
    "## Reading the results\n",
    "We computed:\n",
    "- average comparisons actually used\n",
    "- the value `n log2(n)` as a reference scale\n",
    "\n",
    "We do **not** expect comparisons to equal `n log2(n)` exactly.\n",
    "We *do* expect the comparisons to grow proportionally to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(results):\n",
    "    print(f\"{'n':>6} | {'avg comps':>12} | {'n log2 n':>12} | {'avg/(n log2 n)':>14}\")\n",
    "    print(\"-\" * 55)\n",
    "    for n in sorted(results.keys()):\n",
    "        avg_c = results[n][\"avg_comparisons\"]\n",
    "        ref = results[n][\"n_log2_n\"]\n",
    "        ratio = (avg_c / ref) if ref else float('nan')\n",
    "        print(f\"{n:>6} | {avg_c:>12.2f} | {ref:>12.2f} | {ratio:>14.4f}\")\n",
    "\n",
    "print_table(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995d537",
   "metadata": {},
   "source": [
    "### What does the ratio mean?\n",
    "If merge sort is O(n log n), then:\n",
    "\n",
    "avg_comparisons â‰ˆ C * (n log2 n)\n",
    "\n",
    "So the ratio avg_comparisons / (n log2 n) should hover around a constant C\n",
    "as n grows (it might wiggle a bit, but it shouldnâ€™t explode).\n",
    "\n",
    "If the ratio grows without bound, we'd be in trouble.\n",
    "If it stabilizes, thatâ€™s empirical evidence for O(n log n).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
