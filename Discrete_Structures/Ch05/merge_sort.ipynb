{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Sort Lab üß†üß∞\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Implement merge sort (recursive) and the merge step.\n",
    "2. Count *comparisons* made during sorting.\n",
    "3. Experimentally confirm the book‚Äôs big claim: merge sort uses about **O(n log n)** comparisons.\n",
    "\n",
    "### What the book gives us\n",
    "- A recursive picture of merge sort splitting and merging a list (see the diagram on page 3).\n",
    "- **Algorithm 9**: recursive merge sort (page 4).\n",
    "- **Algorithm 10**: merging two sorted lists (page 5).\n",
    "- A key bound: merge sort comparisons are **O(n log n)** (page 6).\n",
    "\n",
    "We'll turn those ideas into runnable code and evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04d75f",
   "metadata": {},
   "source": [
    "## Quick intuition\n",
    "Merge sort does two repeated actions:\n",
    "\n",
    "### 1) Split\n",
    "Keep splitting the list into halves until each piece has size 1.\n",
    "\n",
    "### 2) Merge\n",
    "Merge sorted halves back together.\n",
    "The merge step is where comparisons happen.\n",
    "\n",
    "A key fact from the text:\n",
    "If you merge two sorted lists with lengths `m` and `n`,\n",
    "the merge needs at most **m + n ‚àí 1** comparisons.\n",
    "(That‚Äôs the engine behind the O(n log n) result.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class SortStats:\n",
    "    comparisons: int = 0\n",
    "    writes: int = 0   # how many items we append into merged output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02445bde",
   "metadata": {},
   "source": [
    "## Tracking ‚Äúwhat happened‚Äù during sorting (Stats object)\n",
    "\n",
    "This cell defines a tiny data container called `SortStats`. It‚Äôs not part of merge sort *itself*, it‚Äôs a **scoreboard** we carry along while the algorithm runs.\n",
    "\n",
    "### What `@dataclass` does for us\n",
    "`@dataclass` tells Python: ‚ÄúThis is mostly just data.‚Äù\n",
    "So Python automatically creates helpful boilerplate like:\n",
    "- an `__init__` method (so we can do `SortStats()` easily),\n",
    "- a nice printable representation (so we can see the values cleanly),\n",
    "- and consistent field handling.\n",
    "\n",
    "That means we can focus on the *algorithm*, not on writing boring setup code.\n",
    "\n",
    "### What the fields mean\n",
    "- `comparisons`: how many times we compared two items (like `left[i] <= right[j]`)\n",
    "- `writes`: how many items we placed into the merged output list\n",
    "\n",
    "These numbers give us a concrete way to test ideas from the book, like:\n",
    "- ‚ÄúMerging two sorted lists of sizes m and n takes at most m + n ‚àí 1 comparisons.‚Äù\n",
    "- ‚ÄúMerge sort has about O(n log n) comparisons overall.‚Äù\n",
    "\n",
    "### Why the ‚Äúbottom half‚Äù matters (the `= 0` defaults)\n",
    "The lines:\n",
    "```python\n",
    "comparisons: int = 0\n",
    "writes: int = 0\n",
    "are default starting values.\n",
    "\n",
    "Without them, Python would require you to provide values every time:\n",
    "\n",
    "SortStats(comparisons=0, writes=0)  # annoying and easy to forget\n",
    "With defaults, you can just do:\n",
    "\n",
    "stats = SortStats()\n",
    "That‚Äôs the whole point: we want a fresh, empty scoreboard every time we run the sort,\n",
    "so our measurements start at zero and the results are trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c221238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sorted_lists(left, right, stats: SortStats):\n",
    "    \"\"\"\n",
    "    Merge two already-sorted lists into one sorted list.\n",
    "\n",
    "    We count:\n",
    "      - stats.comparisons: element-to-element comparisons (left[i] <= right[j])\n",
    "      - stats.writes: how many values we write into the output list\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    # While both lists still have items left to compare...\n",
    "    while i < len(left) and j < len(right):\n",
    "        # This is the \"real\" comparison merge sort analysis cares about:\n",
    "        stats.comparisons += 1\n",
    "\n",
    "        if left[i] <= right[j]:\n",
    "            merged.append(left[i])\n",
    "            stats.writes += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            stats.writes += 1\n",
    "            j += 1\n",
    "\n",
    "    # One of the lists ran out. Copy the leftovers from the other list.\n",
    "    # No more element-to-element comparisons are needed here.\n",
    "    if i < len(left):\n",
    "        merged.extend(left[i:])\n",
    "        stats.writes += (len(left) - i)\n",
    "\n",
    "    if j < len(right):\n",
    "        merged.extend(right[j:])\n",
    "        stats.writes += (len(right) - j)\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af85539",
   "metadata": {},
   "source": [
    "## Why we only add 1 comparison inside the loop\n",
    "\n",
    "The book‚Äôs claim about merging two lists (like ‚Äúat most m + n ‚àí 1 comparisons‚Äù) is counting\n",
    "**comparisons between the actual data values**, not bookkeeping checks.\n",
    "\n",
    "In our code, the only place we compare *values* from the lists is here:\n",
    "\n",
    "```python\n",
    "if left[i] <= right[j]:\n",
    "So that‚Äôs what stats.comparisons measures.\n",
    "```\n",
    "\n",
    "The while i < len(left) and j < len(right) line does evaluate two conditions,\n",
    "but those are index/bounds checks, not comparisons of elements.\n",
    "They matter for runtime in real machines, but they are not what the classic merge sort math is tracking.\n",
    "\n",
    "\n",
    "```markdown\n",
    "## Step-by-step: what the merge function does\n",
    "\n",
    "We are given two lists:\n",
    "- `left` is already sorted\n",
    "- `right` is already sorted\n",
    "\n",
    "Our goal: build `merged`, a new sorted list containing everything from both.\n",
    "\n",
    "### 1) Start with empty output and two pointers\n",
    "- `merged = []` will hold the final sorted result\n",
    "- `i` points into `left`\n",
    "- `j` points into `right`\n",
    "\n",
    "### 2) Repeatedly pick the smaller ‚Äúfront‚Äù element\n",
    "While both lists still have unprocessed items:\n",
    "1. Compare the current front items: `left[i]` and `right[j]`\n",
    "2. Whichever is smaller gets appended into `merged`\n",
    "3. Move that list‚Äôs pointer forward (`i += 1` or `j += 1`)\n",
    "4. Each append is a ‚Äúwrite‚Äù into the output list\n",
    "5. Each element-to-element decision uses exactly **one** comparison\n",
    "\n",
    "### 3) Copy leftovers after one list runs out\n",
    "Eventually, either:\n",
    "- `i == len(left)` (we used all of `left`), or\n",
    "- `j == len(right)` (we used all of `right`)\n",
    "\n",
    "At that moment, the remaining items in the other list are already sorted,\n",
    "and they are all larger than everything we already placed into `merged`.\n",
    "\n",
    "So we can copy them straight in with `extend(...)`:\n",
    "- no more element comparisons needed\n",
    "- but we still count the writes because we are still adding items to `merged`\n",
    "\n",
    "### 4) Return the merged sorted list\n",
    "`merged` is sorted and contains all items from both inputs.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr, stats: SortStats):\n",
    "    \"\"\"\n",
    "    Recursive merge sort.\n",
    "    Splits list, sorts halves recursively, then merges.\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    mid = len(arr) // 2\n",
    "    left_sorted = merge_sort(arr[:mid], stats)\n",
    "    right_sorted = merge_sort(arr[mid:], stats)\n",
    "\n",
    "    return merge_sorted_lists(left_sorted, right_sorted, stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72807b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [8, 2, 4, 6, 9, 7, 10, 1, 5, 3]  # similar to the page-3 example list\n",
    "stats = SortStats()\n",
    "sorted_data = merge_sort(data, stats)\n",
    "\n",
    "sorted_data, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c432ec0",
   "metadata": {},
   "source": [
    "## What should we expect?\n",
    "If `n = 10`, merge sort splits into halves until size 1, then merges back.\n",
    "\n",
    "The book‚Äôs storyline:\n",
    "- merge step comparisons are bounded (‚â§ m + n ‚àí 1)\n",
    "- total merges happen across about log2(n) \"levels\"\n",
    "- so total comparisons scale like **n log2(n)**\n",
    "\n",
    "Next: we‚Äôll measure comparisons for different `n` and compare to `n log2(n)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701803a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_trial(n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    arr = [random.randint(0, 10**9) for _ in range(n)]\n",
    "    stats = SortStats()\n",
    "    out = merge_sort(arr, stats)\n",
    "    assert out == sorted(arr), \"Sort failed!\"\n",
    "    return stats.comparisons\n",
    "\n",
    "def run_experiment(ns, trials=30):\n",
    "    results = {}\n",
    "    for n in ns:\n",
    "        comps = [run_one_trial(n) for _ in range(trials)]\n",
    "        results[n] = {\n",
    "            \"avg_comparisons\": sum(comps) / len(comps),\n",
    "            \"min_comparisons\": min(comps),\n",
    "            \"max_comparisons\": max(comps),\n",
    "            \"n_log2_n\": n * math.log2(n) if n > 1 else 0\n",
    "        }\n",
    "    return results\n",
    "\n",
    "ns = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "results = run_experiment(ns, trials=40)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc68796",
   "metadata": {},
   "source": [
    "## Reading the results\n",
    "We computed:\n",
    "- average comparisons actually used\n",
    "- the value `n log2(n)` as a reference scale\n",
    "\n",
    "We do **not** expect comparisons to equal `n log2(n)` exactly.\n",
    "We *do* expect the comparisons to grow proportionally to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(results):\n",
    "    print(f\"{'n':>6} | {'avg comps':>12} | {'n log2 n':>12} | {'avg/(n log2 n)':>14}\")\n",
    "    print(\"-\" * 55)\n",
    "    for n in sorted(results.keys()):\n",
    "        avg_c = results[n][\"avg_comparisons\"]\n",
    "        ref = results[n][\"n_log2_n\"]\n",
    "        ratio = (avg_c / ref) if ref else float('nan')\n",
    "        print(f\"{n:>6} | {avg_c:>12.2f} | {ref:>12.2f} | {ratio:>14.4f}\")\n",
    "\n",
    "print_table(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995d537",
   "metadata": {},
   "source": [
    "### What does the ratio mean?\n",
    "If merge sort is O(n log n), then:\n",
    "\n",
    "avg_comparisons ‚âà C * (n log2 n)\n",
    "\n",
    "So the ratio avg_comparisons / (n log2 n) should hover around a constant C\n",
    "as n grows (it might wiggle a bit, but it shouldn‚Äôt explode).\n",
    "\n",
    "If the ratio grows without bound, we'd be in trouble.\n",
    "If it stabilizes, that‚Äôs empirical evidence for O(n log n).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
