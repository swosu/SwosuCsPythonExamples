"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2112606","AI Institute for Intelligent CyberInfrastructure with Computational Learning in the Environment (ICICLE)","OAC","AI Research Institutes, Information Technology Researc, Special Projects - CNS","11/01/2021","01/10/2024","Dhabaleswar Panda","OH","Ohio State University","Cooperative Agreement","Varun Chandola","10/31/2026","$20,099,998.00","Vipin Chaudhary, Raghu Machiraju, Beth Plale, Eric Fosler-Lussier","panda@cse.ohio-state.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","132Y00, 164000, 171400","075Z, 120Z, 7231, 8004, 9102","$0.00","Although the world is witness to the tremendous successes of Artificial Intelligence (AI) technologies in some domains, many domains have yet to reap the benefits of AI due to the lack of easily usable AI infrastructure. The NSF AI Institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment (ICICLE) will develop intelligent cyberinfrastructure with transparent and high-performance execution on diverse and heterogeneous environments. It will advance plug-and-play AI that is easy to use by scientists across a wide range of domains, promoting the democratization of AI. ICICLE brings together a multidisciplinary team of scientists and engineers, led by The Ohio State University in partnership with Case Western Reserve University, IC-FOODS, Indiana University, Iowa State University, Ohio Supercomputer Center, Rensselaer Polytechnic Institute, San Diego Supercomputer Center, Texas Advanced Computing Center, University of Utah, University of California-Davis, University of California-San Diego, University of Delaware, and University of Wisconsin-Madison. Initially, complex societal challenges in three use-inspired scientific domains will drive ICICLE?s research and workforce development agenda: Smart Foodsheds, Precision Agriculture, and Animal Ecology.  <br/><br/>ICICLE?s research and development includes: (i) Empowering plug-and-play AI by advancing five foundational areas: knowledge graphs, model commons, adaptive AI, federated learning, and conversational AI. (ii) Providing a robust cyberinfrastructure capable of propelling AI-driven science (CI4AI), solving the challenges arising from heterogeneity in applications, software, and hardware, and disseminating the CI4AI innovations to use-inspired science domains. (iii) Creating new AI techniques for the adaptation/optimization of various CI components (AI4CI), enabling a virtuous cycle to advance both AI and CI. (iv) Developing novel techniques to address cross-cutting issues including privacy, accountability, and data integrity for CI and AI; and (v) Providing a geographically distributed and heterogeneous system consisting of software, data, and applications, orchestrated by a common application programming interface and execution middleware. ICICLE?s advanced and integrated edge, cloud, and high-performance computing hardware and software CI components simplify the use of AI, making it easier to address new areas of inquiry. In this way, ICICLE focuses on research in AI, innovation through AI, and accelerates the application of AI. ICICLE is building a diverse STEM workforce through innovative approaches to education, training, and broadening participation in computing that ensure sustained measurable outcomes and impact on a national scale, along the pipeline from middle/high school students to practitioners. As a nexus of collaboration, ICICLE promotes technology transfer to industry and other stakeholders, as well as data sharing and coordination across other National Science Foundation AI Institutes and Federal agencies. As a national resource for research, development, technology transfer, workforce development, and education, ICICLE is creating a widely usable, smarter, more robust and diverse, resilient, and effective CI4AI and AI4CI ecosystem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322063","CC* Data Storage: Remote Instrumentation Science Environment for Intelligent Image Analytics","OAC","Campus Cyberinfrastructure","08/01/2023","06/28/2023","Prasad Calyam","MO","University of Missouri-Columbia","Standard Grant","Kevin Thompson","07/31/2025","$500,000.00","Kannappan Palaniappan, Matthew Maschmann, Teresa Lever, Filiz Bunyak Ersoy","calyamp@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","CSE","808000","","$0.00","This NSF CC* project acquires and manages an on-premises data storage system supporting multiple scientific application storage needs for material science, biomedical science and other research activities on campus. The platform also provides an experimental testbed for developing innovations in intelligent image analytics. Project activities serve scientific experiments on university campuses that frequently involve control of specialized instruments such as scanning electron microscopes, and image data collection from those instruments. The significance of the project activities is in the automated handling of remote instrumentation and image analytics to overcome the manual processes in current practice that requires notable effort and time, and could lead to inconsistencies or errors. <br/><br/>As a next-generation storage environment, the Remote Instrumentation Science Environment (RISE) developed in this project is a shared resource at the intra-campus level located at the campus datacenter and at the inter-campus level via a federated data sharing fabric. RISE ensures adherence to FAIR (findable, accessible, interoperable, and reusable) principles and equitable access.  The RISE data storage system configurations are driven by scientific use cases of image analytics pipelines in areas such as material biomedical science, plant science, and biochemistry. The RISE storage system development features flexible web services to perform automated image data collection/analysis guided by an intelligent agent and involves collaborations with campus information technology group and the Open Science Grid. Usability studies in this project advance knowledge in terms of understanding challenges in orchestration and maintenance of large on-premise storage systems for diverse image analytics applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1750936","CAREER: Scalable and Adaptable Cross-Domain Autonomous Health Assessment","CNS","Information Technology Researc, Special Projects - CNS, CSR-Computer Systems Research","05/01/2018","07/25/2022","Nirmalya Roy","MD","University of Maryland Baltimore County","Continuing Grant","Marilyn McClure","04/30/2025","$650,316.00","","nroy@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","CSE","164000, 171400, 735400","1045, 120Z","$0.00","The wide availability of commodity smart home sensor systems (Google Home, Amazon Echo, etc.) and internet-of-things (IoT) devices (Fitbit, Actigraph, etc.) is making it easier to continuously monitor individuals' health-related vital signals, activities, and behaviors to provide just-in-time health intervention to the aging population. This CAREER project seeks to design, implement, and evaluate heterogeneous sensor systems in smart homes that help ameliorate the progressive functional and behavioral health decline of older adults. The work specifically looks at cross-domain approaches that can accommodate variability in behavior, activity, and physiological health conditions across a large population and diverse set of smart home sensor systems. The inability to build scalable and adaptable activity and behavior monitoring models across domains such as multi-occupant homes with heterogeneous internet-of-things devices is a major impediment to adoption of smart home technologies for healthcare applications. The project develops novel deep transfer learning techniques, optimization-based heuristics, opportunistic sensing architecture, and spatiotemporal dynamical systems-based approaches to address the diversity, adaptability, and reliability of activity and behavior recognition models across different users and technologies, while leveraging a human-in-the-loop control for improving the performance of the sensor systems. These techniques will help automate activity and physiological health monitoring at scale, and thereby improve the design and study of adaptive interventions for elderly people, their families, and professional caregivers. <br/><br/>In order to realize autonomous health assessment methodologies in practice, it is necessary to build an activity and behavior recognition system across multiple inhabitants and various connected consumer devices that can select, adapt, and cope with device and user heterogeneities, privacy characteristics, resource constraints and scarcity of labeled data. To address the above-mentioned problems, this research project contributes to new methodology in four ways. First, it is introducing deep transfer learning activity recognition model and multi-user multi-device optimization-based heuristics that automatically help adapt the inherent variations across different domains, including user/device-type/device-instance. Second, it is designing a spatio-temporal dynamical system approach based on fractal dynamics to mitigate the variability in various sensor signals, and capture the self-similarity of human physiological health markers and establish the parametric task performance dependency between functional and behavioral health measurements. Third, it posits an opportunistic sensing architecture and human-in-the loop activity model for real-time data sharing and annotation that help optimize the user interruption and system performance. Fourth, it is designing a distributed implementation of tailored-computational techniques in actual smart home deployments, and evaluating the effectiveness of sensor-based functional and behavioral models and algorithms for just-in-time health assessment in actual living environments. In addition to the targeted focus on education, an ongoing collaboration with the University of Maryland, School of Nursing is being leveraged for real deployment of smart home sensor systems and technologies at three retirement community centers and senior homes in the greater Baltimore area to compound the impact of proposed evidence-based research efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117449","MRI: Acquisition of a High-Performance Computing Cluster for Science and Engineering Research at the University of Kansas","OAC","Major Research Instrumentation, Information Technology Researc, EPSCoR Co-Funding","10/01/2021","08/24/2023","Brian Laird","KS","University of Kansas Center for Research Inc","Standard Grant","Alejandro Suarez","09/30/2024","$687,060.00","Ward Thompson, Suzanne Shontz, Yinglong Miao","blaird@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457563","7858643441","CSE","118900, 164000, 915000","1189, 9102, 9150","$0.00","A High Performance Computing (HPC) cluster resource will be built at the University of Kansas (KU) that will advance computational science and engineering at KU and selected Primarily Undergraduate Institutions (PUIs). This cluster, nicknamed BigJay, will be part of the KU Community Cluster housed in the state-of-the-art Advanced Computing Facility and maintained by the KU Center for Research Computing (CRC). The major users form a multidisciplinary group from 11 different academic departments/programs at KU: Aerospace Engineering, Bioengineering, Chemistry, Chemical and Petroleum Engineering, Computational Biology, Electrical Engineering and Computer Science, Geography, Geology, Mathematics, Molecular Biosciences, and Physics & Astronomy. In addition, faculty and undergraduate researchers from four different Primarily Undergraduate Institutions (PUIs) with ties to KU (The College of St. Scholastica, Prairie View A&M University, Hobart & William Smith College, and the University of Southern Indiana) will have access to the enabled computational resources. Additionally, workshops will be offered by the CRC and participants on cluster use and advanced computational methods, that will also be available to students (and faculty) from KU and the participating PUIs. The proposed HPC resources will enable exposure to HPC techniques to students in graduate and advanced undergraduate courses. Finally, the BigJay cluster will stimulate increased interdisciplinary collaborations centered around HPC, including with the PUI faculty participants. <br/><br/>BigJay will be made up of a mix of CPU, high-memory CPU, and GPU-enabled processors and include gigabit ethernet and high-speed Infiniband networking. The proposed configuration was chosen to reflect the diverse computing need of the participants. The fundamental research enabled by BigJay will span algorithm development, computational methodology, machine learning, and theoretical descriptions of physical systems. The work will impact problems of importance to society ranging from the development of battery materials to cancer recognition to optimization of catalytic systems, as well as improved theories of physical and biological systems. Interdisciplinary collaborations will be facilitated by the workshops that will introduce students and faculty across disciplines. These will be used as an opportunity to identify commonalities in research topics, numerical methods, and computational tools. Collaborations will also be encouraged by instituting a Computational Research Symposium, consisting of posters and short talks, that will be held biannually for the participants, as well as others who are engaged in computational research on the KU campus. The proposed purchase will substantially enhance HPC research efforts at KU and the PUIs and make participating research groups potentially more competitive for funding from federal, non-profit, or industrial sources. It will further be used to enhance student training in scientific and technical computing through research, the CRC and participant-led workshops, and inclusion of HPC components in graduate and advanced undergraduate courses.<br/><br/>This project is jointly funded by the Major Research Instrumentation (MRI) program, the Established Program to Stimulate Competitive Research (EPSCoR), and the Computer & Information Science & Engineering (CISE) Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346343","CC* Campus Compute: Interdisciplinary GPU-Enabled Compute","OAC","Campus Cyberinfrastructure","05/01/2024","11/30/2023","Richard Page","OH","Miami University","Standard Grant","Amy Apon","04/30/2026","$494,792.00","David Seidl, Jens Mueller, scott campbell","pagerc@miamioh.edu","501 E HIGH ST","OXFORD","OH","450561846","5135293600","CSE","808000","","$0.00","The Talon graphics processing unit (GPU) cluster represents a major leap forward in GPU-accelerated computing at Miami University. Workforce development and education efforts utilizing the Talon GPU cluster are strengthening artificial intelligence literacy and incorporating artificial intelligence into the higher education landscape in a manner consistent with best educational practices. These efforts are training an engaged community prepared to meet the needs of the U.S. economy in machine learning and artificial intelligence. The project is leveraging Talon GPU cluster capabilities by providing access through the Open Science Grid, enabling 20% of cluster uptime for research from the broader community.<br/><br/>The Talon GPU cluster is a dedicated high-performance computing cluster comprising multiple nodes, each with multiple state-of-the-art GPUs. Resource-intensive GPU-accelerated computing efforts are facilitated by onboard memory for each GPU and high-capacity on-node disk space. Parallel computing job performance is enhanced by fast interconnects. Research enabled by the Talon GPU cluster spans all academic divisions at Miami University and includes the development of sophisticated simulations of atomic nuclei, the development and use of AI chatbots for complex technical handbooks, in silico screening of enzyme inhibitor candidates via machine learning, and the development of machine learning algorithms for automated analysis of image-based manufacturing quality control. The interdisciplinary and collaborative efforts of Miami University researchers and educators are incorporating the societal and pedagogical impact of artificial intelligence into courses and further connect the research to education efforts centered on ethical issues around artificial intelligence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2400504","Conference: SF CISE Funding Opportunities for Collaborative Research in Puerto Rico","CNS","Information Technology Researc","10/15/2023","10/13/2023","Nayda Santiago","PR","University of Puerto Rico Mayaguez","Standard Grant","Jeffrey Forbes","09/30/2024","$36,263.00","","naydag.santiago@upr.edu","259 BLVD ALFONSO VALDES","MAYAGUEZ","PR","006806475","7878312065","CSE","164000","7556, 9150","$0.00","This project supports a conference that will connect NSF officials, including Dr. Margaret Martonosi, Assistant Director (AD) for Computer and Information Science and Engineering (CISE) of the National Science Foundation (NSF), with researchers in Puerto Rico's higher education institutions in Computer Science and associated disciplines. Puerto Rico has numerous public and private universities, many of which are Hispanic Serving Institutions offering CISE-related programs. This event disseminates information and opportunities related to CISE in Puerto Rico and promotes collaborations.<br/><br/>The Intellectual Merit of this project is to expose Puerto Rico computing related researchers to opportunities in the CISE NSF directorate programs, train faculty on writing successful proposals for NSF, and foster the interactions between members of the Puerto Rico researchers? community to produce additional increased collaborations. <br/><br/>The broader impact of the conference is that it will serve Hispanic Serving Institutions researchers, increasing diversity among researchers in CISE related fields and opportunities for graduate and undergraduate students. It will additionally increase the capacity in computer science and related fields to promote innovation among Puerto Rico researchers.  Through this program, the National Science Foundation?s mission is strengthened by opening more research, training, and career opportunities for a diverse workforce.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2206950","Collaborative Proposal: SaTC: Frontiers: Securing the Future of Computing for Marginalized and Vulnerable Populations","CNS","Information Technology Researc, Secure &Trustworthy Cyberspace","10/01/2022","06/13/2023","Kevin Butler","FL","University of Florida","Continuing Grant","Dan Cosley","09/30/2027","$1,678,269.00","Eakta Jain, Patrick Traynor","butler@cise.ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","CSE","164000, 806000","025Z, 065Z, 8087, 9178, 9251","$0.00","Computing systems and services are an essential part of modern society and are deeply embedded in people?s daily lives. However, as practices and technologies for ensuring security and privacy of computing systems emerge and rapidly change, the needs of marginalized and vulnerable populations have been under-addressed, as have the consequences of their exclusion. This Frontiers-scale project seeks to fundamentally change how security and privacy in computing is approached, to make centering the needs of marginalized and vulnerable populations the norm. To do so, the team of researchers will create security and privacy design principles that mitigate harm and enhance the benefits of both current and future computing technologies. This work will be informed by direct collaboration with marginalized and vulnerable communities and by strong technical foundations and social science theories. This project will build and sustain a community of researchers to ensure that the needs of marginalized and vulnerable populations are centered in security and privacy over the long term. Such work will develop research methodologies and outcomes that inform design, education, and policy to impact both the scientific community and society at large.<br/><br/>The project focuses on examining three major themes: assessing the security and privacy needs of marginalized and vulnerable populations, informing and co-creating solutions that intersect with current and emerging technologies, and systematizing and applying foundational design principles. The first area involves quantitative and qualitative human-centered research methods and direct community input to address the unique challenges and needs of different populations. The second area involves identifying how technology can be leveraged or reimagined to address these needs through methodologies that consider security and privacy goals for systems and data. The final area involves iteratively synthesizing lessons and experiences from the previous two areas to support integrating security, privacy, and safety needs of marginalized and vulnerable populations into future technology design and researcher efforts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232048","CAREER: Toward Autonomous Decision Making and Coordination in Intelligent Unmanned Aerial Vehicles' Operation in Dynamic Uncertain Remote Areas","CNS","Information Technology Researc, Special Projects - CNS, CPS-Cyber-Physical Systems","08/01/2022","02/28/2024","Fatemeh Afghah","SC","Clemson University","Continuing Grant","David Corman","07/31/2025","$494,036.00","","fafghah@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","CSE","164000, 171400, 791800","7918, 9251, CL10","$0.00","Unmanned aerial vehicles (UAVs) have been increasingly utilized in several commercial and civil applications such as package delivery, traffic monitoring, precision agriculture, remote sensing, border patrol, hazard monitoring, disaster relief, and search and rescue operations to collect data/imagery for a ground command station nearby. Current implementations of UAV-based operations heavily rely on control, inference, task allocation, and planning from a human controller that can limit the operation of drones in missions where the operation field is not fully observable to the human controller prior to the mission and reliable and continuous communication is not available between the UAVs and the ground station or among the teammate UAVs during the mission. The UAVs can be particularly useful in such unstructured and unknown environments to provide agile surveying or search-and-rescue operations. Therefore, the future of UAV technology focuses on the development of small, low-cost, and smart drones with a higher level of autonomy.  Such drones can facilitate a wide range of sophisticated missions performed by a fleet of cooperative UAVs with minimum human intervention and lower cost. <br/><br/>The objective of this research is to develop theoretical and practical frameworks for operation, situational awareness, coordination, and communication of a network of fully autonomous multi-agent systems (e.g., UAVs) in dynamic and unknown environments with minimum human interventions. This research can facilitate a new set of applications for autonomous multi-agent systems in remote and dynamic environments. This project involves an integrated set of research, implementation, and experimental validation thrusts to develop novel frameworks for autonomous decision making, coalition formation, coordination, spectrum management, and task allocation in UAV systems. The developed techniques can be utilized in other multi-agent cognitive systems such as robotic systems, and autonomous driving vehicles where quick search, surveillance, and reactions are required with limited human interventions.<br/><br/>This project also offers a number of educational and outreach activities to integrate the results of this research in curriculum enhancement, student mentorship, engaging underrepresented minority and female students, developing hands-on UAV-based sensing experiments for elementary and middle school students and outreach to the community to enhance public awareness about new applications of UAV systems through collaboration with Flagstaff Festival of Science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2230458","Conference: Grant-Seeking Workshops in Computer Science for Smaller, Independent, Primarily Undergraduate Institutions","CNS","Information Technology Researc","07/01/2022","07/06/2022","Samantha Sabalis","DC","Council of Independent Colleges","Standard Grant","Dan Cosley","06/30/2026","$183,846.00","","ssabalis@cic.edu","1 DUPONT CIR NW","WASHINGTON","DC","200361110","","CSE","164000","025Z, 7556, 9102","$0.00","This grant supports efforts by the Council of Independent Colleges (CIC) to increase the number of research grants in computer science sought by and awarded to faculty members at small to midsized independent predominantly undergraduate institutions (PUIs). The award supports a series of three workshops targeting faculty members in computer science and related disciplines to write effective and compelling proposals to the National Science Foundation?s (NSF) Computer and Information Science and Engineering Research Initiation Initiative (CRII) and Faculty Early Career Development Program (CAREER) programs.<br/><br/>The goals of these workshops are to:<br/>1) Train up to 180 early career faculty members in computer science and related disciplines from predominantly undergraduate institutions (PUIs) to submit competitive proposals to the CAREER and CRII programs.<br/>2) Prepare grants administrators at up to 180 institutions to better collaborate with faculty members in computer science on seeking and managing NSF research grants?and, in the process, strengthen the grant-seeking capacity of these institutions.<br/>3) Increase the number of grants awarded to CIC member institutions in NSF?s CRII and CAREER programs for computer science and related disciplines, with the goal of twenty awarded projects by 2025.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104292","Collaborative Research: NGSDI: Foundations of Clean and Balanced Datacenters: Treehouse","CNS","Information Technology Researc, Special Projects - CNS","05/01/2021","04/26/2023","Asaf Cidon","NY","Columbia University","Continuing Grant","Daniel Andresen","04/30/2025","$379,405.00","","ac4665@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","CSE","164000, 171400","","$0.00","Computing requires energy. Most computing is done in centralized hubs called datacenters.  Datacenters consume an estimated 1-2% of worldwide electricity production. Datacenter computing, and its energy use, is projected to continue to grow rapidly, perhaps as fast as doubling every few years. This is simply not sustainable. The Treehouse project aims to improve the energy efficiency of datacenter computing by making datacenter computing energy use accountable to users at a fine-grained level and by reducing unnecessary waste in the most frequently used parts of datacenter computation.<br/> <br/>Treehouse improves datacenter energy efficiency in several ways. Treehouse introduces a new computational abstraction that allows new energy optimizations by both application developers (by making application energy use visible at a fine-grained level) and systems designers (by identifying when energy-efficient optimizations can be safely performed without compromising user goals for application performance and reliability). Additional strategies include reducing unnecessary software bloat, reducing resource stranding, and new algorithms to exploit the opportunity posed by new types of hardware with complex tradeoffs between performance and energy use.<br/> <br/>Beyond better energy and resource management, Treehouse provides end users the tools to understand and reduce their individual carbon use from cloud services. This can fundamentally change the way the cloud computing industry thinks about datacenter energy use. Datacenter operators can provide new energy efficient computing models at lower cost. Treehouse software systems and protocols will be open source. Through outreach and new educational materials, Treehouse will pioneer the training of a new type of energy-aware engineer to meet societal needs for an energy-efficient computing infrastructure.<br/> <br/>Treehouse will produce software artifacts, hardware designs, and the results of running those programs and artifacts. These materials will be available for public use under a permissive open source license, archived in multiple locations, and available at the project website treehouse.cs.washington.edu for at least five years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104243","Collaborative Research: NGSDI: Foundations of Clean and Balanced Datacenters: Treehouse","CNS","Information Technology Researc, Special Projects - CNS","05/01/2021","06/13/2023","Mosharaf Chowdhury","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Daniel Andresen","04/30/2025","$377,252.00","","mosharaf@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","CSE","164000, 171400","","$0.00","Computing requires energy. Most computing is done in centralized hubs called datacenters.  Datacenters consume an estimated 1-2% of worldwide electricity production. Datacenter computing, and its energy use, is projected to continue to grow rapidly, perhaps as fast as doubling every few years. This is simply not sustainable. The Treehouse project aims to improve the energy efficiency of datacenter computing by making datacenter computing energy use accountable to users at a fine-grained level and by reducing unnecessary waste in the most frequently used parts of datacenter computation.<br/> <br/>Treehouse improves datacenter energy efficiency in several ways. Treehouse introduces a new computational abstraction that allows new energy optimizations by both application developers (by making application energy use visible at a fine-grained level) and systems designers (by identifying when energy-efficient optimizations can be safely performed without compromising user goals for application performance and reliability). Additional strategies include reducing unnecessary software bloat, reducing resource stranding, and new algorithms to exploit the opportunity posed by new types of hardware with complex tradeoffs between performance and energy use.<br/> <br/>Beyond better energy and resource management, Treehouse provides end users the tools to understand and reduce their individual carbon use from cloud services. This can fundamentally change the way the cloud computing industry thinks about datacenter energy use. Datacenter operators can provide new energy efficient computing models at lower cost. Treehouse software systems and protocols will be open source. Through outreach and new educational materials, Treehouse will pioneer the training of a new type of energy-aware engineer to meet societal needs for an energy-efficient computing infrastructure.<br/> <br/>Treehouse will produce software artifacts, hardware designs, and the results of running those programs and artifacts. These materials will be available for public use under a permissive open source license, archived in multiple locations, and available at the project website treehouse.cs.washington.edu for at least five years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2105494","Collaborative Research: NGSDI: CarbonFirst: A Sustainable and Reliable Carbon-Centric Cloud-Edge Software Infrastructure","CNS","Information Technology Researc, Special Projects - CNS","05/01/2021","06/13/2023","Prashant Shenoy","MA","University of Massachusetts Amherst","Continuing Grant","Daniel Andresen","04/30/2025","$1,185,664.00","Ramesh Sitaraman, David Irwin, Mohammadhassan Hajiesmaili","shenoy@cs.umass.edu","101 COMMONWEALTH AVE","AMHERST","MA","010039252","4135450698","CSE","164000, 171400","7354","$0.00","Cloud computing platforms continue to grow exponentially, and are becoming the foundation of our information-based economy.  While the cloud?s energy demand grew more slowly than expected over the past decade due to aggressive energy-efficiency optimizations, there are few remaining optimization opportunities using traditional methods. As a result, the cloud?s continued exponential growth will translate into exponentially rising energy demand, which will position it as one of the primary contributors to global carbon emissions. To address the problem, this project elevates carbon to a first-class metric in designing a sustainable and reliable cloud-edge software infrastructure that can enable continued exponential growth.<br/><br/>The project's foundation is a software-defined energy virtualization layer that provides applications visibility into, and control of, their own energy and carbon usage. The project will leverage this foundation to develop higher-level systems abstractions for supporting carbon-efficient applications at different geographical scales including: a cluster balloon technique, which automatically adjusts applications? energy usage to match a volatile clean energy supply at local edge sites; edge hopping mechanisms, which exploit lower regional energy volatility to balance energy across edge sites; and carbon capping policies, which track applications? global grid carbon emissions and restrict grid energy after reaching the cap.<br/><br/>The project has the potential for significant societal impact by enabling commercial cloud platforms to sustainably continue their exponential growth.  The project will conduct outreach by incorporating topics from the proposal into summer programs for local middle and high school students at the partner institutions. The project will also impact the curriculum at these institutions by adopting elements of edge, cloud, and sustainable computing into graduate and advanced undergraduate courses.  Finally, the project will recruit a diverse group of students by leveraging institutional diversity efforts and will involve undergraduate students through Research Experience for Undergraduate (REU) projects.<br/><br/>The project will make its software artifacts, datasets, and research results available to the research community on the project website at http://www.carbonfirst.org and via the UMass Trace Repository  at http://traces.cs.umass.edu. Artifacts derived from this project will be maintained on the project website and the trace repository for a minimum of five years after the project's conclusion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018979","CC* CRIA: Building CI Strategies and Capacity at the Tribal Colleges","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/13/2023","Gwendolyn Huntoon","VA","American Indian Higher Education Consortium","Continuing Grant","Kevin Thompson","06/30/2024","$249,995.00","James Bottum","ghuntoon@aihec.org","121 ORONOCO ST","ALEXANDRIA","VA","223142015","7038380400","CSE","723100, 808000","","$0.00","The AIHEC CC* CRIA project: Building CI Strategies and Capacity at the Tribal Colleges, addresses the issue of bringing the nation?s 37 Tribal Colleges and Universities (TCUs) into the national cyberinfrastructure-enabled STEM research and education community. This comprehensive CI strategy focuses on CI training, planning and community-building involving both STEM faculty and TCU IT organizations, advancing the capacity of TCU faculty and students to participate in the national STEM research and education infrastructure. CI research and education stakeholders participating in the project include Internet 2, the Texas Advanced Computing Center (TACC), the Center for Computationally Assisted Science and Technology (CCAST), the University of Colorado Boulder Research Computing Center, and the Oklahoma University Supercomputing Center or Education and Research (OSCER). The project implements a cost-effective model for broadening participation of an important historically underrepresented population in STEM.  <br/><br/>The project implements a connectivist model of knowledge management in which interactions among members of a network adaptively distribute resources needed to support individual and group activities to achieve common goals of the network membership. In this project, TCU STEM faculty and IT professionals and national CI STEM domain-specific researchers comprise this extended adaptive network. Project-related activities involve interactions within the network (e.g. training events, work with data analytic tools) that generate new knowledge and skills, build the collective capacity of the TCU members of the network to develop, implement and support research and education programming. Capacity-building interactions among members of this extended network include knowledge-sharing, mentoring and collaborative problem-solving.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1920182","MRI: Development of an Instrument for a Comprehensive Study of Alzheimer's Disease: Multimodal Imaging, Visualization, Machine Learning and Therapeutic Brain Stimulation","CNS","Major Research Instrumentation, Information Technology Researc, Special Projects - CNS, IIS Special Projects","10/01/2019","09/07/2022","Malek Adjouadi","FL","Florida International University","Continuing Grant","Marilyn McClure","09/30/2025","$3,423,641.00","Naphtali Rishe, Armando Barreto, Mercedes Cabrerizo, Shanna Burke","adjouadi@fiu.edu","11200 SW 8TH ST","MIAMI","FL","331992516","3053482494","CSE","118900, 164000, 171400, 748400","1189","$0.00","This project develops an instrument to establish an integrated infrastructure not only in terms of multiple recording modalities, but also in terms of collection and management of structural, functional, and metabolic brain data. As an application it utilizes the comprehensive study of Alzheimer's Disease (AD),notably neuropsychological testing, genetics, and demographic factors, all linked to a database with common evaluations and standardized measures, thus setting up an environment most suitable for multi-site studies and the merging of data across sites. This instrument is expected to create an appropriate environment for seeking the gold standards of machine/deep learning (i.e., stability, sparsity, interpretability, accuracy, and ability to handle missing data inherent to clinical studies and to confront multicollinearity--an intrinsic characteristic from the repeated measures design in longitudinal studies). This work integrates a Neuroimaging Web-Services Interface, Multimodal Neuroimaging Platform, and a Computational Platform. The instrument is created to overcome the opacity of ensemble methods such as ANNs (Artificial Neural Networks) through sparse, and yet highly interpretable, decision trees and processes. The deliberation process for a diagnosis or prognosis is likely to be enhanced by learning what significant features are essential to lead to a given classification and/or prediction outcome. The modular structure of the design allows extensions to other application domains that involve other neurological disorders where brain imaging is part of any subject critical care. Utilizing the instrument through the web interface as a cyber-physical system may enable the research community to apply new data mining concepts or to execute novel classification and prediction algorithms on a multimodal and computationally-effective neuroimaging platform, thus opening the field for multi-site studies, as well as extensive data sharing. <br/> <br/>This sophisticated instrument allows for training new graduates who combine engineering and computing know-how into the fields of bioscience, computing, and medicine. It significantly promotes science and national health by eliciting new understanding of a disease that according to the data reported in 2017 by the Alzheimer's Association, affects 10% of the population over 65 (with growing costs estimated above $259 billion).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845208","CAREER: Scalable Sparse Linear Algebra for Extreme-Scale Data Analytics and Scientific Computing","OAC","CAREER: FACULTY EARLY CAR DEV, Information Technology Researc, CYBERINFRASTRUCTURE","02/15/2019","09/21/2022","Metin Aktulga","MI","Michigan State University","Continuing Grant","Juan Li","01/31/2025","$499,999.00","","hma@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","CSE","104500, 164000, 723100","026Z, 1045","$0.00","This project addresses several technical challenges and develops a computing infrastructure to enable solving very large scientific problems that require high end computing such as for physics and material sciences (""scientific computing"") and for analyzing patterns within huge amounts of data such as those generated by social media (""big data analytics"").  A unifying computational motif in the seemingly disparate fields of big data analytics and scientific computing is that the models currently used to solve the relevant problems often result in large amount of data with significant, irregular gaps (technically known as ""sparse matrices""). The scale of solving such problems typically require execution on massively parallel computers. Due to the unique characteristics associated with sparse matrix computations, achieving high performance and scalability is challenging. This project aims to develop an extensive set of scalable sparse matrix algorithms and software to address such challenges. By significantly improving the productivity of domain scientists working on big data analytics and scientific computing, this project serves the national interest, as stated by NSF's mission: to promote the progress of science; to advance the national health, prosperity and welfare; or to secure the national defense. Research plans are tightly integrated with educational and outreach objectives at various levels. The centerpiece of the outreach efforts is a Computer Science summer school and mentorship plans for high school students. <br/><br/>To tackle the challenges presented by the increasingly deep memory hierarchies of modern computer architectures that include cache, high-bandwidth device memories (HBM), DRAM, and non-volatile random access memory (NVRAM) and facilitate high performance execution of sparse matrix computations, a comprehensive research plan is explored. The centerpiece of this project is a data-flow middleware with a simple application programming interface, called DeepSparse, that aims to support a wide variety of sparse solvers, while ensuring architecture and performance portability. DeepSparse converts a given sparse solver code into a directed acyclic graph (DAG) where nodes represent computational tasks and edges represent the data-flow between tasks. Novel DAG partitioning and scheduling algorithms, which are also extended to their hypergraph counterparts, are developed to ensure that data movement between memory layers is minimized during execution of the task graph. Performance models based on the extended Roofline model and innovative memory management schemes that draw upon ideas from disk storage systems are explored to ensure high bandwidth and low latency access to sparse solver data on NVRAM devices. All software and tools developed in this research are distributed as open source projects for a broad impact. Overall, goals of this project are well aligned with the National Strategic Computing Initiative, which aims to foster innovations that can bring the fields of big data analytics and scientific computing closer.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232601","CC* Data Storage: Supporting Big-Data Edge Computing using Hybrid and Cloud-Native Storage Infrastructure","OAC","Campus Cyberinfrastructure","09/01/2022","08/26/2022","Xiaosong Li","WA","University of Washington","Standard Grant","Kevin Thompson","08/31/2024","$500,000.00","Andrew Connolly","xsli@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","CSE","808000","","$0.00","The University of Washington (UW) Information Technology (UW-IT) and the Office of Vice Provost for Research are supported by a Campus Cyberinfrastructure (CC*) award to create a novel central data storage, curation, and sharing platform. The core tenant of this research ecosystem is the development of a scalable, elastic, low-latency data storage framework supporting a wide range of interdisciplinary research, bolstered by advanced high-throughput data science tools, an open-source high-performance data management system, and the expertise of professional storage cyberinfrastructure engineers. The new cyberinfrastructure empowers researchers to perform scalable, data-intensive analysis across disciplinary and institutional boundaries to inform new discoveries, including materials, particle physics, environmental, medical, and health sciences. The project develops a broad range of data science training programs to provide interdisciplinary research experiences to the trainees and supports data science education and research at partner minority-serving institutions, preparing them for diverse education leadership roles, and simultaneously inspiring enthusiasm for big-data sciences in the broader community.<br/><br/>The project identifies and removes data storage bottlenecks that are common across scientific domains and develops a modern data storage cyberinfrastructure that scales scientific analyses to thousands of processors and petabytes of data locally as well as on top of existing national NSF cyberinfrastructure, while maintaining seamless synchronization with commercial cloud-based storage. The deployment of the 9.6 PB usable new storage infrastructure advances all big data research disciplines by providing scalable low-latency data access and egress across all three local UW campuses (Seattle, Bothell, Tacoma).  The new data storage ecosystem facilitates a research paradigm shift by (1) advancing the development of high-throughput data analytics; (2) encouraging collaborative big-data research through a common data landing path; (3) providing a highly secure yet permissive data management plan, to allow compliant use with regulated research data; and (4) supporting convergent science by bringing together substantially different science and engineering disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2216332","MRI: Development of a Terahertz Measurement Facility for Wireless Communications, Electronics and Materials","CNS","Major Research Instrumentation, Information Technology Researc, Special Projects - CNS","10/01/2022","05/03/2024","Theodore Rappaport","NY","New York University","Standard Grant","Deepankar Medhi","09/30/2026","$3,008,000.00","Zoya Popovic, Habarakada Madanayake, Shuai Nie, Davood Shahrjerdi","tsr@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","CSE","118900, 164000, 171400","1189, 9178, 9251","$0.00","The Terahertz (THz) Measurement Facility, a collaboration between New York University, University of Colorado at Boulder, University of Nebraska?Lincoln, and Florida International University, is a laboratory to support basic measurements of devices, circuits, materials, and radio propagation channels at the highest reaches of the radio spectrum. While today?s cellular telephones and wi-fi networks operate at frequencies below 100 GHz, there is great promise for greater download speeds and vast new wireless applications by moving up to the underexplored sub-THz and THz frequency bands ? frequencies from 100 to 500 GHz.  This MRI grant provides a facility to explore wireless components and systems at these new frequencies. <br/><br/>This grant supports three areas of measurement: a) Radio Frequency Integrated Circuit (RFIC) measurements, b) radio propagation and channel modeling, and c) metrology and calibration, over the contiguous frequency range of 75 GHz to 500 GHz. A unique concept of this facility is the loan of equipment, where institutions may borrow THz components to conduct remote field measurements for wireless communications, propagation, and sensing. Evolving semiconductors and integrated circuits, as well as the next-generation electronics based on layered materials (e.g., graphene), will be measured at THz bands using the RFIC probe station. This facility will have a broad impact on the future of communications, materials, and devices. The creation of new calibration and metrology approaches are vital for accurate and repeatable measurements throughout the US research community in this underexplored range of frequencies. The study of nanotechnology devices using the RFIC probe station will unleash new capabilities in sensing, communications, and computing that may have a transformative impact on society. The radio propagation measurement systems offer vital knowledge for researchers in industry, academia and international standard bodies who will design future high-speed wireless networks for 6G, 7G and beyond. Students using this facility will gain knowledge at these new frequency bands. The THz Measurement Facility will host a robust website for the explanation of available equipment, tutorials for learning how to use the facility, and a repository of measurement results, metrology approaches, and recent research results. The website link is https://engineering.nyu.edu/THzLAB and will be maintained and updated regularly. Popular simulators, measurement studies, calibration results, student and collaborator activities, sponsor and vendor activities, equipment user notes, and K-12 outreach events will be placed on the website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2137123","CI CoE: Demo Pilot: Minority Serving Cyberinfrastructure Consortium","OAC","Campus Cyberinfrastructure","10/01/2021","06/07/2023","Ana Hunsinger","DC","INTERNET2","Standard Grant","Kevin Thompson","09/30/2024","$2,999,998.00","Algirdas Kuslikis, James Bottum, Damian Clarke, Richard Alo, Deborah Dent","ana@internet2.edu","1150 18TH ST NW","WASHINGTON","DC","200363880","7349134264","CSE","808000","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>The Minority Serving - Cyberinfrastructure Consortium (MS-CC) is a collaborative initiative to improve cyberinfrastructure at Historically Black Colleges and Universities (HBCUs), Tribal Colleges and Universities (TCUs), Hispanic Serving Institutions (HSIs) and other Minority Serving Institutions (MSIs). In collaboration with Internet2 and the American Indian Higher Education Consortium, MS-CC is developing mechanisms to increase access to cyberinfrastructure resources, funding, and professional development opportunities for faculty, staff, and students at HBCUs, TCUs, HSIs, and other MSIs. MS-CC is taking an agile and adaptive approach to operating as a consortium, with a long-term commitment to learning and adjusting based on successes and lessons learned, matched to the diverse size and missions of these colleges and universities.<br/><br/>A key outcome of this grant is the formalization of a vibrant community of practice across MS-CC campuses that involves collaboration on cyberinfrastructure, education, and research applications.  This grant enables HBCUs, TCUs, HSIs and other MSIs to accomplish together what they cannot do separately. The MS-CC is broadening participation in science, technology, engineering, and mathematics (STEM) by historically underrepresented groups in the United States? research enterprise, enabling new perspectives to emerge and expand capabilities for the nation.  MS-CC is advancing our nation?s economic growth, national security and global prosperity in ways that reflect the unique expertise and talent from HBCUs, TCUs, HSIs and other MSIs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104548","Collaborative Research: NGSDI: Foundations of Clean and Balanced Datacenters: Treehouse","CNS","Information Technology Researc, Special Projects - CNS","05/01/2021","04/26/2023","Thomas Anderson","WA","University of Washington","Continuing Grant","Daniel Andresen","04/30/2025","$530,069.00","Irene Zhang","tom@cs.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","CSE","164000, 171400","","$0.00","Computing requires energy. Most computing is done in centralized hubs called datacenters.  Datacenters consume an estimated 1-2% of worldwide electricity production. Datacenter computing, and its energy use, is projected to continue to grow rapidly, perhaps as fast as doubling every few years. This is simply not sustainable. The Treehouse project aims to improve the energy efficiency of datacenter computing by making datacenter computing energy use accountable to users at a fine-grained level and by reducing unnecessary waste in the most frequently used parts of datacenter computation.<br/> <br/>Treehouse improves datacenter energy efficiency in several ways. Treehouse introduces a new computational abstraction that allows new energy optimizations by both application developers (by making application energy use visible at a fine-grained level) and systems designers (by identifying when energy-efficient optimizations can be safely performed without compromising user goals for application performance and reliability). Additional strategies include reducing unnecessary software bloat, reducing resource stranding, and new algorithms to exploit the opportunity posed by new types of hardware with complex tradeoffs between performance and energy use.<br/> <br/>Beyond better energy and resource management, Treehouse provides end users the tools to understand and reduce their individual carbon use from cloud services. This can fundamentally change the way the cloud computing industry thinks about datacenter energy use. Datacenter operators can provide new energy efficient computing models at lower cost. Treehouse software systems and protocols will be open source. Through outreach and new educational materials, Treehouse will pioneer the training of a new type of energy-aware engineer to meet societal needs for an energy-efficient computing infrastructure.<br/> <br/>Treehouse will produce software artifacts, hardware designs, and the results of running those programs and artifacts. These materials will be available for public use under a permissive open source license, archived in multiple locations, and available at the project website treehouse.cs.washington.edu for at least five years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018975","CC* Team: Piloting a CI-Enabled Tribal College and University Research Collaboration","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","10/01/2020","09/27/2023","Gwendolyn Huntoon","VA","American Indian Higher Education Consortium","Standard Grant","Kevin Thompson","09/30/2024","$1,679,994.00","James Bottum, Dana Skow, Chad Davis, Steve Sobiech","ghuntoon@aihec.org","121 ORONOCO ST","ALEXANDRIA","VA","223142015","7038380400","CSE","723100, 808000","9102","$0.00","The American Indian Higher Education Consortium and institutional partners Sitting Bull College, Nueta Hidatsa Sahnish College, United Tribes Technical College Turtle Mountain Community College, Cankdeska Cikana Community College, and North Dakota State University are partnering on this Cyber Team project that establishes the socio-technical foundations of a cyberinfrastructure (CI)-enabled collaboration that will both support current STEM research and education programs and provide the framework for aggressive research program development. The project involves faculty, students and IT staff at the six participating institutions collaborating on CI improvements, IT staff capacity-building, and distributed STEM programming focused on environmental science, eventually including engineering, digital manufacturing and other STEM disciplines.<br/><br/>The project will provide professional development, support, mentoring and collaborative research opportunities for faculty and students at all North Dakota Tribal Colleges and Universities (TCUs). The project will enhance opportunities for economic development and enhance the design and delivery of health, education, and other services developed through CI-enabled basic and applied research. It will serve as a model for engaging TCUs nationally in the adoption of CI resources necessary to support implementation of a significantly broader range of research and education activities, particularly involving collaborations with the larger research community.<br/><br/>The project?s collaborative capacity-building framework implements the connectivist learning model in which networked human and physical resources constitute a complex adaptive system that generates actionable knowledge (both technical and domain-specific) sustainably. The project tests the hypothesis that a connectivist framework can be transformative in building the research and education capacity of small resource challenged institutions and organizations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1828567","MRI: Acquisition of a Regional Resource for Long-Term Archiving of Large Scale Research Data Collections","OAC","Major Research Instrumentation, Information Technology Researc, Data Cyberinfrastructure, mospheric Sci Cluster Prgrm","09/01/2018","07/31/2023","Henry Neeman","OK","University of Oklahoma Norman Campus","Standard Grant","Alejandro Suarez","08/31/2024","$967,755.00","Amy McGovern, Horst Severini, Kendra Dresback, Laura Bartley","hneeman@ou.edu","660 PARRINGTON OVAL RM 301","NORMAN","OK","730193003","4053254757","CSE","118900, 164000, 772600, 779000","062Z, 1189, 9150","$0.00","The project will support University of Oklahoma (OU)'s acquisition, deployment and maintenance of a large-scale storage instrument -- the OU & Regional Research Store (OURRstore). This instrument will enable faculty, staff, postdocs, graduate students and undergraduates to pursue data-intensive research (by building large and growing data collections), and to share and publish these datasets (which they can make discoverable and searchable). Science, Technology, Engineering and Mathematics (STEM) research is increasingly data-intensive, with massive growth of research data collections. Yet a substantial fraction of universities and colleges have been underprepared not only for the volume, velocity and variety of data, but especially for long term stewardship of rapidly growing data collections. In addition, many STEM research projects require substantial storage during their experiments, so much so that holding it all on disk is too expensive to be practical. This challenge is quickly becoming more acute, because disk prices are now improving much more slowly than in the past.<br/><br/>OURRstore is a large-scale storage instrument, consisting of a substantial tape library, as well as crucial support subsystems such as servers, disk, network components and software. OURRstore allows the massive growth of storage capacity needed to address the requirements of the large and broad research community that it serves. Via an innovative, low cost business model, OURRstore offers a cost-effective data storage and access and solution. The research topics supported by the instrument include: weather forecasting, weather radar and weather data mining, including for severe storms such as tornadoes and hurricanes; earthquake triggers and seismology; data and modeling for agriculture, forestry, water and earth ecosystems; coastal simulation; molecular systems that control growth and development of vegetation; microbial contamination of infrastructure; effects of repeated stress on organisms; RNA processing in mitochondria; nanotechnology; visual neuroscience; physiological adaptation to extreme environments; astrophysical objects and stellar atmospheres; malware cybersecurity; influence of microblogs on social media. OURRstore maximizes its impact across the US in the following ways. First, it serves as a national model for affordable, large scale, long term, multi-institutional storage. Second, the OURRstore community includes a substantial number of people from underrepresented populations: over 200 research team members are one or more of the following: women, African Americans, Hispanics, Native Americans, disabled, and US veterans. Third, the instrument will enhance OU's current ""Supercomputing in Plain English"" webinar series, a popular outreach program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835739","Elements: Data: U-Cube: A Cyberinfrastructure for Unified and Ubiquitous Urban Canopy Parameterization","OAC","Special Initiatives, Data Cyberinfrastructure, mospheric Sci Cluster Prgrm, EarthCube","01/01/2019","08/24/2023","Daniel Aliaga","IN","Purdue University","Standard Grant","Sharmistha Bagchi-Sen","09/30/2024","$599,999.00","Rajesh Kalyanam, Dev Niyogi","aliaga@cs.purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","CSE","164200, 772600, 779000, 807400","062Z, 077Z, 1525, 4444, 7923","$0.00","Urban canopy parameters (UCPs) can be used in model simulations to study the health and behavior of a city, determine the ability to sustain a growing population, and study potential impacts of extreme weather events.  The ability to identify and compute urban canopy parameters has been a missing element in city models; this project develops that capability for use in city design and analysis, integrating weather models and remote sensing data to infer a 3D model of cities of various sizes.  The project deploys innovative science-based analysis tools within an extensible, broadly-available cyberinfrastructure portal, allowing users to ingest satellite imagery and other geographic information system (GIS) data to calculate urban canopy parameters.  The cyberinfrastructure would improve urban modeling and planning, particularly for extreme weather events.  The tools and high-performance computing and storage resources would be usable by other researchers through a portal.  Potential beneficiaries include smaller and disadvantaged cities and countries without the resources for urban characterization and modeling necessary for such urban planning.  There are also plans to transfer the results of this research to communities beyond college students -- to local teachers and secondary students and museums, and to the GIS urban planning user communities at local, state, and international levels.<br/><br/>The project develops cyberinfrastructure which would use a novel inverse modeling approach incorporating satellite images, social science and urban zonal data, to infer a 3D model of a city from which urban canopy parameters could be derived for use in simulation models.   The focus is on weather modeling, urban parameterization and a desire to better understand sustainable urbanization.  The main cyberinfrastructure products will be 3D urban models and UCP values for urban locations.  These UCP parameters will be used for fine-scale urban weather modeling, and evaluation of various classification techniques and simulation models in an integrated portal.   The approach differs from prior work that relied on simple urban canopy models, either tuned for a large metropolis or assuming that all cities are the same.  The team uses a cyberinfrastructure platform at Purdue (HubZERO) and the Geospatial Data Analysis Building Blocks (GABBs), a suite of software modules developed during a previously funded NSF Data Infrastructure project.  The resulting platform can be deployed using Amazon Web Services, extending built-in geospatial data capabilities and providing a scalable CI solution.  This platform can be used by researchers to test predictive models or deploy applications that have been developed.  The team has cultivated relationships with the research communities and stakeholders relevant to the proposed research.  Through the World Urban Database and Access Portal Tools (WUDAPT) project -- a community-based project to gather a census of cities around the world -- the team is already connected to the urban planning community globally.  The project will improve urban weather modeling accuracy and increase availability of and access to the new techniques, capabilities and dedicated cyberinfrastructure.   The results have the potential to support city officials and urban planners, especially in regions with the fastest rate of urbanization and/or those in developing countries, where access to computational resources is likely to be limited.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure will be jointly supported by the Division of Chemical, Bioengineering, Environmental, and Transport Systems, within the NSF Directorate for Engineering; and the Division of Atmospheric and Geospace Sciences and the Integrative and Collaborative Education and Research (ICER) Program, within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2143044","CAREER: Voice Technologies for Helping Older Adults Navigate Uncertain Information in Decision Making","IIS","Information Technology Researc, HCC-Human-Centered Computing","10/01/2022","08/02/2023","Robin Brewer","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Dan Cosley","09/30/2027","$480,596.00","","rnbrew@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","CSE","164000, 736700","102Z, 1045, 7367","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>People often create strategies for determining if the information they encounter online is credible. For example, they may evaluate the website?s structure, professionalism, or other design features. However, many of these strategies are not helpful when searching for information when using voice technologies, many of which lack screens and thus don?t present visual credibility cues. Further, older adults, who often use voice technologies to address accessibility challenges posed by vision, motor, and cognitive disabilities, are also often more susceptible to contradictory or uncertain information that characterizes important topics such as healthcare. Combined, these factors place older adults at greater risk of making risky decisions when accessing information online. This project investigates approaches for developing voice technologies that help older adults with credibility assessment, managing uncertain or inconsistent information, and making informed decisions. The work will advance knowledge about how older adults use voice technologies in seeking information online, develop models for non-visual information seeking, and create new ways of interacting with voice assistants and information to reduce these risks. <br/><br/>This project draws upon behavior change research to investigate how to nudge older adults towards better information decisions when using voice technologies. To do so, the research team will engage older adults as community partners, not only in involved in data collection, but also as members of the research team. The research will (1) provide empirical evidence of older adults? information behaviors when using voice interfaces; (2) evaluate a range of choice architecture strategies for voice-based information seeking and decision-making, using findings to inform the design of a framework for non-visual information seeking; (3) implement the design framework through a toolkit of novel audio tools; and (4) deploy these tools, evaluating their effectiveness at mitigating information uncertainty ?in-the-wild? with older adults. The outcomes of the work will advance research in human computer interaction, information retrieval and sensemaking, and aging. Further, as voice interface design is understudied in computing education, this project will also contribute new modules for teaching voice application design to middle and high school students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2230143","CNS Core: Small: Managing Electrical and Thermal Energy in Sustainable Computing Systems","CNS","Information Technology Researc, CSR-Computer Systems Research","10/01/2022","08/29/2022","David Irwin","MA","University of Massachusetts Amherst","Standard Grant","Daniela Oliveira","09/30/2024","$325,965.00","Fatima Anwar, Jeremy Gummeson","irwin@ecs.umass.edu","101 COMMONWEALTH AVE","AMHERST","MA","010039252","4135450698","CSE","164000, 735400","7923","$0.00","Sustainable computing systems operate on zero-carbon renewable energy harvested from their environment, such as solar or wind, and stored in batteries.  Importantly, renewable-powered systems may be deployed in many different climates that subject them to a wide range of ambient temperatures that significantly alter the efficiency and correctness of these systems' underlying components, including their processors, batteries, solar cells, and clocks. Unfortunately, current systems are either designed for a narrow and ideal temperature range, and thus are often unreliable under even slight temperature variations, or must consume significant additional energy to maintain an ideal temperature within a narrow window, which significantly reduces their energy-efficiency.   To address the problem, this project proposes fundamental research on the design of sustainable renewable-powered systems that are ectothermic in that they jointly manage and adapt to variations in the electrical and thermal energy available in their environment to optimize their energy-efficiency, performance, and reliability. While the temperature responses of individual system components are well-known, ectothermic design takes a holistic systems approach that exploits relationships between components and their environment to co-optimize system-wide energy-efficiency, correctness, performance, and reliability.  To this end, this project will develop methods for understanding, modeling, and exploiting the dependencies between computation, heat generation, the ambient environment, renewable energy availability, and the workload.  <br/><br/>This project includes numerous broader impacts.  The project has the potential for significant societal impact and positively address the climate concerns of computing by improving the energy-efficiency, correctness, performance, and reliability of sustainable renewable-powered computer systems. Our project also has the potential for technical impact by improving the design of renewable-powered systems at all scales--from small embedded platforms to large data centers.  This project plans to engage in multiple educational activities, including giving tutorials that focus on the relationship between computing and energy consumption at UMass summer programs for high school students in engineering and computing, and integrating ectothermic design into graduate and undergraduate courses. Finally, the project will emphasize the recruitment of students from under-represented groups in computing.<br/><br/>This proposal is funded in part from the DCL on Design for Sustainability in Computing (NSF-22-060)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2145357","CAREER: Learning and Using Community-Driven Natural Language Processing Models","IIS","Information Technology Researc, Info Integration & Informatics","06/01/2022","05/26/2023","Anthony Rios","TX","University of Texas at San Antonio","Continuing Grant","Sylvia Spengler","05/31/2027","$217,998.00","","anthony.rios@utsa.edu","1 UTSA CIR","SAN ANTONIO","TX","782491644","2104584340","CSE","164000, 736400","102Z, 1045, 7364","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>There is a growing interest in applying Natural Language Processing (NLP) to a wide array of tasks, including, but not limited to, health, online moderation, and education. NLP-related research has generally focused on large models uniformly applied to everyone independent of their writing style and social norms, thus, assuming a one-size-fits-all solution. Nevertheless, NLP-based models do not perform equally for all communities because of different writing styles (e.g., dialects) and choices of topical discussion (e.g., Sports vs. Technology). Furthermore, social norms vary between communities, making the original intended use of some NLP models potentially irrelevant. Hence, applying the same NLP model to everyone may cause harm if communities are not directly considered. Therefore, researchers and practitioners must evaluate NLP models on community data before they deploy them. They must also work with communities to determine whether the technology is sound given the community's social norms and needs. This project will address two critical questions: ""How can stakeholders know whether the model will harm specific communities when put into production?"" and ""What community-specific language patterns cause errors in various NLP models?"". By answering these questions, this project intends to develop tools to help communities participate in the technology development process, which will enable them to decide whether a specific technology is relevant to the community or not. Finally, this project will also create standards-based lessons for high school students in San Antonio by training local high-school teachers in community-driven NLP, which will potentially help local students better consider NLP applications in their lives.<br/><br/>Overall, this project will provide researchers and NLP practitioners with a better understanding of applying and developing NLP models for small communities rather than focusing on a one-size-fits-all framework. Specifically, to address this goal, this project has three objectives. Objective 1 will identify strategies to find correlations between community-specific language and NLP model performance. This objective will result in a better understanding of the inductive biases of NLP models for community-specific applications. Objective 2 will create a tool that can facilitate participatory NLP design by helping community-specific stakeholders identify potential good and harmful outcomes that may be caused by applying a specific NLP model. More importantly, the objective will help decision-makers decide when NLP should or should not be deployed in their communities. Objective 3 will identify methods to improve NLP models for specific communities. The goal is to identify methods to incorporate a community's unlabeled data into existing labeled NLP datasets to improve community-specific model performance. Finally, the project will impact the broader NLP community via the release of open-source software that implements the tools and techniques this award generates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346744","CC* Regional Computing: CENVAL-ARC: Central Valley Accessible Research and Computational Hub","OAC","Campus Cyberinfrastructure","06/01/2024","12/12/2023","Sarvani Chadalapaka","CA","University of California - Merced","Standard Grant","Amy Apon","05/31/2026","$1,000,000.00","Emily Jane McTavish, Anna Klimaszewski-Patterson, Suzanne Sindi","schadalapaka@ucmerced.edu","5200 N LAKE RD","MERCED","CA","953435001","2092012039","CSE","808000","","$0.00","The Central Valley Advanced Research Computing (CENVAL-ARC) initiative aims to address the computational resource needs in the historically underserved Central Valley region of California, enhancing research and educational opportunities for a broad spectrum of students and researchers within Hispanic-Serving Central Valley institutions.  The project is led by the University of California, Merced (UCM), in collaboration with California State Universities (CSUs) - CSU Sacramento, CSU Stanislaus, and CSU Fresno.<br/><br/>CENVAL-ARC's hardware setup comprises Graphical Processing Unit (GPU) nodes and Central Processing Unit (CPU) nodes, each with distinct configurations of processor and memory specifications. The hardware seamlessly integrates into UCM's existing Pinnacles High Performance Computing (HPC) cluster. The cluster also features efficient fast scratch and large data storage, enhancing data storage and accessibility for science drivers. CENVAL-ARC utilizes Ethernet and Omnipath networks, supported by open-source tools like XDMoD, for efficient cluster management and data transfers. Its connection to a Science DMZ enhances data exchange with external institutions, fostering collaborative research. Running on RedHat Linux with the Slurm scheduler, CENVAL-ARC's computational resources accommodate diverse research tasks. Standard CPU nodes and large-memory CPU nodes support activities such as intricate systems modeling and numerical optimization, while GPUs are indispensable for executing double-precision floating-point calculations and managing Artificial Intelligence  and Machine Learning (AI/ML) workloads.<br/><br/>CENVAL-ARC plays a transformative role in advancing research throughout the Central Valley. The project supports yearly research symposia that engage dozens of regional participants and empower researchers across various scientific domains. This initiative aligns with the National Science Foundation's vision for catalyzing scientific transformation through cyberinfrastructure, fostering translational research and discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2144960","CAREER: IIS: RI: Foundations of Deep Neural Network Robustness and Efficiency","CCF","Information Technology Researc, Comm & Information Foundations","05/01/2022","09/12/2023","Salimeh Yasaei Sekeh","ME","University of Maine","Continuing Grant","Phillip Regalia","04/30/2027","$506,759.00","","salimeh.yasaei@maine.edu","5717 CORBETT HALL","ORONO","ME","044695717","2075811484","CSE","164000, 779700","1045, 7936, 9102, 9150","$0.00","Deep neural networks have led to significant advances in science and engineering and play an important role in the success of modern machine learning in various real-world applications including vision, speech, pattern recognition, and biology to name a few.  When developing deep-learning solutions, accuracy or performance metrics are often a key point of emphasis. While performance is critical, the computational load of the training process and security of the final solution play an equally important role in a real-world setting. Recent advances in adversarial learning models hold significant promise in improving various learning methods and defending against threats, but the fundamental aspects of these models are still poorly understood, which limits their performance guarantees for efficient and robust decisions. With this in mind, this project investigates simultaneously tackling three desirable properties when developing deep networks: 1) performance, 2) efficiency, and 3) robustness. This project also includes a comprehensive plan to integrate the research results into inclusive, diverse, and cross-disciplinary educational multilevel programs by funding graduate research assistants, summer research fellowship for high-school students and teachers, and organizing a hybrid (online and in-person) deep-learning boot camp. <br/><br/>The overall goal of this research program is to develop a comprehensive and fundamental understanding of the robustness and computational aspects of deep networks by leveraging tools and concepts from probability, information theory, and statistics. This project aims to make critical advances in 1) proper formulations of subnetwork adversarial robustness, 2) characterizing transferability via curriculum learning, and 3) developing efficient approaches for reducing computational complexity involved in training, among others. The theoretical and methodological outcomes of this cross-disciplinary project will broaden the prior knowledge of deep learning and will improve prediction, exploration, and detection applications of machine-learning models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2334701","Tribal College and University Cyberinfrastructure Facilitation","OAC","Campus Cyberinfrastructure","09/15/2023","09/12/2023","Gwendolyn Huntoon","VA","American Indian Higher Education Consortium","Standard Grant","Kevin Thompson","08/31/2028","$4,999,494.00","Alexander Grandon, Carrie Billy","ghuntoon@aihec.org","121 ORONOCO ST","ALEXANDRIA","VA","223142015","7038380400","CSE","808000","","$0.00","Through the Tribal College and University (TCU) CI Facilitation initiative the American Indian Higher Education Consortium (AIHEC) collaborates with the 37 TCUs nationwide to enable the identification, refinement, adoption, and integration of cyberinfrastructure resources into the TCU campus information technology environment in support of STEM applications. Specific activities including campus cyberinfrastructure facilitation, cyberinfrastructure engagement, strategy and policy development, and CI workforce and professional development activities to support the advancement and use of cyberinfrastructure resources and technologies for crosscutting mission-driven applications such as Native language, climate science, and geographic information systems. Communities of practice, starting with the IT Directors/CIOs and STEM faculty communities provide collaboration and capacity building opportunities within the TCUs. The initiative?s flexibility allows each TCU to tailor project activities to their individual science application, campus information technology, and faculty and student needs. <br/><br/>Access to better cyberinfrastructure resources accelerate the exposure and adoption of new technologies and techniques including data storage policies and technologies that preserve data sovereignty and reflect cultural data access requirements. The CI-enabled STEM research at the TCUs provide education and workforce development opportunities to the underrepresented American Indian and Alaska Native populations serving many first-generation students. The use of cyberinfrastructure resources to support, preserve, and promote Native American culture, languages, and heritage empowers the TCU community, including students, faculty, and staff, to become leaders grounded in indigenous values.<br/><br/>This effort is also supported by National Discovery Cloud for Climate (NDC-C) resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2236449","CRII: CNS: Towards Spectrum and Energy Efficient Large-scale IoT Communications: A Cross-layer Optimization Approach","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2022","08/09/2023","Haijian Sun","GA","University of Georgia Research Foundation Inc","Standard Grant","Alhussein Abouzeid","04/30/2025","$209,998.00","","hsun@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","CSE","164000, 736300","044Z, 102Z, 7363, 8228","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Although 5G has dramatically improved network capacity and spectrum efficiency (SE), the explosive growth of Internet of Things (IoT) demands for more spectrum and energy resources to support high device density and massive traffics. It is estimated that at least 5.2 GHz bandwidth is required for just eHealth Care IoT if spectrum is accessed exclusively, or 1.3 GHz even with dynamic sharing strategy. It is clear that shortage of spectrum resources is a major bottleneck for the success of IoT popularity. On the other hand, current IoT devices use standards such as Bluetooth, LoRA, Sigfox, narrow-band IoT (NB-IoT), or Zigbee, which require power-hungry active radio frequency components like oscillators and converters. Battery-driven IoT devices can hardly sustain years of life-cycle goal even with infrequent transmission and optimized low-power protocols. Thus, sustainable energy consumption is another challenge. With tens of billions of IoTs desire for connectivity by 2030, there is a pressing need to address both SE and energy efficiency (EE) challenges to accommodate for such densified IoT networks. This research seeks to improve SE and EE performance while providing guaranteed quality of service (QoS) for IoTs at large-scale, thereby providing a feasible and practical connectivity solution in massive IoT era. Outcomes from this project can bring following impacts: 1) a hybrid and cooperative communication architect for IoTs, which combines benefits from both active and passive mode; 2) integration of research and curriculum design, capstone projects to both undergraduate and graduate students; 3) cutting-edge research experiences to a primarily undergraduate institution (PUI). <br/><br/>The core approach is to enable IoT device with a wireless-powered hybrid communication structure that can not only minimize energy footprint with energy harvesting from ambient signals, but also integrate coordinated passive and active communication to support versatile QoS needs with efficient spectrum utilization through user cooperation. This project offers a holistic solution to deliver following innovations. 1) A novel PHY transmission architect. It combines a bio-inspired symbiotic radio to coordinate excessive interference. Optimization problems for SE and EE metrics are introduced from PHY resource allocation perspective. 2) The co-designed MAC layer protocol to ensure proper user and resource coordination. Two protocols will be introduced, one for maximum performance and the other for lower complexity. 3) System validation with software and hardware implementations. Extensive experimental verification is designed to systematically validate the performance of proposed schemes and algorithms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919631","MRI: Development of an Instrument that Performs Behavioral Analysis for Neuropsychiatric Disorders like Tourette Syndrome","CNS","Major Research Instrumentation, Information Technology Researc, Special Projects - CNS","10/01/2019","09/07/2022","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Marilyn McClure","09/30/2024","$1,863,323.00","Gail Bernstein, Christine Conelea, Daniel Keefe, Victoria Interrante","npapas@cs.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","CSE","118900, 164000, 171400","1189, 9251","$0.00","This project, developing an instrument to enable a pioneering way to canonically represent and systematically quantify behavioral manifestations for a suite of neuropsychiatric disorders, integrates a powerful computer cluster with various sensor modalities and associated immersive technology components that facilitate data analysis and perception of physical and virtual experiences. The instrument will bring together neuropsychiatrists and engineering scientists seeking the development and deployment of tools that promote discovery and new knowledge in their respective domains, and innovation with immediate societal impact. It aims to enable medical experts to explore clinical hypotheses during diagnosis and treatment. Concurrently, scientists/engineers will explore the implementation of new computational tools and algorithms that satisfy new requirements utilizing a set of clinical hypotheses.<br/><br/>Challenging the current understanding of a spectrum of neuropsychiatric conditions and triggering the explorations of new ways to affect them, this instrument should contribute to our understanding of the presence and evolution of mental states. Its unique characteristics will help in integrating a range of informational cues (e.g., visual, acoustic, haptic) along with virtual simulated information. This interplay between physical and virtual experiences and stimuli and their perception in an immersive environment constitutes the ultimate objective which will unleash creativity in better understanding and treat mental illnesses. <br/><br/>The tight coupling of hardware and software components will enhance the various types of interactions such as expert-patient, and patient-virtual objects. Specifically, the instrument will support the following research efforts:<br/> - Comprehensive compilation of behavioral data,<br/> - Development of visualization tools,<br/> - Delivery of haptic information feedback, <br/> - Creation and interaction of virtual objects, and<br/>        - Enhanced sentiment understanding from audio/visual signals, facial expression representation and analysis.<br/>The instrument enables the creation of communication channels with clinics across the country. Moreover, it contributes in forming a strong foundation to explore and understand a broad variety of neuropsychiatric disorders manifested via verbal and non-verbal communication pathways, aims to form a foundation that promotes strong collaboration targeting real-world challenges with immediate societal and economic impact, and offers an environment for discovery via the strong coupling of data visualization, User Interfaces-User eXperience (UI-UX) design, immersive perception, data exploration, and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1933974","Forum on Cyber Resilience","CNS","Information Technology Researc, Special Projects - CNS, , , , ","10/01/2019","09/14/2023","Jon Eisenberg","DC","National Academy of Sciences","Continuing Grant","Ralph Wachter","12/31/2024","$2,106,703.00","Tho Nguyen, Lynette Millett","jeisenbe@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","CSE","164000, 171400, U14500, V18400, W20600, X36100","1640, 1714, 7918, 8237","$0.00","The National Academies Roundtable, the Forum on Cyber Resilience, established in 2015, facilitates and enhances the exchange of ideas among scientists, practitioners, and policy makers concerned with the resilience of computing and communications systems that are embedded in the critical national infrastructures and social systems.  Resilience encompasses not only security in the face of attacks, resistance to degradation, and the ability to recover from adverse events, but also broadly interpreted, the capacity for innovation and adaptation with the ability to absorb technological disruption and surprise in a way that reflects societal values, such as privacy, and needs of the infrastructure's many stakeholders. The value of the Forum on Cyber Resilience derives from the ability of the members to choose topics of mutual interest and then enable discussions on these issues among members and experts in a neutral setting, building trust and promoting innovative problem-solving. <br/><br/>This National Academy of Sciences (NAS) Forum on Cyber Resilience provides a unique venue for stakeholders from academia, industry, and government to broadly consider resilience technologically along with the societal tensions that may arise among them. The Forum explores critical issues of shared interest, framing critical questions and needs on resilience and incubating activities of value to the participants -- encompassing discussions on research directions, emerging technologies, design and engineering approaches, human-system interactions, policy and social implications, education and workforce considerations, and institutional roles. The Forum supports cross-sector and public-private dialogue about goals and challenges, achieving goals shared by the Forum members and their stakeholders for society, enabling early identification of promising approaches to improve resilience, and exploring case studies to extract lessons applicable in other domains and resilience. The Forum events are open to the public, and their reports are publicly available on the NAS website.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201428","CC* Compute: Koa - A High Performance and Flexible Research Computing Resource","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","06/01/2022","05/03/2023","Sean Cleveland","HI","University of Hawaii","Standard Grant","Amy Apon","05/31/2024","$416,000.00","Giuseppe Torri, Alice Koniges, Jason Leigh, Peter Sadowski","seanbc@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","968222247","8089567800","CSE","808000, 915000","9150, 9251","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>The University of Hawaii (UH) aims to establish the CC* Compute: Koa - A High Performance and Flexible Research Computing Resource to support computationally intensive research, education and practice across the UH ten-campus system. The project aims to install a compute cluster Koa, which is architected in response to current resource constraints, particularly scratch storage. The cluster hardware includes scratch storage of 750TB, 2 Graphics Processing Unit (GPU) nodes comprising 10x Nvidia A4000 cards, and a total of 384 CPU cores across 8 compute nodes. Koa provides UH faculty, researchers and students state-of-the-art computational resources focused towards machine learning, artificial intelligence and large scale simulation. The Koa computing cluster is a shared inter-campus resource available to all UH researchers. Koa focuses to support researchers in the specialties of astronomy, atmospheric science, ocean science, microbiome science, and computer & data science, across UH's ten-campus system. Koa resources enable researchers to scale up research to process larger datasets and models in addition to accelerating existing workflows through the new advanced architecture that ties extremely fast storage to the compute resources over a high-speed network, which enables a shorter time to result. The strategic partnership with the Open Science Grid allows for efficient usage of unused Koa resources by the national research community as well as a gateway to a national compute platform for Koa users. Additionally, the Koa resource aids hands-on training in data science and computational science for the next generation of researchers and data scientists through partnership with the Hawaii data science institute and local community for workshops and classroom access.<br/><br/>Koa enables a larger scope and scale of analysis by providing 750TB of high-speed Lustre parallel scratch storage capacity accessible from all compute and GPU accelerated resources. Koa?s 200Gb HDR infiniband network enables Koa?s Lustre to achieve I/O speeds up to 96Gb/s and provide increased computational throughput for I/O heavy workflows.  Koa?s external network connections increase data transfer speeds to national, commercial cloud and academic resources via the combination of 100Gb/s data transfer node connection and high-speed parallel file system, enhancing end-to-end big data workflows. Koa also provides virtualized infrastructure to support specialized computational use cases such as: immersive analytics and science gateways. The strategic partnership with the Open Science Grid allows for harvesting unused cycles on Koa by the national research community as well as a gateway to national compute platform for Koa users. The integration of Koa?s high speed file system with the regional Jestream2 NSF Cloud infrastructure hosted at UH allows researchers to easily span computing environments and modalities between the cloud and local Koa computing resources to support new deep learning and artificial intelligence workflows, visualizations and applications.<br/><br/>This project is funded through the collaborative efforts of the Office of Advanced Cyberinfrastructure (OAC) and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346630","CC* Regional Networking:  Advancing Research and Education at small colleges in Rural and Metropolitan Alabama and Tennessee through IT Architecture Enhancements","OAC","Campus Cyberinfrastructure","06/01/2024","04/30/2024","Samuel D'Angelo","GA","Georgia Tech Research Corporation","Standard Grant","Kevin Thompson","05/31/2026","$1,037,598.00","VINSON HOUSTON, Damian Clarke, Sajid Hussain, Timothy Warren","cas.dangelo@oit.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","303186395","4048944819","CSE","808000","","$0.00","The Georgia Institute of Technology (GT) in collaboration with Southern Light Rail and Southern Crossroads (SLR/SoX) provides advanced networking services for the research and education (R&E) communities across the Southeastern United States. This project extends advanced network services and provides cyberinfrastructure (Cl) access, training and support to researchers and educators from Jacksonville State University and four Minority Serving Institutions (MSIs): Tennessee State University, Fisk University, Meharry Medical College, and Alabama State University.<br/><br/>GT works with these institutions to implement robust and secure 10 gigabit networking that enables and supports the computational research and education at these institutions. Furthermore, GT and SoX are establishing and engaging in outreach activities and are identifying other researchers in these institutions who are interested in accessing Cl resources and training. The project addresses the institutions? networking and Cl needs and enhances their connectivity. By enabling, supporting, and promoting connectivity and computational research at these institutions, this project has a transformative impact on these institutions' research and education activities campus-wide. It strengthens the research ecosystem and network of expertise in the Southeastern states and expands the range of institutions involved in computational research.  The network connectivity facilitated by this project builds on the work completed in related projects, increases research training and workforce development for students at four partner Minority Serving Institutions, and lays the groundwork for project expansion to other MSIs and underserved institutions.  Additionally, the project team continues training and fostering mentoring opportunities for a diverse group of students, including women and minorities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1937181","Support for Core Research Activities and Studies of the Computer Science and Telecommunications Board","CNS","Information Technology Researc","10/01/2019","09/13/2023","Jon Eisenberg","DC","National Academy of Sciences","Continuing Grant","Darleen Fisher","09/30/2025","$2,497,635.00","","jeisenbe@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","CSE","164000","","$0.00","This Computer and Information Science and Engineering (CISE) Directorate special project supports part of the core operations of the Computer Science and Telecommunications Board (CSTB) of the National Academy of Sciences for the next 3 years. It also funds three CSTB studies of issues and technologies emerging in the field of computer science. The CSTB provides objective, nonpartisan, scholarly analysis, blending expertise in computer science, electrical engineering, telecommunications, and various applications and impacts of information technology (IT). CSTB's work is known and used by a wide and growing circle of agencies, officials, and experts in the field. CSTB has come to be seen as an authoritative source on IT trends, IT policies, and their implications for broader public policy. CSTB delivers its reports and provides briefings to key decision-makers and practitioners in the research, education, and policy-making communities. Its study groups engage significant numbers of women, minorities, and people from across the country. Core support enables it to plan for, oversee, and follow up on projects and to be forward-thinking and proactive in formulating new activities. <br/><br/>This project provides a source of strategic thinking for researchers, educators, and research funding agencies on research and education needs, computing and communications trends and issues, and their implications for broader public policy.  Through its membership of national leaders and industry, CSTB engages experts from CISE research and technical communities in mixed collaborative groups, developing ideas, explanations, assessments, and recommendations reflecting diverse points of view and rigorous peer review.  The CSTB studies produce reports that represent the consensus views of an expert committee and leverage deep insights into computing and communications technologies, including future directions and insights from other related fields.  CSTB is a focal point for engaging the research community in the development of new research agendas and in raising the level of policy debate.  Core support enables the CSTB to plan, oversee, and follow up on projects and studies and to continue to provide strategic leadership in an area of critical importance to the nation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2325469","Conference: Support for MERIF Workshop 2023","CNS","Information Technology Researc","05/15/2023","05/09/2023","Zongming Fei","KY","University of Kentucky Research Foundation","Standard Grant","Deepankar Medhi","04/30/2025","$99,605.00","","fei@cs.uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","CSE","164000","115Z, 7556, 8002, 9150","$0.00","Research infrastructure projects aim to develop advanced research facilities and provide broad access to research infrastructures to the community to conduct cutting-edge research. Over the years, multiple research infrastructure projects in NSF's Computer and Information Science and Engineering (CISE), which can be considered in the mid-scale range, have been awarded; they are in different stages of their life cycles. Some earlier funded projects (CloudLab, Chameleon, Colosseum) are relatively mature with a stable user base, eager to explore how to do cross-platform experimentation with newer projects. Other available platforms (such as FABRIC, SAGE, COSMOS, POWDER, AERPAW) have relatively newer components in the infrastructure and strive to expand their research user base. <br/><br/>The Midscale Experimental Research Infrastructure Forum (MERIF) plans to hold a MERIF Workshop on May 22-24, 2023 in Boston, MA, to bring together above-mentioned research infrastructure providers and research users to share experience and improve infrastructure to support science and engineering research. The workshop is intended to help recruit additional researchers who can make highest and best use of the current midscale research infrastructure facilities through raising community awareness, reducing barriers to use, sharing best practices, and facilitating cross-facility research. The workshop will also include training opportunities for students from diverse backgrounds to learn how to use various platforms to conduct research. Further information about the workshop is available at http://merif.org<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2131940","Mid-scale RI-1 (M1:IP): EduceLab: Infrastructure for Next-Generation Heritage Science","IIS","Mid-scale RI - Track 1, Information Technology Researc, CISE Research Resources, CYBERINFRASTRUCTURE, Info Integration & Informatics, EPSCoR Co-Funding","10/01/2021","09/11/2023","William Seales","KY","University of Kentucky Research Foundation","Continuing Grant","Sylvia Spengler","09/30/2026","$13,518,974.00","Suzanne Smith, Thomas Balk, Corey Baker, Hugo Reyes-Centeno","seales@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","CSE","108Y00, 164000, 289000, 723100, 736400, 915000","7364, 8812, 9102, 9150, 9251","$0.00","This project will create EduceLab, a data-centric instrument platform for Heritage Science (HS). HS applies the tools, techniques, and rigor of scientific measurement and inquiry to diverse heritage contexts, such as artifacts and curated collections, biological remains, and human-impacted environments. The proposed mid-scale instrument ecosystem is designed around key scientific capabilities that the research community has embraced as crucial and fundamental to addressing the challenging variability of HS contexts. The ecosystem includes materials characterization; advanced multimodal imaging (tomography, photography, photogrammetry); cyberinfrastructure and methodologies for capturing, structuring, and processing large-scale data sets; and mobile and flexibly-deployed instrumentation for in-situ data acquisition and initial evaluations. Viewed as a holistic ecosystem uniquely capable of addressing previously insoluble research questions in HS, EduceLab will be commissioned as operational clusters based on usage patterns that match the diverse HS communities and constituencies: laboratory equipment in a fixed, controlled environment for precise measurement and analysis (BENCH); mobile equipment that can be deployed on-site for collections and landscapes that cannot travel (MOBILE); and a protean environment for envisioning, building, and testing custom instrument configurations that address contexts impervious to laboratory analysis and on-site capabilities (FLEX). The clusters will be interconnected via cyberinfrastructure (CYBER) that powers efficient data flow through the entire infrastructure, from data acquisition to structured analysis, supporting the application of new techniques in data science and artificial intelligence. EduceLab will function as a novel Mid-scale Research Infrastructure ? a variable scientific lens ? for the rigorous, data-driven exploration of compelling heritage science research questions, engendering new discoveries as well as fundamental scientific advancement.  Headquartered within the University of Kentucky?s William S. Webb Museum for Anthropology, EduceLab will build cohesion across a diverse group of national and international stakeholders and constituents: trainees/students; researchers in HS and in areas where data acquisition activities overlap; practitioners at institutions with active heritage collections and connected research efforts; and community members with investments in the ongoing conservation, preservation, study, dissemination, and educational activities.<br/><br/>EduceLab in its organization will allow rigorous scientific exploration in the face of the unique challenges posed by natural and cultural heritage contexts. The EduceLab ecosystem will answer ongoing demands for agility and innovation in measurement, computational, and instrumentation solutions to these problems. Such advances will in turn continuously promote knowledge, growth, and innovation across a range of scientific disciplines involved in HS, including data science, computer science, computer vision, imaging science, systems engineering, chemical and materials engineering, anthropology, and cyberinfrastructure. The proposed instrumentation strongly aligns with the National Science Foundation?s ?Big Ideas? initiatives, including: Growing Convergence Research (EduceLab and HS is a convergence activity in itself, blending a number of scientific disciplines and relying on robust collaborations for success); Harnessing the Data Revolution (EduceLab activities will be data-driven and data-intensive, relying on emerging computational tools at play across disciplines like chemistry, physics, and computer science to capture the essence of objects and their environments); and NSF INCLUDES (the EduceLab ecosystem combines STEM with heritage fields in a culturally informed way to strategically grow the interest and participation of underrepresented groups in STEM fields and increase societal public literacy and engagement with science and technology.).<br/><br/>This project is jointly funded by Mid-Scale Research Infrastructure-Track 1 program, Information and Intelligent Systems, the Established Program to Stimulate Competitive Research (EPSCoR), and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1531330","MRI: Development of a Solar UAV Instrument","CNS","Major Research Instrumentation, Information Technology Researc, Special Projects - CNS, CSR-Computer Systems Research, IIS Special Projects","10/01/2015","02/16/2024","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Deepankar Medhi","03/31/2025","$1,604,398.00","Jiarong Hong, Miki Hondzo, Fotis Sotiropoulos, Demoz Gebre-Egziabher","npapas@cs.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","CSE","118900, 164000, 171400, 735400, 748400","1189, 9251","$0.00","This project, developing a high efficiency solar power enabled aerial instrument that incorporates sensory and processing requirements into the design methodology, focuses on the development of a (robotic) instrument that fills a niche in certain applications (i.e., small scale solar powered Unmanned Aerial Vehicles (UAVs)). The work involves the creation of a platform for real-world information gathering applications with special focus on robust navigation, distributed sensing, and collaborative scenarios. In particular, the development of a small scale solar powered UAV provides a robotic platform capable of communication, sensory coverage, and flight endurance unattainable through traditional UAV design. Existing UAVs utilizing solar power are constrained to large aircraft designs requiring a traditional runway for takeoff and landing. In contrast, electric powered small scale UAVs suffer from minimal flight time. Additionally, aerial robots provide significant advantages over ground based systems as they are unaffected by terrain and obstacles, and provide an information-rich vantage point from the air. Conventional aerial robots are significantly limited by their flight time and therefore their deployment is severely restricted. Flight time has been the central limitation for airborne sensory information and has prevented experimental research and real-world applications from being performed. The development of a high efficiency aircraft that leverages solar energy as a resource provides a solution to the flight time problem.<br/><br/>The methodologies mentioned involve the incorporation of hierarchical planning methods that include high-level reasoning for the optimal number of UAVs/sensors required, and low-level reasoning for efficient sensor placement throughout the environment. Specifically, the use of solar powered UAV platforms will enable work in the areas of precision agriculture and environmental science, requiring strong demand for high endurance systems capable of supporting a variety of sensor and measurement units. (For example, timely and repetitive relevant information regarding crop health is necessary for corrective actions to be implemented.) Additionally, collaborative operation of dynamically placed sensor platforms and devices can only be enabled through the use of small solar powered airplanes like the ones to be implemented. The instrument that this project funds enables work in application areas that require continuous and repetitive operation such as Energy, Environment, Agriculture, etc. Consequently, this work addresses the<br/>- Development of a small scale solar powered platform that is capable of multi-day flight, <br/>- Experimental validation of the scaled down solar UAV instrument, <br/>- Long-term solar powered flight planning based on sensory data collected, and<br/>- Creation of benchmarks that will allow comprehensive evaluation of the small solar powered UAV instrument."
"2118285","HDR Institute: HARP- Harnessing Data and Model Revolution in the Polar Regions","OAC","HDR-Harnessing the Data Revolu, ANT Integrated System Science, Polar Cyberinfrastructure, CYBERINFRASTRUCTURE, Info Integration & Informatics, Data Cyberinfrastructure","01/01/2022","08/31/2023","Maryam Rahnemoonfar","MD","University of Maryland Baltimore County","Cooperative Agreement","Sylvia Spengler","12/31/2026","$13,731,122.00","Shashi Shekhar, Jianwu Wang, Mathieu Morlighem, Aneesh Subramanian, Jan Lenaerts","maryam@lehigh.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","CSE","099Y00, 529200, 540700, 723100, 736400, 772600","062Z, 1079, 5294, 7231, 7364, 7556, 9102, 9251","$0.00","Climate-change induced loss of polar ice sheets impacts many lives and increases coastal flooding by rising sea level and affecting ocean circulation. However, it remains difficult to accurately predict how quickly the ice sheets will continue to shrink. In particular, we are still challenged by a limited understanding of transdisciplinary processes that determine ice sheet change, such as the role of subglacial topography and ice-atmosphere-ocean interactions. Timely investment in machine learning and data intensive research can revolutionize the way that scientists currently answer questions related to ice dynamics. This HDR Institute serves as a research hub where experts in data science, Arctic and Antarctic science, and cyberinfrastructure in academia, government, and private sectors come together to develop transformative and integrative data science solutions to reduce uncertainties in projecting future sea-level rise and climate change. i-HARP researchers investigate the potential of novel physics-aware data science and machine learning approaches to address national priorities and challenges on Navigating the New Arctic, climate change, and sea-level rise.<br/><br/>The HDR Institute aims to harness massive heterogeneous, noisy, and discontinuous data in space and time and integrate data with numerical and physical models. Researchers at i-HARP are investigating novel data science techniques including deep generative adversarial networks, graph neural networks, meta learning, hybrid networks, physics-informed machine learning, causal artificial intelligence, data assimilation, spatiotemporal deep learning, and scalable algorithms. Due to the fundamental nature of data science problems that i-HARP addresses, the solutions can be translated to other disciplines such as remote sensing, medicine, and autonomous driving. Moreover, the convergence team champions multiple clusters of research-integrated educational initiatives, with a specific focus on facilitating cross-disciplinary collaborations, training next-generation multi-disciplinary researchers and engaging the public in scientific inquiry as related to climate change and data science. In partnership with related communities, i-HARP designs curricula, and offers hands-on community workshops, lecture series, conference tutorials, and training. i-HARP engages students from underrepresented minority groups by leveraging several existing organizations for underrepresented minorities.<br/><br/>This project is part of the National Science Foundation's Big Idea activities in Harnessing the Data Revolution (HDR).  This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Section for Antarctic Sciences and the Section for Arctic Sciences within the NSF Office of Polar Programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2233872","Collaborative Research: EAGER: Towards a Design Methodology for Software-Driven Sustainability","OAC","Information Technology Researc","09/01/2022","08/17/2022","Mehdi Mirakhorli","NY","Rochester Institute of Tech","Standard Grant","Varun Chandola","08/31/2024","$100,000.00","Callie Babbitt","mehdi23@hawaii.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","CSE","164000","7916, 9102","$0.00","With the proliferation of computing technologies in our society, software plays an increasingly prominent role in contributing to and solving sustainability challenges. Despite its importance, however, sustainability remains a poorly understood concept among developers, with a general lack of tools, knowledge, and techniques that can be used to design and validate software systems that account for sustainability. This project aims to elevate sustainability as a first-class quality attribute of software that can be explicitly analyzed and designed for and to empower developers with methods for building sustainability into their systems. In addition, this project contributes novel design strategies and patterns that can influence the behavior of users towards sustainable use of computing products. The outcome of this project lays a foundation for further research in software engineering techniques and methodologies for sustainable computing.<br/><br/>To develop an in-depth understanding of sustainability challenges in software engineering, the first step in this project involves an empirical investigation of software applications in emerging, sustainability-relevant domains such as cyber-physical systems, IoT and mobile systems. Next, based on the outcome of this study, the project employs well-established requirements engineering methods to define sustainability as a collection of quality attributes, scenarios, and metrics. Finally, building on this definition, the project develops a catalog of design strategies and patterns that developers can use to build sustainability into their system as an explicit goal. This project also aims to establish a community of researchers in software engineering and other disciplines to encourage increased future activities in sustainability research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201435","CC* Compute: GPU HPC Cluster Partition for Research, Education, and Student Success","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","05/15/2022","08/09/2023","Stephen Wheat","OK","Oral Roberts University","Standard Grant","Amy Apon","04/30/2025","$431,382.00","Andrew Lang, Jan Woerner, Julianna Goelzer","swheat@oru.edu","7777 S LEWIS AVE","TULSA","OK","741710003","9184957547","CSE","808000, 915000","9150, 9251","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Oral Roberts University aims to deploy a GPU-based server partition called Eli, and integrate it with Titan, a campus wide resource. Eli will address the GPU and storage needs of computing for research, research training, and education. The proposed hardware is 6 GPU Compute Nodes with 2 Intel 32-Core Xeon Gold CPUs having 16 x 32G DRAM, 4 NVIDIA Tesla A30 GPUs, 1 SATA SSD boot drive (960GB) and 2 NVMe Solid State Drives (4TB each). It also includes a meta data Server, storage server, and NFS node.<br/><br/>This project addresses two areas of improvement in the Oral Roberts University (ORU) Research Computing and Analytics (ORCA) facility, providing GPU resources and adding much needed parallel file system (PFS) resources. These new resources are deployed as a third cluster partition, named Eli, in ORCA?s existing Titan cluster.  Along with Eli?s 100/200Gb HDR InfiniBand fabric, the Eli PFS supports native communication from each of Titan?s other partitions with 100GbE OmniPath and 40GbE QDR InfiniBand. Eli?s GPU nodes has potential to address a local and regional void of GPU capacity for traditional HPC codes (especially VASP and GAMESS) as well as emerging Data Science codes. VASP and GAMESS consume many core hours. The GPUs will accelerate these efforts while freeing previously used resources to other applications. With ORU?s Data Science degree concentrations, nearly half of the new ORU research efforts are Data Science related, driving demand for tools such as TensorFlow and Natural Language Processing tools. Eli?s PFS provides the much-needed capacity and performance to support the existing and emerging science drivers. While serving ORCA?s current users, Eli?s augmentation of Titan provides new researchers additional motivation to embrace HPC/AI methods.  Titan/Eli is freely available for use by all Oklahoma academic institutions.  The ORCA management team actively evangelizes HPC/AI to nascent researchers and provides opportunities for substantial undergraduate research experiences. Success is measured by system utilization, availability, and increased researcher participation. <br/><br/>This project aligns with ORU?s NSF CC*-funded 100GbE connectivity to the OneOklahoma Friction Free Network (OFFN), a science DMZ for 19 Oklahoma academic institutions and is funded through the collaborative efforts of the Office of Advanced Cyberinfrastructure (OAC) and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346681","CC* Networking Infrastructure: FAN4Science: Flow-Aware Networking for Data-Intensive Science","OAC","Campus Cyberinfrastructure","04/01/2024","04/09/2024","Shafaq Chaudhry","FL","The University of Central Florida Board of Trustees","Standard Grant","Kevin Thompson","03/31/2026","$649,999.00","Murat Yuksel, Fahad Khan, Matthew Hall, Sheila Amin Gutierrez de Pinere","shafaq.chaudhry@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","CSE","808000","","$0.00","Democratizing access to local, regional, and national supercomputing facilities is essential for researchers to meet their increasingly data-intensive science objectives. Furthermore, research is exceedingly collaborative with national or global research teams studying phenomena like global pandemic, climate change, smart cities, and exploring the universe's secrets. These objectives can be achieved through high-speed connectivity to powerful computing platforms through the interconnected web of research and education networks. The FAN4Science project brings together experts in network engineering, campus IT, and regional network providers to address research and development challenges of enabling a high-throughput and flexible network for University of Central Florida (UCF)'s researchers. <br/><br/>The scientific merit of this project is to explore the use of flow-aware networking methods to increase effective capacity of a campus network for researchers while also enabling innovative research in computer networking. The project (a) builds a 100Gbps research network to enable targeted research labs to get high-speed access to UCF's advanced computing facility; (b) installs performance monitoring and data transfer nodes at key locations to monitor research data movement; (c) augments the campus network with a Software-Defined Networking (SDN) setup for UCF research and teaching activities; and (d) enables experimental SDN research, utilizing real traffic, that explores speculative flow installations to reduce end-to-end latency. The project offers research opportunities to UCF graduate students and hands-on learning experience with installing real networking and performance monitoring equipment. Lessons learned have the potential to evolve into an SDN-controlled campus network design that can handle regular campus traffic and research traffic.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1919667","MRI: Development of ACCORD, a Community Cyberinstrument for Broadening Access to Research on Sensitive Data","OAC","Special Projects - CNS, CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","10/01/2019","06/15/2020","Ronald Hutchins","VA","University of Virginia Main Campus","Standard Grant","Kevin Thompson","09/30/2024","$3,770,656.00","Scott Midkiff, Masha Sosonkina, Thomas Cheatham, Deborah Crawford","rrh8z@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","CSE","171400, 723100, 808000","1189, 9251","$0.00","This 11 public university collaboration in Virginia develops ACCORD, an innovative research computing cyberinstrument to support the storage, access, and usage of sensitive data for researchers at participating institutions across the State. Beyond simply securing research data, the ACCORD cyberinstrument provides customizable secured research environments that can be integrated into each institution's workflows and security models to assure compliance with relevant laws and regulations applicable to specific data types. Through ACCORD, researchers across the State of Virginia have access to new research computing capabilities not previously possible (or not easily accessible), enabling them to undertake projects that require complex protection of sensitive data. In addition to enabling new research, ACCORD advances important dimensions of research such as reproducibility and data accountability. ACCORD is also a new model for collaboration, providing a common platform for researchers at multiple institutions and in different disciplines to securely share data and analytics.<br/><br/>In addition to driving exciting scientific discoveries with strong societal impact, the Virginia ACCORD program prioritizes research training and education in data security and privacy for students and researchers across the State. The Virginia ACCORD Consortium comprises universities of different sizes and missions, including minority-serving and non-PhD granting institutions, and is an effective platform for outreach and broadening participation to underrepresented and underserved communities. Lessons learned from the ACCORD's new model of collaboration and sharing compute resources are also being disseminated to other research computing consortia and communities across the U.S.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346732","CC* Regional Computing: ORCA: Oregon Regional Computing Accelerator","OAC","Campus Cyberinfrastructure","05/01/2024","04/23/2024","Will Pazner","OR","Portland State University","Standard Grant","Amy Apon","04/30/2026","$993,210.00","Josef Dufek, Steven Corbato, Robert Keever, Gregory Anderson","pazner@pdx.edu","1600 SW 4TH AVE","PORTLAND","OR","972015508","5037259900","CSE","808000","","$0.00","The Oregon Regional Computing Accelerator (Orca) project establishes a shared computational resource available for use by students, faculty, and researchers at universities and colleges across the Oregon region. This project enhances the computational capabilities and broadens the scope of computation-based educational opportunities in Oregon. Participating institutions in Orca include Oregon's research universities, small liberal arts colleges, and regional universities. The Orca computing cluster is hosted at Portland State University, and is accessible to users throughout the region by means of broadband fiber connectivity provided by Link Oregon. In addition to providing a high-performance cluster, the Orca project also facilitates trainings and research symposia on data- and computation-enable science.<br/><br/>The project provides access to high-performance graphics processing unit (GPU)-accelerated computational resources, leveraging new and existing cyberinfrastructure investments at Portland State University. The cluster supports a diverse set of science drivers from among the participating institutions, with applications ranging from molecular systems biology to corpus linguistics to multiphase magma dynamics. Extramural usage and high-throughput computing are facilitated through the Open Science Grid and the Partnership to Advance Throughput Computing. The cluster is made easily accessible through a web-based Open OnDemand interface. Given the importance of GPU-based computing to a broad range of research areas, Orca strengthens Oregon's research and education in the fields of artificial intelligence, machine learning, data science, and computational science and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346664","CC* Campus Compute: Colgate Compute Acceleration for Research and Education","OAC","Campus Cyberinfrastructure","05/01/2024","04/23/2024","Niranjan Davray","NY","Colgate University","Standard Grant","Amy Apon","04/30/2026","$499,948.00","Anthony Chianese, Michael Loranty, Kyriakos Tsoukalas, Grusha Prasad","ndavray@colgate.edu","13 OAK DR","HAMILTON","NY","133461386","3152287457","CSE","808000","","$0.00","The Colgate Compute Acceleration for Research and Education (CCARE) project upgrades Colgate?s cyberinfrastructure and enhances the research capabilities of Colgate University by adding computational capability and increasing the network speed of Colgate?s supercomputer, thus enabling researchers to collaborate with other institutions and access advanced computational scientific software. Through the CCARE project, Colgate contributes computing resources to the Open Science Pool of the OSG, thus contributing to research advancements across diverse domains on a national level. CCARE has a broader impact on the New York six liberal arts consortium by fostering the exchange of research computing knowledge among its members, and on the New York Madison county public schools by leading and supporting STEM educational opportunities via Colgate?s supercomputer computational resources.<br/><br/>The CCARE project transforms Colgate?s research-DMZ network with the use of IPv6 and upgrades Colgate supercomputer?s networking infrastructure with latest InfiniBand technology, while adding state-of-the-art GPU-Compute designed for artificial intelligence tasks and training of machine learning models. In the era of generative artificial intelligence, the CCARE project enables an efficient and inclusive approach to scientific discovery, supporting unique and often interdisciplinary research requirements in various fields such as training machine learning models, analyzing biological datasets, studying virus mutations, studying organometallic compounds, analyzing satellite remote sensing data, and applying deep learning techniques in the environmental and social sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319181","Collaborative Research: FMitF: Track I: Knitting Semantics","CCF","FMitF: Formal Methods in the F, Information Technology Researc","10/01/2023","03/05/2024","Gilbert Bernstein","WA","University of Washington","Standard Grant","Pavithra Prabhakar","09/30/2026","$486,531.00","Adriana Schulz","gilbo@cs.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","CSE","094Y00, 164000","071Z, 8206, CL10","$0.00","What is the meaning of a program written to control an industrial knitting machine? Usually, we say two programs are equivalent (mean the same thing) if they compute the same output data for every given input. But manufacturing programs don?t take input and they manufacture objects rather than output data. This project studies when two programs to control knitting machines are equivalent in the sense that they manufacture equivalent garments or fabric. It does so by connecting the mathematical study of programming languages with the mathematical field of Knot Theory (a branch of Topology). In tandem with the basic theory, this project develops a new, more robust software stack for compiling knitting machine programs, and new design tools leveraging the theory and infrastructure. This project?s novelties are in providing the first precise definition of manufacturing programs for non-rigid objects, and in developing theory, compilers and tools on that basis. The project has the potential to improve the efficiency and robustness of machine knitting ? the only presently viable approach to whole-garment manufacturing not reliant on extensive manual labor. Such advances in textile production may eventually enable onshoring/localization of garment production, as well as more ecologically sustainable manufacturing through just-in-time production, customization, and waste reduction. <br/><br/>The project covers the research and development of three layers: a basic theory of knitting machine programs, a new compiler infrastructure, and new design tools exploiting and demonstrating these advances. The theory level consists of a formal (denotational) semantics for a machine-independent knitting machine language called Knitout. Programs denote knot theoretic diagrams. Investigators are developing a complete axiomatic characterization of this equivalence, a decision procedure for the equivalence (possibly in P, i.e., in polynomial time), and associated normal forms/optimization procedures. The compiler level consists of a number of scheduling, checking, and optimization passes operating on formalized knitout programs, and interoperability links to tools in the existing Knitout ecosystem. The design tool level consists of tools for creating and debugging complex, non-standard textile designs, as well as the design of user hints and scheduling controls to help expert users ensure efficient manufacturability of their designs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925764","CC* Team: SWEETER -- SouthWest Expertise in Expanding, Training, Education and Research","OAC","Information Technology Researc, CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2019","02/21/2024","Dhruva Chakravorty","TX","Texas A&M University","Continuing Grant","Kevin Thompson","06/30/2024","$1,432,000.00","JoAnn Browning, Timothy Cockerill, Emily Hunt, Diana Dugas, Lizely Madrigal","chakravorty@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","CSE","164000, 723100, 808000","9251","$0.00","The efficacy of an  interdisciplinary research team is frequently limited by a researcher's ability to draw together a cohort of collaborators with needed scientific expertise.  SWEETER: South West Expertise in Expanding Training, Education and Research is a network of resources, both training and personnel, that collaborate and foster cooperation across the boundaries of disciplines and institutions.  SWEETER unites not-for-profits, community colleges, minority serving institutions, research-intensive universities, and industry from multiple states to develop this research network.  The collaboration leverages expertise in several facets of computational sciences to address long-standing issues encountered in sharing resources by employing a model of resource sharing where each institution is a provider and receiver of resources.<br/><br/>Research specialists from various sites will support domain scientists at site workshops, SWEETER annual events, and online avenues. To encourage these relationships, SWEETER offers a web-portal and shared cyberinfrastructure that will help remove resource barriers faced by community colleges and smaller institutions. SWEETER strengthens efforts for researcher preparation by offering remote and in-person training opportunities, and propagating elements of its learning environment to formal curricular efforts. Importantly, SWEETER employs an innovative evaluation strategy to assess its successes in the aspects of research, training and education. By fostering diverse partnerships developed through existing efforts, SWEETER will significantly broaden the involvement of traditionally underrepresented populations in computing at the K-12, undergraduate and graduate levels of learning. In all, SWEETER offers a flexible, portable and scalable framework to develop communities of practice that will inform the design of future research networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2238047","CAREER: A Holistic Developer-Centered Approach to Enhance Privacy for Data-Driven Applications","CCF","Information Technology Researc, Software & Hardware Foundation, Secure &Trustworthy Cyberspace, EPSCoR Co-Funding","07/01/2023","04/26/2024","Sepideh Ghanavati","ME","University of Maine","Continuing Grant","Sol Greenspan","06/30/2028","$261,813.00","","sepideh.ghanavati@maine.edu","5717 CORBETT HALL","ORONO","ME","044695717","2075811484","CSE","164000, 779800, 806000, 915000","025Z, 1045, 7944, 9102","$0.00","The number of privacy violations in the U.S. has increased dramatically, with many companies experiencing harmful and costly breaches. Such breaches negatively impact users and lead to financial and reputational costs for developers. Their adverse effects underscore the need for holistic privacy engineering solutions throughout the software development lifecycle. Recent privacy research has made progress in this regard, yet significant gaps remain, including insufficient implementation guidelines and ex-ante detection of privacy violations. This project addresses these gaps by investigating how novice and expert developers currently implement privacy rules, supporting the developers in detecting privacy behaviors, designing privacy-preserving solutions, and automating the implementation of privacy rules in code. This project will result in models and tools to enhance privacy for all types of software applications. Hence, the project will benefit society by (i) helping to protect the privacy of vulnerable groups, including Maine youth, migrants, and asylum seekers, (ii) supporting the software industry and the U.S. economy, and (iii) broadening access to STEM and computer science education and careers for girls, women, minorities, first-generation students, and other underrepresented groups. <br/><br/>This interdisciplinary project fundamentally advances knowledge in privacy and software engineering by investigating new theories, methods, and tools to describe software privacy behaviors prior to development and then ensure their effective implementation in code. Through three thrusts, the project will: (i) generate new scientific knowledge about the privacy awareness and expertise of developers and the challenges they face; (ii) reduce the effort of manually implementing privacy requirements in code via the development of novel privacy-related code generation models; (iii) minimize privacy violations through examining the current practices for writing privacy-related code and creating novel solutions to improve such practices; and (iv) strengthen communications between legal and technical experts by developing a shared infrastructure for defining and reasoning about privacy solutions from both technical and policy perspectives.<br/><br/>This project is jointly funded by the Software and Hardware Foundations (SHF) program, the Secure and Trustworthy Cyberspace (SaTC), and the the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2212296","Collaborative Research: CNS Core: Medium: Innovating Volumetric Video Streaming with Motion Forecasting, Intelligent Upsampling, and QoE Modeling","CNS","Information Technology Researc, Special Projects - CNS, CSR-Computer Systems Research","10/01/2022","08/30/2023","Bo Han","VA","George Mason University","Continuing Grant","Jason Hallstrom","09/30/2026","$262,781.00","","bohan@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","164000, 171400, 735400","044Z, 7924, 9251","$0.00","Volumetric video is an emerging type of multimedia content. Unlike traditional videos and 360-degree videos that are two-dimensional (2D), every frame in a volumetric video consists of a three-dimensional (3D) scene or object represented by a 3D model such as a point cloud or a polygon mesh. The 3D nature of volumetric video enables viewers to move with six degrees of freedom, leading to a truly immersive viewing experience. However, compared to conventional videos, streaming volumetric videos over the Internet faces unique challenges: a lack of precise motion tracking, extremely high bandwidth consumption, a lack of quality-of-experience (QoE) models, and complex interactions among network, computation, and motion. This project proposes a holistic research agenda addressing the above challenges through four core approaches: multi-dimension, multi-modality motion sensing and prediction; deep-learning-based content upsampling; comprehensive QoE modeling; and resource/motion adaptation. The project aims at demonstrable systems and networking research with a synergy among multimedia systems, networking, wireless sensing, machine learning, computer vision, and graphics. The research team will develop prototypes and evaluate them in real-world settings. <br/><br/>Internet video streaming is playing a key role in today's world, especially during the COVID-19 pandemic. As a key enabler of immersive telepresence, volumetric content will become popular in the near future. The techniques developed from this project will significantly reduce network resource usage and improve the user experience for volumetric content delivery over the Internet. This will benefit both producers and consumers of volumetric videos, as well as boost their applications in various domains such as education, telehealth, manufacturing, and entertainment, ultimately leading to a high impact on societies and economies. The project will also offer new education components, and provide a platform for various Broadening Participation in Computing (BPC) activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322260","CC* Regional Computing: Enhancing Computing At Regional Schools in the Rocky Mountain Advanced Computing Consortium (RMACC)","OAC","Campus Cyberinfrastructure","10/01/2023","04/21/2024","Shelley Knuth","CO","University of Colorado at Boulder","Standard Grant","Amy Apon","09/30/2025","$1,033,411.00","Jan Mandel, Mitchell McGlaughlin, Joel Sharbrough, Jarrod Schiffbauer","shelley.knuth@colorado.edu","3100 MARINE ST","Boulder","CO","803090001","3034926221","CSE","808000","","$0.00","This project to the University of Colorado and partners in the Rocky Mountain Advanced Computing Consortium (RMACC) expands the capacity of the Alpine Computing Cluster, a regional resource for RMACC.  The project expands capacity by more than 15% and triples the number of high-memory nodes.  A wide range of research is supported, including physics, astronomy, engineering, biology, and atmospheric science.  The project supports diverse workloads for the approximately 30 member institutions of RMACC.  Student engagement is strong, as ten students from across the RMACC region are part of a cohort working closely with the system administration team to learn how to navigate the design, procurement, deployment, and administration of the system.  The project supports researchers in EPSCoR jurisdictions, at Primarily Undergraduate Institutions, Hispanic Serving Institutions, and institutions primarily serving rural communities within RMACC.<br/><br/>As of February 2023, the Alpine cluster comprises 18,080 AMD Milan CPU cores as well as 60 GPU accelerators (36xNVIDIA A100?s, 24xAMD MI100?s). The 317 nodes are interconnected through 25Gb Ethernet, with standard compute nodes additionally featuring HDR100 InfiniBand links. Alpine is also equipped with a 1.9PB GPFS scratch filesystem. The expansion includes 32 nodes of 64C AMD Milan CPUs and 256GB of memory as well as eight nodes with 64C AMD Milan CPUs and 1TB of memory. It also includes two InfiniBand switches and associated HDR cables for high-throughput, low-latency networking. The expansion will provide an additional 1.3PB to Alpine?s high-speed scratch filesystem to meet the growing demand for data intensive computing research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201474","CC* PLANNING: RE-CONSTRUCTING THE CAMPUS CYBERINFRASTRUCTURE OF A SMALL, LIBERAL ARTS HBCU IN ORDER TO MAXIMIZE STEM INNOVATION AND INTEGRATION","OAC","Campus Cyberinfrastructure","05/01/2022","04/07/2022","Sohel Quazi","TX","Texas College","Standard Grant","Kevin Thompson","04/30/2025","$100,000.00","","squazi@texascollege.edu","2404 N GRAND AVE","TYLER","TX","757021962","9035938311","CSE","808000","","$0.00","Texas College is undertaking a planning activity allowing it to complete an assessment of its current infrastructure, to identify the elements needed to evolve campus cyberinfrastructure (CI) to better support STEM research and education, to understand and document campus-wide CI needs of researchers and students driving future CI improvements, and to produce an initial campus CI plan.<br/><br/>Impact of this planning grant extends beyond Texas College?s need for a carefully researched and prepared roadmap for technological infrastructure enabling a more advanced future. This activity also is an initial step toward improving the quality of the STEM and non-STEM education produced at Texas College. Alternate learning styles can be accommodated via technology. Students with STEM knowledge gaps can experience near-instant remediation. Culturally concordant examples can be tailored to reflect various sociocultural backgrounds within a classroom. Flight simulations can be used to teach Physics and Physical Sciences. With a sufficient cyberinfrastructure, major barriers are removed from the intellectual expansion in how teaching and learning occurs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322199","CC* Planning: Developing a Plan to Improve Research Computing at Christopher Newport University","OAC","Campus Cyberinfrastructure","08/01/2023","06/26/2023","William Phelps","VA","Christopher Newport University","Standard Grant","Kevin Thompson","07/31/2024","$99,966.00","Ryan Fisher, Samuel Henry","william.phelps@cnu.edu","1 AVENUE OF THE ARTS","NEWPORT NEWS","VA","236063072","7575947392","CSE","808000","","$0.00","The aims of this project are to develop and publish a five-year cyberinfrastructure plan for scientific computing at Christopher Newport University (CNU). The past decade has seen CNU dramatically increase its research capabilities and impact, however the computing resources used are often not accessible to the wider campus. A unified computing plan, which includes the creation of a scientific computing cluster, is needed to support CNU?s research programs and to provide resources to faculty across the university. In collaboration with IT services, we are identifying the computing resources necessary to meet research needs and estimating the infrastructure required to support those resources. The end goal of this project is to develop a plan for comprehensive scientific computing infrastructure at CNU that will support existing research, enable researchers to expand into new fields, and allow CNU to contribute to national cyberinfrastructure by providing new resources to the Open Science Grid.  <br/> <br/>CNU is a primarily undergraduate institution that has a significant focus on student involvement in research. Research areas that are heavy users of computational resources include experimental nuclear physics, gravitational wave astronomy, natural language processing, other applications of artificial intelligence to data science, and mathematical simulation.<br/>Through the improved cyberinfrastructure supported by this work, students will have significantly improved opportunities to engage in research that both expands the forefront of our understanding of science and explores the use of emerging technologies. Women and underrepresented students are specifically supported and encouraged to engage in these research opportunities through several CNU's initiatives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2144887","CAREER: SocioCulturally Competent Agents to Study and Improve Human-AI interaction","IIS","Information Technology Researc, HCC-Human-Centered Computing","05/01/2022","03/13/2024","Christopher Dancy","PA","Pennsylvania State Univ University Park","Continuing Grant","Todd Leen","04/30/2027","$656,531.00","","cld5070@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","CSE","164000, 736700","102Z, 1045, 7367, 9232","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Artificial Intelligence (AI) systems have become ubiquitous in society; people constantly interact and cooperate with AI systems to accomplish tasks. Whether it is interacting with an AI agent in an online virtual environment or deciding where to eat next with the help of a virtual assistant, AI systems are increasingly integrated into our everyday lives. With such wide-spread deployment, understanding how a person?s context affects their interaction with AI systems is vital to designing and developing competent and ethical AI systems. AI systems should be able to interact as appropriately with someone who is stressed or has been marginalized by societal structures as well as it does with someone free of stress or has not been subject to socio-cultural structures that systematically marginalize them. This research will advance the understanding of how one can create AI systems that can adapt to differences in people (and the social structures that might mediate these differences), while also advancing the understanding of how a person?s context affects the way they perceive and cooperate with an AI system. The research in this project is complemented with multiple educational thrusts, including a plan to develop a pre-orientation program that targets incoming first year undergraduates from traditionally marginalized groups. The program will help undergraduate students be critically reflective how their own context affects the way they might design, develop, and implement AI systems. <br/><br/>The goal of this work is to develop a process-based multilevel computational theory that describes how human socio-cultural knowledge can positively affect the design, development, and interaction with AI agents. The project aims to understand how one can develop competent AI agents that can account for differing socio-cultural perspectives and their effects. Products of this work include an extended computational cognitive architecture and new cognitive AI agents that will be useful for the development of other AI systems. These products will also be useful for general computational models that simulate human behavior. The project will result in a computational model of interactions between physiological, affective, and cognitive systems that modulate human behavior, as well as an account for socio-cultural knowledge that affords certain uses of these systems and processes during human-AI interaction. The computational model will connect areas related to human behavior and AI on several time scales to make the modeling, simulation, and study of human-AI interaction more tractable. In concert with the computational models and tools, the investigator will conduct studies that provide a deeper understanding of how socio-cultural perspectives and knowledge affects and is used by people while cooperating with AI agents during tasks. Results from these studies will inform the computational model. The work will result in a deeper qualitative and quantitative and under-standing of the processes and knowledge that mediate human-AI interaction, the development of more competent AI agents, and open-source computational tools to continue to expand upon this understanding.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2332841","EAGER: Revolutionizing Wikipedia?s Relationship with New and Emerging Knowledge","IIS","Information Technology Researc, HCC-Human-Centered Computing","09/01/2023","04/23/2024","Stevie Chancellor","MN","University of Minnesota-Twin Cities","Standard Grant","Ephraim Glinert","02/28/2026","$337,448.00","Loren Terveen","steviec@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","CSE","164000, 736700","1640, 7367, 7916, CL10","$0.00","Millions of people view Wikipedia?s articles daily, and many services rely on the quality of Wikipedia to power their business and AI ventures. This project will contribute new and validated, high-fidelity designs for the fundamental mechanisms Wikipedia uses to organize knowledge, with the goal of enriching the types of knowledge it includes and expand the evidentiary standards it employs. Wikipedia's success rests on a radical knowledge production process?anyone can edit the articles?coupled with a strict evidentiary epistemology that mandates ""reliable sources"" and a ""neutral point of view"". But along with the success has come systematic problems and controversies over what knowledge is ""valid."" These processes may have the effect of excluding much valuable knowledge, including (1) knowledge based on different ways of knowing, such as oral histories; (2) less established knowledge, including emerging knowledge about new medical treatments and therapies; and (2) knowledge directed toward different goals, for example personal experiences that contextualize textbook knowledge. These exclusions occur through the decisions of Wikipedia editors: which edits they allow and which they undo and what rules they enforce. <br/><br/>Research will fill the theory-practice gap in Wikipedia research via well-evaluated, high-fidelity prototypes that can change the evidentiary practices in the online encyclopedia. This goal will be accomplished through four phases of work: Phase 1: Translate theory and practice into low-fidelity prototypes for new knowledge representations. Phase 2: Validate initial designs with a small panel of Wikipedia experts. Phase 3: Develop the most promising ideas from Phase 2 into high-fidelity, functional prototypes. Phase 4: Evaluate the high-fidelity prototypes through an asynchronous online community for design critique and generation. Expected scientific outcomes of this project include new design prototypes and policies for Wikipedia that expand and enrich its knowledge-production epistemology, and expert and stakeholder insights into the potential feasibility, efficacy, and acceptability of the designs and policies. This multi-stage, iterative design process is applicable to other complex and risky arenas, such as designing interventions for credibility assessment in social platforms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209629","Collaborative Research: Elements: TRAnsparency CErtified (TRACE): Trusting Computational Research Without Repeating It","OAC","Data Cyberinfrastructure, Software Institutes","07/15/2022","04/21/2024","Lars Vilhuber","NY","Cornell University","Standard Grant","Sylvia Spengler","06/30/2025","$166,000.00","","lars.vilhuber@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","CSE","772600, 800400","077Z, 7923, 8004, 9251","$0.00","Research communities across the natural and social sciences are increasingly concerned about the transparency and reproducibility of results obtained by computational means. Calls for increased transparency can be found in the policies of peer-reviewed journals and processing pipelines employed in the creation of research data products made available through science gateways, data portals, and statistical agencies. These communities recognize that the integrity of published results and data products is uncertain when it is not possible to trace their lineage or validate their production. Verifying the transparency or reproducibility of computational artifacts?by repeating computations and comparing results?is expensive, time-consuming, and difficult, and may be infeasible if the research products rely on resources that are subject to legitimate restrictions such as the use of sensitive or proprietary data; streaming, transient, or ephemeral data; and large-scale or specialized computational resources available only to approved or authorized users. The TRACE project is addressing this problem through an approach called certified transparency - a trustworthy record of computations signed by the systems within which they were performed. Using TRACE, system owners and operators certify the original execution of a computational workflow that produces findings or data products. By using a TRACE-enabled system, researchers produce transparent computational artifacts that no longer require verification, reducing burden on journal editors and reviewers seeking to ensure reproducibility and transparency of computational results. TRACE presents an innovative and efficient approach to ensuring the transparency of research that uses computational methods, is consistent with the vision outlined by the National Academies, and enables evidence-based policymaking based on transparent and trustworthy science.<br/><br/>The central goal of the TRACE project is the development, validation, and implementation of a technical model of certified transparency. This includes a set of infrastructure elements that can be employed by system owners to (1) declare the dimensions of computational transparency supported by their platforms; (2) certify that a specific computational workflow was executed on the platform; and (3) bundle artifacts, records of their execution, technical metadata about their contents, and certify them for dissemination. The first phase of the project focuses on the development of a conceptual model and technical specification that can be used to certify the description of a system, termed a Transparency-Certified System (TRACE system), and the aggregation of artifacts along with records of their execution, termed Transparency-Certified Research Objects (TROs).  The second phase focuses on the development of reusable software components implementing the TRACE model and approach. To demonstrate certified transparency, the toolkit is used to TRACE-enable existing platforms including Whole Tale, SKOPE, and the SLURM workload manager. These TRACE-enabled systems produce certified TROs that can be trusted and do not need to be repeated or re-executed to verify that results were obtained as claimed.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Social and Economic Sciences within the Directorate for Social, Behavioral and Economic Sciences; and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346318","CC* Planning: South Florida Regional Data Science Cyberinfrastructure","OAC","Campus Cyberinfrastructure","05/01/2024","04/21/2024","Ravi Vadapalli","FL","University of Miami","Standard Grant","Amy Apon","04/30/2025","$99,990.00","Julio Ibarra, Samuel Darko, Jason Ball","rxv441@med.miami.edu","1320 SOUTH DIXIE HIGHWAY STE 650","CORAL GABLES","FL","331462919","3052843924","CSE","808000","","$0.00","South Florida (in the proximity of 50 miles from Miami) is home to 16 higher education institutions with a combination of public and private models across two- and four-year education settings. Many of them are small, minority-led, minority-serving, underrepresented, or under-resourced institutions of higher education. This project facilitates, engages, and assists minority institutions in South Florida in assessing and developing strategies to build sustainable campus cyberinfrastructure and catalog future expertise needs in data science as a part of building regional cyberinfrastructure.<br/><br/>This collaborative project across Florida Atlantic University, Florida International University, Florida Memorial University, and University of Miami together with Bethune-Cookman University, Nova Southeastern University, and Miami Dade College is hosting three workshops to capture current and future expertise needs in science and engineering, strategies for creating sustainable campus cyberinfrastructure, and the science drivers. <br/><br/>The University of Miami, in collaboration with Florida International University and Florida Atlantic University, is providing research facilitation, computing resources, and application support services to evaluate and benchmark science drivers. This effort enhances practical understanding of current and future expertise needs in science and engineering and sustainable campus cyberinfrastructure requirements among participating institutions.  Partnerships with existing CC* programs such as MS-CC and Florida regional organizations such as Sunshine State Education and Research Computing Alliance will help expand the outreach and impact of the project outcomes across other institutions in South Florida.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2131987","Mid-scale RI-1 (M1:DP): Designing a global measurement infrastructure to improve Internet security","OAC","Mid-scale RI - Track 1, Special Projects - CNS, CYBERINFRASTRUCTURE, CCRI-CISE Cmnty Rsrch Infrstrc, Campus Cyberinfrastructure","10/01/2021","07/18/2023","Kimberly Claffy","CA","University of California-San Diego","Continuing Grant","Kevin Thompson","09/30/2024","$7,897,527.00","David Clark, Bradley Huffaker","kc@caida.org","9500 GILMAN DR","LA JOLLA","CA","920930021","8585344896","CSE","108Y00, 171400, 723100, 735900, 808000","8812, 9102, 9251","$0.00","While the Internet has become critical infrastructure permeating all aspects of modern society, its security and trustworthy character are subject to constant threats and attacks.  Any enterprise or service can have its traffic deflected to a masquerading site that attempts to mimic the legitimate site, steal user credentials, disrupt security, or defraud users.  The security of the Internet infrastructure is a high priority for the security research community, but that community is greatly hindered by a lack of relevant data.  The goal of this project is a validated design for a transformative infrastructure to support collection, curation, archiving, and expanded sharing of data needed to advance critical and stunted scientific research on the security, stability, and resilience of Internet infrastructure.<br/><br/>The measurement infrastructure design effort will focus on the Internet transport layer: routing, domain naming, addressing, and key management (the Certificate Authorities).  Community workshops and interactive prototyping efforts will enable evaluation of a proposed design of a measurement infrastructure to acquire and share such security-relevant data at a macroscopic scale.  Due to the complexity, volume, and sensitivity of resulting data, the design will cover many facets of data management: curation, post-processing analytics, privacy preserving data analysis frameworks, data discovery and accessibility, and normalized data sharing agreements. To support use of and feedback on prototypes, the PIs will develop an on-line course on Network Infrastructure Data Science, that will facilitate responsible (ethical, privacy-respecting) use of the datasets and analytics, and STEM/cybersecurity workforce training. This design project will enable the application of data-intensive methods to data from the global Internet infrastructure, overcoming a key barrier to scientific and engineering advances to navigate current and future Internet-related harms. The project will contribute to a broad range of disciplines that now depend on data about the Internet, including network science, socioeconomic studies, international relations, and political science. The resulting capabilities to support data acquisition, curation, and sharing will have an inherent equalizing effect on the research community. Moreover, if successful, this cyberinfrastructure will increase the trustworthiness of the Internet for U.S. citizens, and serve as a model for rest of the world.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2223515","IMR: MT: Tools for Programming Distributed Data-plane Measurements","OAC","Networking Technology and Syst, Campus Cyberinfrastructure","09/01/2022","09/25/2023","David Walker","NJ","Princeton University","Standard Grant","Rob Beverly","08/31/2024","$600,000.00","Jennifer Rexford, Hyojoon Kim","dpw@cs.princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","736300, 808000","115Z, 7363","$0.00","Understanding the flow of traffic across key networks---what it is composed of and how it changes---is critical for improving modern information services. Traditionally, however, it has been difficult for researchers to develop new tools for dissecting this traffic and analyzing its characteristics, while taking care to maintain user privacy.  Recently, though, the development of relatively cheap programmable switches has made it possible to develop diagnostic tools and place them directly inside the network, on the path through which traffic flows. In such a position, new tools have the potential to see all the internet traffic as it flows by, from a university campus to the broader internet, for instance, or along a corporate wide-area network or data center.   Unfortunately, while it is possible to develop such tools, doing so is currently an incredibly difficult and error-prone process. To ameliorate this situation, the research team will develop Lucid, a new programming language and system that will facilitate the process of developing, debugging, and deploying network measurement tools in live programmable networks. The research team will deliver a compiler that translates high-level Lucid programs into lower-level code that execute in multiple places---directly on programmable switches, or in support, on servers connected to the network in question. In addition, the team will deliver a collection of reusable components that network measurement researchers can plug together to get started on a new idea quickly.  To help teach researchers how to use the new language, the team is developing tutorials for major conferences in networking.   To summarize, this project will impact the performance, reliability, and security of critical networks by facilitating the development of new measurement tools that can discover network optimization opportunities, detect failures, and rapidly recognize attacks that disrupt online services.<br/><br/>Traditional measurement tools and datasets, while incredibly useful, have significant limitations in scale and coverage. Measurement researchers should capitalize on the exciting advances in programmable data planes to analyze Internet traffic and performance as packets traverse the network. Analyzing traffic directly in the data plane (e.g., network switches, routers) enables sophisticated analysis without sacrificing efficiency or divulging sensitive user information, and enterprise networks, such as university campuses, provide an excellent opportunity to use these programmable data planes in practice. However, programming the data plane is not easy. Existing languages, such as P4, are very low-level, have an extremely steep learning curve, and are notoriously difficult to work with (with seemingly legitimate programs often failing to compile). This project addresses these pain points by delivering new programming support in the form of Lucid, a high-level language designed to support cooperative measurement across multiple locations and device types.  More specifically, the research team is developing compilers that will target both Intel Tofino programmable switches (via P4) and software servers (via eBPF).  Using both kinds of devices, researchers will be able to develop and deploy a range of different kinds of distributed measurement tools.  The research team will also develop an interpreter for the language so that interesting new research ideas may be developed and debugged prior to deployment.  The infrastructure developed by the research team will also include a suite of libraries that implement key data structures and utilities useful in network measurement and in support of data privacy.  To teach the community how to use our language, libraries, tools, and infrastructure, the team will develop documentation and tutorials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2144208","CAREER: Incentives, Fairness, and Efficiency without Monetary Transfers","CCF","Information Technology Researc, Algorithmic Foundations","07/01/2022","04/18/2024","Christos Psomas","IN","Purdue University","Continuing Grant","Peter Brass","06/30/2027","$378,512.00","","apsomas@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","CSE","164000, 779600","102Z, 1045, 7924, 7926, 7932, 9251","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>The Internet and the vast increase in the availability of data have transformed algorithm design, as well as computer science in general. From computational resources and advertising space, to food donations, loans, kidneys, and vaccines, algorithms are increasingly being used to decide how scarce resources are allocated. As opposed to traditional optimization, the input to these algorithms must be solicited from strategic agents, with their own, private preferences over the algorithm's output.  And, from resource allocation in the cloud and spectrum auctions to tournaments in major sporting events (such as the Olympics), it is well-understood that these strategic entities will behave to optimize their own benefit to the extent possible while still ?following the rules,? leading to unpredictable final outcomes. At the same time, unlike traditional optimization, in many of these modern applications, system designers must also consider whether their system is equitable among its participants. Classic work in Economics, as well as extensive work in the intersection of Computer Science and Economics, provides a rich toolkit for designing algorithms that are immune to strategic manipulations as well as algorithms that balance fairness and efficiency. This project aims to advance and develop this theory with a focus on domains where monetary transfers are not allowed, by taking aim at several fundamental open questions. The project also contains plans to design, develop, and deploy a system that is based on the proposed theoretical research and that serves the local community by enabling local non-profit organizations that fight hunger to allocate their food donations in a more efficient manner.<br/><br/>The project will expand the reach of theory into areas where there is a major gap in current understanding, by taking aim at several key theoretical questions in the following three complementary thrusts. (1) Foundations of mechanism design without money. The project takes aim at fundamental questions in this space, with the goal of developing tools for designing truthful mechanisms for a number of paradigms: divisible and indivisible goods, static and dynamic environments, worst-case and Bayesian objectives. (2) Mechanism design with imperfect rationality. There are important domains in which protecting against fully rational, expected utility-maximizing participants is overly cautious. This project puts forward and explores several possibilities for modeling imperfect rationality. (3) Mechanism design with imperfect expressivity. Eliciting complex utility functions is often infeasible, e.g., because of agents' cognitive limitations. The project explores the trade-off between expressiveness and ease of elicitation in mechanism design without money.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2008680","CAREER: Collaboratively Perceiving, Comprehending, and Projecting into the Future: Supporting Team Situational Awareness with Adaptive Multimodal Displays","IIS","Information Technology Researc, HCC-Human-Centered Computing","08/12/2019","07/27/2022","Sara Riggs","VA","University of Virginia Main Campus","Continuing Grant","Dan Cosley","06/30/2025","$527,214.00","","sriggs@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","CSE","164000, 736700","1045, 7367","$0.00","Especially in data-rich and rapidly changing environments, effective teams need to give members the information needed to develop awareness of their own, their teammates', and the overall team's current situation.  However, attentional demands are high on such teams, raising questions of how to both monitor those attentional demands and develop systems that adaptively provide needed information not just through visual displays that are often overloaded, but through other senses including touch and sound.  Most existing work on adaptive multimodal interfaces for situational awareness focuses on individuals; this project will address how to do this work for teams, using unmanned aerial vehicle (UAV) search and rescue as its primary domain.  This includes developing conceptual models that connect individual and team-level situational awareness, algorithms that use eye gaze data to assess both situational awareness and workload in real-time, and multimodal display guidelines that adaptively present information to the most appropriate team members through the most effective modes.  This work will fundamentally advance research on understanding and designing to support team interaction, leading to practical improvements in a variety of safety-critical domains.  The project also has a significant educational component, providing research opportunities for both graduate and undergraduate students and conducting design activities aimed at outreach and broadening participation in STEM disciplines, including workstation design to support teams of people with disabilities in manufacturing contexts.<br/><br/>The research work has two main thrusts.  The first involves collecting baseline data through a study where pairs of novices are trained to carry out simulated UAV search and rescue tasks using a standard visually-focused interface; the team will collect situational awareness (SA) assessments using existing validated surveys, eye gaze data, and team interaction data and member characteristics.  This data will be used to build two main models.  The first is a model that relates team dynamics and individual member characteristics with levels of SA and performance, using qualitative analysis of recorded observational and audio data, along with focus group interviews with participants.  The second is a quantitative model that attempts to predict SA using eye gaze data, using both a factor analysis of eye gaze data and Markovian models of how teams and their members transition their visual attention between interface elements and tasks to predict levels of SA.  These models will support unobtrusive assessments of SA that avoid the interruptions imposed by existing surveys and are necessary for developing the adaptive multimodal interfaces that are the other main thrust of the project.  This second thrust will use the models of attention and problematic tasks and contexts identified in the first study to iteratively develop a pilot suite of multimodal interfaces that combine visual, audio, and tactile information channels.  These multimodal interfaces will be evaluated using a series of studies similar to the first set, with the goal of developing cost-benefit models for presentation modes and types of information that minimally interfere with teams' existing visual workload while still providing information that raises individual and team SA.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925704","CC* Networking Infrastructure: A High-Performance Science DMZ and Dedicated Research Network for Duquesne University","OAC","Campus Cyberinfrastructure","07/01/2019","05/31/2023","Sheryl Reinhard","PA","Duquesne University","Standard Grant","Kevin Thompson","06/30/2024","$462,680.00","Rehana Leak, Demerit McCarthy, Donald Moskiewski, Brad Maloney","reinhard@duq.edu","600 FORBES AVENUE","PITTSBURGH","PA","152820001","4123961537","CSE","808000","","$0.00","Duquesne University is implementing a dedicated research network and Science DMZ for the facilitation of science-driven research, education and collaboration. A vibrant and growing Duquesne research community conducts leading edge research and scientific experimentation in a broad range of science domains. The campus network is unable to support the data-intensive workflows generated by the research community. While science drivers and research workflows vary, there is a common need for fast and unrestricted data movement to internal and external research computing resources. The research-focused Science DMZ cyberinfrastructure addresses the limitations of the general purpose network by establishing a dedicated, scalable and secure network that is optimized for data-intensive science workflows. <br/> <br/>The project objective is to establish a dedicated 120Gbps Science DMZ network to support the Duquesne research community. A critical element of the Science DMZ is a Data Transfer Node (DTN). The DTN will provide University researchers with a secure, highly-available, high-performance data exchange point to facilitate the secure and efficient sharing of research data with collaborating institutions and national computational research facilities. The Science DMZ will connect to a dedicated 10Gbps uplink to the Pennsylvania-wide education and research computing network (KINBER).<br/> <br/>The new friction-free cyberinfrastructure not only accelerates large scale data movement, it facilitates new and more efficient workflows resulting in the acceleration and advancement of scientific discovery. The research network improves opportunities to train the next generation of research scientists and to inspire underrepresented groups to participate in scientific research activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322377","CC* Data Storage: FASTER Data Infrastructure to Accelerate Computing","OAC","Campus Cyberinfrastructure","09/01/2023","08/21/2023","Lisa Perez","TX","Texas A&M University","Standard Grant","Kevin Thompson","08/31/2025","$499,527.00","Theodora Chaspari, Zhe Zhang, Honggao Liu, Dhruva Chakravorty","perez@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","CSE","808000","","$0.00","Researchers using computational modeling and data-analytics approaches require access to cutting-edge Cyberinfrastructure (CI) that is coupled with high-performance storage infrastructure. To address this growing need, the shared, parallel, high-performance, and high-capacity storage system is coupled to the state-of-the-art National Science Foundation (NSF) funded Launch and composable FASTER computing clusters for use across 11 universities in The Texas A&M University System (TAMUS). Connected over a state-wide high throughput network, the storage infrastructure offers researchers with seamless access to cutting-edge computing accelerators, support for large datasets, accessible software frameworks, interactive computing approaches, web-applications for advanced computing, and a comprehensive researcher-training program. Research benefiting from this open-source storage system includes: artificial intelligence and machine learning, cybersecurity, health population informatics, bioinformatics, materials design, climate modeling, quantum computing architectures, biomedical imaging, geosciences, and workflows for data collected on instruments such as cryogenic electron microscopes.<br/><br/>With an enrollment of over 150,000 students in the TAMUS, the storage system will support researchers at ten emerging research institutions, that includes 6 Hispanic Serving Institutions and the oldest Historically Black College University in Texas. The growing CI ecosystem will foster burgeoning programs in Data Sciences, facilitate new research collaborations around CI, and help build a cohesive vision for regional computing. The national research community will benefit through 20% of the storage space committed as part of the federated data sharing fabric PATh/OSDF, allocations on the FASTER cluster offered by NSF ACCESS, and direct connectivity to most major cloud service providers via Internet2.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232857","Research Infrastructure: CC* Data Storage: Multi-Petabyte Open Storage (MPOS) at the American Museum of Natural History","OAC","Campus Cyberinfrastructure","09/01/2022","08/26/2022","Juan Montes","NY","American Museum Natural History","Standard Grant","Kevin Thompson","08/31/2024","$498,202.00","Scott Schaefer, Michael Benedetto","jmontes@amnh.org","200 CENTRAL PARK W","NEW YORK","NY","100245102","2127695975","CSE","808000","","$0.00","This project provides the American Museum of Natural History (AMNH) a major expansion of data management and large-scale centralized storage supporting a broad range of research projects and provides supporting services to facilitate the acquisition, processing, and sharing of data. This significant expansion of storage and additional services allows AMNH to address storage and data management needs institutionally and addresses long-standing problems acutely felt across the Museum?s research divisions and graduate school.<br/> <br/>In this project, AMNH expands its data storage capabilities using Open Storage Network (OSN) storage pods, providing over 2 petabytes (PB) of centralized storage for active research data. The OSN pods are connected to the Museum?s Science DMZ, a high-performance network designed for research data flows, which provides connections to onsite high-performance computing (HPC) clusters, data transfer nodes (DTNs) in labs, and analytical systems, as well as to other institutions and resources in the cloud via a connection to the Internet2. The storage provided by the OSN pods is directly accessible by both local and cloud-based resources such as compute nodes at various cloud services and supercomputing centers nationally. AMNH leverages the Integrated Rule-Oriented Data System (iRODS) for data management, indexing, automated ingest, storage tiering, compliance, data integrity, and publishing services. The Clowder Framework provides a scalable data management framework supporting any data format across multiple research domains. Finally, AMNH makes available 20% of its allocated OSN storage to the shared OSN cloud, making it accessible to researchers nationally.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2333668","Conference: 2023 NSF Campus Cyberinfrastructure PI Workshop","OAC","Campus Cyberinfrastructure","09/01/2023","07/27/2023","Jennifer Leasure","WA","The Quilt","Standard Grant","Kevin Thompson","08/31/2024","$109,142.00","Jennifer Griffin","jen@thequilt.net","1455 NW LEARY WAY STE 400","SEATTLE","WA","981075138","2067821091","CSE","808000","","$0.00","The 2023 Campus Cyberinfrastructure PI Workshop builds upon the success of the previous Campus Cyberinfrastructure Workshops providing an opportunity for recipients of all active NSF Campus Cyberinfrastructure (CC*) awards to meet in-person, exchange project findings, interact with national cyberinfrastructure resources and experts as well as collaborate across project areas and project regions. By again co-locating the CC* workshop with the Quilt Fall Member Meeting, the broader programmatic scope encourages relationships between campus cyberinfrastructure, science driven applications, and regional and national cyberinfrastructure resources. The meeting schedule provides additional opportunities for interaction between PIs as well as between the diverse groups participating in the meetings. Included in the 2023 CC* PI Workshop is a set of post-workshop tutorials focused on topics relevant to challenges facing CC* project teams.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346751","CC* CIRA: BRICCs - Establishing Pathways for Regional Computing","OAC","Campus Cyberinfrastructure","04/01/2024","03/05/2024","Dhruva Chakravorty","TX","Texas A&M University","Standard Grant","Kevin Thompson","03/31/2026","$200,000.00","Tabitha Samuel, Lisa Perez, Amy Schultz, Sarah Janes","chakravorty@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","CSE","808000","","$0.00","Increasing use of advanced computing in academic and research pursuits has encouraged two-year institutions to offer their students and researchers pathways to large-scale computing. Advanced cyberinfrastructure (CI) resources are required to support the educators, students and researchers engaged in cloud computing, data-sciences, and related cross-cutting fields such as smart manufacturing offered at these institutions. While community colleges differ in terms of enrollment, demographics, geographical location, business models, and programs offered, the underlying institutional needs to store, access, manage, analyze, compute and curate their data remain the same.  By offering a unique collaborative space for regional computing, BRICCs-Pathways will support burgeoning research and education programs in cyber-enabled cross-cutting disciplines at two-year institutions.<br/><br/>This award establishes a BRICCs (Bringing Research Innovation to Community Colleges)-Pathways for Regional Computing, continuing and expanding previous planning and multi-institutional engagement activities, but focused around smaller institutions in Texas. BRICCs-Pathways supports these institutions in developing models to advance CI on campus. The project develops scalable and portable frameworks to promote regional computing efforts by  (i) promoting collaborations with state agencies, four-year institutions, and national CI center in their region; (ii) analyzing the implementations of networking models; (iii) exploring funded models to understand whether anchor agencies are needed for successful implementation; (iv) leveraging partnerships and assimilated information to assist institutions apply for funding for CI resources; (v) studying the technical computing and network characteristics to find an appropriate balance between network security and research and educational needs; and (vi) exploring means to make computing accessible to researchers and students at these institutions.  BRICCs-Pathways alleviates the need for several individual institutional studies by producing workshop reports, implementation studies, shared developed materials, and a set of recommendations to the community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346263","CC* CIRA: Nevada Vision for a co-Developed Impactful Cyberinfrastructure Ecosystem (NV-DICE)","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","04/15/2024","04/11/2024","Scotty Strachan","NV","Nevada System of Higher Education","Standard Grant","Kevin Thompson","03/31/2026","$199,713.00","Kivanc Oner, Anne Milkovich, Brandon Peterson, Sasi Pillay","sstrachan@nshe.nevada.edu","2601 ENTERPRISE RD","RENO","NV","895121666","7025227070","CSE","808000, 915000","9150","$0.00","Technology for academic research is severely under-resourced and under-recognized in the state of Nevada. While the national community has developed best practices for effective research and education information technology (or, ""cyberinfrastructure""), Nevada has remained outside of this evolution. With two fledgling Carnegie R1 universities, a dedicated research institute, and several state and community colleges at risk of falling further behind in cyber expertise, the Nevada System of Higher Education has created a joint discovery and planning project, ""NV-DICE: Nevada Vision for a co-Developed Impactful Cyberinfrastructure Ecosystem"". <br/><br/>NV-DICE brings together the Chief Information Officers and their teams from the major research institutions in Nevada to discover the current state of research IT affairs and plot a collective path forward that will best serve the state. In recognition of the need to work across campus borders, pool subject matter expertise, and capitalize on geographic strengths, the project focuses on developing a true community of practice. Over the two-year planning period, each campus is performing coordinated assessments, distilling priorities, and creating campus CI Plans. The team is engaging teaching-focused institutions to discover the collective state of IT education across Nevada, and co-develop a core regional architecture for networks, identity, and security that enable cooperative and resilient community cyberinfrastructures for all. The resulting update to the Nevada Science and Technology Plan serves as a launching point for future competitive infrastructure and Team Science proposals. NV-DICE's team ""office hours"", annual summits, and cooperative engineering is establishing a new era for Nevada's academic technology culture.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322084","CC* Regional Computing: Development of Undergraduate High-Performance Computing Capacity with the CIMUSE Consortium","OAC","Campus Cyberinfrastructure","12/01/2023","08/08/2023","Jeffrey Woodford","MO","Missouri Western State University","Standard Grant","Amy Apon","11/30/2025","$693,923.00","Colin DeGraf, xiaoyuan suo, Marcus Bond","jwoodford@missouriwestern.edu","4525 DOWNS DR","SAINT JOSEPH","MO","645072246","8162714364","CSE","808000","","$0.00","Four Primarily Undergraduate Institutions (PUIs), Missouri Western State University, Truman State University, Webster University, and Southeast Missouri State University, in the Computational Infusion for Missouri Undergraduate Science and Education (CIMUSE) consortium, provide high-performance computing (HPC) nodes for undergraduate research and education, with accessible state- and nation-wide resources as job sizes grow. University of Missouri Research Support Solutions manages the nodes on a day-to-day basis, allowing the participating institutions to focus on expanding computational usage at their respective campuses and other undergraduate institutions statewide. Talented students learn about HPC at summer workshops. Faculty also increase their knowledge and skills in the use of HPC. The goal is to establish and expand a statewide community using advanced computational techniques for undergraduate research and education that can be replicated nationwide.<br/><br/>The HPC compute nodes (2xAMD EPYC 7713, 2.0 GHz, 128 core/node, 512 GB RAM/node) for the CIMUSE Consortium provide HPC resources to PUIs in Missouri. Research projects will involve undergraduate student researchers, while educational projects focus on developing tools and pedagogies to train students in computational techniques. Initial research projects encompass cosmological simulations, pseudo-Jahn-Teller Effect studies, organic-inorganic perovskite hybrid simulations, rapid intrusion detection, and transposable genomics. Initial education projects involve constructing a Virtual Data Science Lab, incorporating simulations into undergraduate astronomy classes, and introducing cybersecurity exercises to the computer science curriculum. The HPC system will be managed by the Research Support Solutions (RSS) team at the University of Missouri - Columbia, increasing access for the undergraduate community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126303","CC* Compute: The Arizona Federated Open Research Computing Enclave (AFORCE), an Advanced Computing Platform for Science, Engineering, and Health","OAC","Campus Cyberinfrastructure","10/01/2021","09/01/2021","Douglas Jennewein","AZ","Arizona State University","Standard Grant","Kevin Thompson","09/30/2024","$399,997.00","Marisa Brazil, Gil Speyer, Suren Jayasuriya, Susanne Pfeifer","douglas.jennewein@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","CSE","808000","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Drawing upon its mission to enable access to discovery and scholarship in science, engineering, and health, Arizona State University (ASU) is deploying the Arizona Federated Open Research Computing Enclave (AFORCE). AFORCE provides cutting-edge technology to support research and education while advancing the knowledge and understanding of deploying 21st-century cyberinfrastructure in a large public research university. Specifically, this state-of-the-art system is supporting multidisciplinary research and education in science, technology, engineering, and mathematics domains including computational genomics, molecular dynamics, computational materials science, robotics, and imaging.<br/><br/>To increase computational capacity, AFORCE comprises a pool of multiple graphical processing unit (GPU) accelerated computing nodes accessible to extramural researchers through federated authentication provided via InCommon. Moreover, the AFORCE system itself is part of the global Open Science Grid computing pool.  ASU also promotes and enables the use of Open Science Grid by incorporating its capabilities into regular training sessions and faculty engagement events. Finally, AFORCE is configured to also provide cloud burst capabilities allowing compute jobs to be scheduled on commercial clouds. Early career faculty will be specifically targeted for workshops and tutorials, helping encourage their participation in the AFORCE system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232895","CC* Regional Computing: Launch: Advancing Cyberinfrastructure at Minority Serving Institutions","OAC","Campus Cyberinfrastructure","09/15/2022","09/07/2022","Dhruva Chakravorty","TX","Texas A&M University","Standard Grant","Amy Apon","08/31/2024","$1,000,000.00","Lisa Perez, Yossef Elabd, Ahmed Mahdy, Emily Hunt","chakravorty@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","CSE","808000","8238, 9102","$0.00","Launch is a regional computational resource that supports researchers incorporating computational and data-enabled approaches in their scientific workflows at eleven under-resourced institutions in The Texas A&M University System. Employing established best-practices in cyberinfrastructure facilitation, Launch appeals to researchers engaged in multi-disciplinary projects at these under-resourced, albeit research-driven minority serving institutions. Launch fosters a holistic cyberinfrastructure environment by developing pathways that connect researchers at these sites to computational resources and the necessary expertise to manage these resources by offering  (i) easy access to computing resources, (ii) a seamless pathway to large-scale national computing resources, (iii) a managed interdisciplinary software environment, (iv) available expertise in domain-specific computational avenues of research, (v) a training program that teaches researchers how to effectively use computing approaches, and (vi) tools that alleviate the complexity of using computing resources. Launch develops regional expertise in cyberinfrastructure management by employing a collaborative model that bootstraps existing computational expertise with burgeoning campus-level efforts. <br/><br/>Launch accelerates research workflows utilizing numerical simulation, interactive computing, and artificial intelligence/machine learning frameworks, while simultaneously bolstering nascent Data Science programs across these institutions.   Launch plans to host the latest generation of Intel processors, multi-precision NVIDIA multi-precision GPUs, and large memory nodes. These technologies are coupled with state-of-the-art open-source software, to offer a rich computing environment that offers interactive computing environments, web-accessible portals, and intuitive containerization and workflow management support.  Leveraging Partnerships with several large National Science Foundation-funded cyberinfrastructure programs, Launch ensures that researchers will have multiple avenues to ?launch? onto larger, more complex regional and national platforms. Institutions included in this project include Texas A&M University in College Station, Prairie View A&M University, Texas A&M Corpus Christi, West Texas A&M University, Texas A&M International, Texas A&M San Antonio, Texas A&M Kingsville, Texas A&M Texarkana, Tarleton State University, Texas A&M University Commerce, and Texas A&M Central Texas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126116","CC* Networking Infrastructure: Advanced Network For Research at UNC Charlotte","OAC","Campus Cyberinfrastructure","09/01/2021","07/15/2021","Christopher Maher","NC","University of North Carolina at Charlotte","Standard Grant","Kevin Thompson","08/31/2024","$497,187.00","Adam Reitzel, Aidong Lu, Samira Shaikh, Daniel Janies","cmaher9@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","CSE","808000","9102","$0.00","A diverse team of faculty researchers, research computing, and networking experts from across UNC Charlotte are collaborating to develop and deploy an advanced network for research across the campus.  This network dramatically improves digital communication between researchers, scientific instrumentation, visualization workstations, high performance computing (HPC) infrastructure and external collaborators. Access to the advanced network enables higher speed to existing research workflows and allows for new research processes previously unavailable to the team.  For example, massive datasets from multiple DNA sequencing instruments are streamed to powerful HPC clusters thus removing data analysis bottlenecks.  Research applying motion analysis and artificial intelligence to Future of Work research, natural language processing and cyber physical systems are also supported. Applications include fluid dynamics simulation of airflow, which carries infectious microbes, in public transportation.  Research collaborations between UNC Charlotte and other universities is enhanced by speeding acquisition and sharing of research results. Students from varied backgrounds, including UNC Charlotte?s large cohorts from underrepresented groups and first generation college students will work with this state-of-the-art cyberinfrastructure in their coursework and research projects.<br/> <br/>The design is a campus wide 100Gb fibre-based network with Science DMZ including a Data Transfer Node enabling data flows separate from the day-to-day traffic of University business. The network is implemented as a spine-leaf architecture with two spines (64x100GbE) interconnected to each other with 100Gb links by MLAG. One spine is placed in the HPC Data Center and the other is placed in a separate campus Data Center. The network has 6 Leafs (32x100 GbE) supporting connectivity between buildings housing academic departments and HPC infrastructure. Performance is monitored and tuned with PerfSonar nodes. Security is monitored through Zeek (40Gb/s) nodes.  The network and Science DMZ are tuned and optimized in collaboration between researchers and network architects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2127188","CC* Compute: A HPC Cluster for Science Research and Education at Tennessee Tech University","OAC","Campus Cyberinfrastructure","09/01/2021","02/01/2023","Michael Rogers","TN","Tennessee Technological University","Standard Grant","Kevin Thompson","08/31/2024","$399,983.00","Sheikh Ghafoor, Michael Renfro, Syed Rafay Hasan, Alfred Kalyanapu","mrogers@tntech.edu","1 WILLIAM L JONES DR","COOKEVILLE","TN","385050001","9313723374","CSE","808000","102Z","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Researchers at Tennessee Tech University (TN Tech) are making significant upgrades to the campus computing infrastructure that will significantly improve the university researchers? and students? ability to perform, enhance, and expand their systems-oriented, algorithms-oriented, or applications-oriented research activities. This enhanced new computing infrastructure complements other investments already made, in-progress, or in-planning at TN Tech. This modern campus cluster will enable TN Tech to grow and sustain the HPC culture by expanding on its current NSF-funded CyberTraining activities to include hundreds of faculty and their undergraduate students from resource-limited institutions from across the southeast.  This new cluster will help TN Tech to build a regional resource for computational capacity and workforce development expanding opportunities to underrepresented groups in the region.<br/><br/>This award allows TN Tech to procure a state-of-the-art, cost-effective 10 node GPU cluster supporting 20 NVIDIA A100 GPUs, 1280 AMD Epyc2 CPU cores, and 5 TiB of main memory connected with 100 Gbit/s Infiniband.  The cluster provides capabilities not previously available on campus. This project team anticipates significant research projects in computational fluid dynamics, biomechanics, and geospatial analysis, which will engage four partner universities. Seventeen ongoing research projects including 3 teaching projects will directly benefit from the improved infrastructure. Additional research projects will be enabled over time as the PIs and the planned internal advisory committee attract additional researchers and students requiring heterogeneous HPC. The project team expects about 100 of College of Engineering Ph.D. students will use the HPC infrastructure for their research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835782","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure","11/01/2018","08/23/2018","Wei Chen","IL","Northwestern University","Standard Grant","Alejandro Suarez","09/30/2024","$599,778.00","","weichen@northwestern.edu","633 CLARK ST","EVANSTON","IL","602080001","3125037955","CSE","171200, 772600","054Z, 062Z, 077Z, 7925","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346643","Research Infrastructure: CC* Campus Compute: Lawrence 2.0: Advancing Multi-Disciplinary Research and Education in South Dakota","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","07/01/2024","03/20/2024","Ryan Johnson","SD","University of South Dakota Main Campus","Standard Grant","Amy Apon","06/30/2026","$499,920.00","KC Santosh, Daniel Engebretson, Pere Miro, Bess Vlaisavljevich","Ryan.Johnson@usd.edu","414 E CLARK ST","VERMILLION","SD","570692307","6056775370","CSE","808000, 915000","9150","$0.00","The Lawrence 2.0 project expands and updates the Lawrence high performance computing cluster at the University of South Dakota (USD), advancing computationally intensive research and science education throughout South Dakota. The project's initial science and education use cases directly address several fields of immediate national prominence, including clean energy, sustainability, post-traumatic stress disorder, and the effects of anthropomorphic climate change. This project also complements and enhances existing Artificial Intelligence research and education initiatives at USD by keeping pace with growing demand for highly accessible GPU-accelerated computation, particularly among users who are new to high performance computing. Open and available to users at all educational institutions in South Dakota, the expanded Lawrence cluster bolsters research and STEM education opportunities at South Dakota's smaller colleges and universities. Additionally, the project increases USD's contributions to the Open Science Grid (OSG), with all unused compute cycles on Lawrence made available to the OSG Open Science Pool.<br/><br/>The project consists of achieving the following four key objectives: (1) Adding capacity to the Lawrence cluster to accelerate progress on research projects throughout South Dakota; (2) Reducing individuals' barriers to entry by implementing Open OnDemand software for cluster access; (3) Avoiding disruptions to scientific progress by replacing Lawrence's aging core components; and (4) Broadening high performance computing participation and impact via targeted outreach to regional collaborators and tribal institutions. This project is having impact and expanding research and STEM education in South Dakota, an EPSCoR state.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835877","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","Data Cyberinfrastructure","09/01/2018","05/19/2023","Barbara Minsker","TX","Southern Methodist University","Standard Grant","Alejandro Suarez","08/31/2024","$638,151.00","Kenneth Berry, Jessie Zarazaga","minsker@smu.edu","6425 BOAZ ST RM 130","DALLAS","TX","752051902","2147684708","CSE","772600","062Z, 077Z, 7218, 7925, 8048, 9251","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126291","CC* Integration-Large:  (BLUE) Software-Defined CyberInfrastructure to enable data-driven smart campus applications","OAC","Campus Cyberinfrastructure","09/01/2021","09/20/2023","Dijiang Huang","AZ","Arizona State University","Standard Grant","Kevin Thompson","08/31/2024","$500,000.00","Jessica Shoop, Zach Jetson, Baoxin Li, Ming Zhao, Xuesong Zhou","dijiang@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","CSE","808000","102Z","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>A new campus infrastructure called BLUE is developed to enable efficient and secure data-driven research and application development based on distributed IoT devices. BLUE addresses three main challenges for supporting innovative smart campus applications based on distributed IoT devices: (a) establishing a programable campus infrastructure to support distributed and ad hoc IoT services, (b) providing strong security and privacy protection of IoT data, and (c) constructing an edge-cloud infrastructure to provide computing, networking, and storage resources to support smart-campus applications. <br/><br/>BLUE is a new software-defined infrastructure to support IoT-based data processing, analysis, and distribution over distributed IoT data sources. BLUE also supports a set of tangible metrics, such as network QoS metrics, location, resource consumption, etc., to effectively enable researchers to validate their research models. Moreover, BLUE takes privacy and security protection as a fundamental enabling technique by pushing the computation towards the edge computing and networking infrastructure. Research applications built on this project share a common requirement for low-latency transfer of ever-larger data sets with collaborators across multiple geographic sites.  This project will contribute to a national paradigm of campus-level dynamic network services that enables leading-edge network and domain-specific research.<br/><br/>BLUE can benefit the full range of campus scholarly activities, including research activities funded by NSF and other federal agencies.  The outcomes of this project will be shared with the public based on an open-source license agreement. In addition, undergraduate and graduate student researchers will receive diverse STEM skills training, including networking research, big data analysis, and domain-specific research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104025","Elements:  Towards a Robust Cyberinfrastructure for NLP-based Search and Discoverability over Scientific Literature","OAC","NSF Public Access Initiative, Data Cyberinfrastructure, Software Institutes","05/01/2021","09/08/2023","James Pustejovsky","MA","Brandeis University","Standard Grant","Varun Chandola","04/30/2025","$479,466.00","Nancy Ide","pustejovsky@gmail.com","415 SOUTH ST","WALTHAM","MA","024532728","7817362121","CSE","741400, 772600, 800400","077Z, 7923, 8004","$0.00","This project creates an open platform for accessing and mining information from scientific texts that provides access to an array of software, computing resources, and publication data.  Current search technologies typically find many relevant documents, but do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. Natural Language Processing (NLP) strategies are a recognized means to approach this problem, and this project develops the cyberinfrastructure to support sophisticated search and retrieval from scientific publications, use and augmentation of facilities for advanced and well-established natural language processing and machine learning tools, and extraction and aggregation of data from scientific publications. The project leverages two NSF-funded projects: the Language Applications (LAPPS) Grid, which has already proven to be an effective platform for development of NLP applications; and University of Wisconsin?s xDD (formerly, GeoDeepDive), a scalable, dependable infrastructure capable of rapidly growing a digital library of scientific publications, currently including over 13 million documents from multiple distributed commercial and open-access providers. The effort significantly enhances the value of these existing NSF-funded infrastructures by providing access to services for mining scientific publications and lowering the barriers to entry resulting from licensing, redistribution, and intellectual property issues. Scientists may perform large-scale text retrieval and mining using the University of Wisconsin?s high performance computing (HPC) infrastructure through a web-based interface. Iterative domain adaptation capabilities allow scientists to easily adapt existing services to specialized areas without configuring or installing additional components. The potential impact of the cyberinfrastructure is applicable to any community that relies on computational tools for mining large textual datasets, including researchers in sociology, psychology, economics, education, linguistics, digital media, and the humanities.<br/><br/>This project extends the LAPPS Grid to provide access to UW-xDD?s collection of scientific publications and UW?s High Performance Computing facilities, as well as means to rapidly adapt existing, well-established natural language processing and machine learning software tools to new domains and evaluate results. The LAPPs Grid provides a large collection of NLP tools from a wide variety of sources exposed as web services, together with multiple commonly used resources and a front-end document retrieval engine currently configured to access PubMed/PubMedCentral as well as nightly updates of the CORD-19 dataset. The LAPPS Grid is open source, and can be run from the web, on a user?s laptop or desktop, in the cloud, or as a self-contained docker image when it is necessary to protect sensitive or licensed data, when there is no network connection available, or for deployment on remote HPC facilities. All tools and resources can be used interoperably, eliminating the effort required to convert input and output formats to use a set of tools or resources together.  xDD is one of the world?s largest single repositories of scientific publications that spans all domains of knowledge, incorporates new documents automatically and updates API endpoints every hour.  xDD has accumulated millions of documents from multiple commercial and open-access publishers (over 13M publications).  The xDD infrastructure is an integral part of the developing UW-COSMOS pipeline, which consists of a suite of services supporting document processing, including ingestion and parsing of PDFs; extraction of individual document objects such as text sections, figures, tables, and captions; and recall, which creates searchable Anserini and ElasticSearch indexes on the contexts and objects to enable retrieval of information. Specific project activities include implementing efficient retrieval and analysis of xDD?s vast holdings of scientific publications; extending the NLP capabilities of the LAPPS Grid for scientific publication mining and domain adaptation; developing full interoperability between the Grid and xDD/COSMOS; scaling LAPPS Grid services to handle the very large textual datasets available from UW-xDD; and surveying visualization techniques and integrating them into the Grid.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering, and the NSF Public Access program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2234411","Center: IUCRC Phase I UNorth Dakota: Center to Stream Healthcare In Place (C2SHIP)","CNS","Information Technology Researc, IUCRC-Indust-Univ Coop Res Ctr","07/15/2023","04/16/2024","Kouhyar Tavakolian","ND","University of North Dakota Main Campus","Continuing Grant","Mohan Kumar","06/30/2026","$299,999.00","Sandeep Singhal, Bo Liang, Ryan Striker, Julie Smith- Yliniemi, Enrique Alvarez Vazquez, Allison Kelliher","kouhyar@und.edu","264 CENTENNIAL DR STOP 8371","GRAND FORKS","ND","582028371","7017774151","CSE","164000, 576100","5761, 9150","$0.00","As our population ages and regional health disparities grow, significant financial and emotional burdens are imposed on our society, patients, and their families. Advances on many fronts are needed to impact the needs of these patients and assist them in prolonging their autonomy by keeping them in their homes longer and providing a higher level of health monitoring. The Center to Stream Healthcare In Place (C2SHIP) has been established to address these challenges and the six university sites for C2SHIP are University of Arizona, Baylor College of Medicine, University of Southern California, California Institute of Technology, University of Missouri, and the University of North Dakota (UND). C2SHIP brings together leading experts in academic medicine and the biomedical industry to research, develop, and promote in-place care technologies for managing chronic diseases in the home.  <br/> <br/>C2SHIP has three major thrusts: 1) promoting in-place care technologies, 2) academic-industry partnership & resource sharing, and 3) educating a workforce with the capacity for promoting wellness. In the first two thrusts, C2SHIP's trans-disciplinary innovation and research teams focus on developing new material-based sensors, reconfigurable designs, and system integration with data mining, machine learning, and artificial intelligence. By using the ""Internet of Things,"" technological approach, patient data can be streamed in real-time to medical professionals in remote locations, creating a mobile hub for vulnerable patients in their homes.  <br/> <br/>In addition to the first two thrusts, UND contributes expertise to C2SHIP in two unique areas: 1) an innovation-based learning (IBL) model for transdisciplinary student development and 2) indigenous health graduate education. In innovation-based learning, student learning is assessed by developing value from fundamental principles of their discipline while working on transdisciplinary teams.   <br/><br/>As with industry 4.0 skill sets, education 4.0 skill sets are distributed across the professional, innovation, and STEM categories using tools such as epistemic network analysis, big data, and learning analytics. Student engagement in the proposed research/innovation projects will create opportunities for multidisciplinary participation at C2SHIP conferences and workshops. UND also adds value to C2SHIP with an important connection to Indigenous people through collaborations with the Indigenous Health Department at UND.  Graduate students from this program are our ?human-in-the-loop? in the indigenous community looking for health gaps that can be used to inspire solutions within the C2SHIP and IBL process that engenders a knowledgeable workforce for promoting wellness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2212032","III:Medium:Computation and Communication Efficient Distributed Learning","IIS","Information Technology Researc, Info Integration & Informatics","10/01/2022","11/03/2022","Jiliang Tang","MI","Michigan State University","Standard Grant","Raj Acharya","09/30/2026","$1,200,000.00","Hui Liu, Ming Yan","tangjili@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","CSE","164000, 736400","7364, 7924","$0.00","With the explosion of large-scale machine learning tasks and the increasing availability of computational resources, distributed learning has become the cornerstone for extracting information and knowledge from big data. Nodes in distributed learning need to communicate information. Thus, distributed learning faces challenges around computational efficiency similar to conventional machine learning. But it also faces the additional challenge of communication efficiency. These efficiency problems have greatly hindered the applications of distributed learning in large-scale machine learning tasks and complex computing environments, such as resource-limited edge computing. In this project, we embrace new challenges and opportunities to comprehensively study the computation and communication efficiency in distributed learning. The project?s novelties are providing new perspectives for developing and deploying efficient and scalable distributed learning algorithms in large-scale computing clusters with limited communication protocols and network bandwidth. Nowadays, as big data is ubiquitous, the project's impacts are to benefit many real-world applications from various disciplines such as computer science, social sciences and others areas.<br/><br/>This project aims to tackle the major drawbacks in existing distributed learning algorithms from the efficiency perspective and greatly promote the efficiency and scalability of large-scale distributed learning. To achieve this goal, we systematically investigate two distributed learning paradigms, centralized and decentralized learning, as well as two major efficiency obstacles, computation and communication efficiency. To address these paradigms and obstacles, the project has three dedicated designed research directions. Each direction will dramatically extend the science through not only providing rigorous theoretical guarantees, but also comprehensive empirical studies in practical systems. The core intellectual is a comprehensive investigation on science and the design of novel methodologies to deepen our understanding on the efficiency, scalability, and practical usages of distributed learning systems and algorithms. The outcomes of this project will be: (1) New efficient and scalable distributed learning algorithms with state-of-the-art computation and communication efficiency, as well as predictive accuracy; (2) Theoretical analysis such as convergence rate and communication complexity; and (3) Open-source implementations of all key algorithms, systems, and frameworks. The proposed research will involve graduate and undergraduate students in pursuing their thesis or honor's projects. Discoveries and research findings of this project will be tightly integrated into several current and new courses. Instructional content will be created to enable fast distribution of our results to a wide audience, and tools will be built to help machine learning knowledge awareness and adoption. The findings of this project will be timely disseminated via multiple means such as a distributed learning repository, journal and conference publications, special purpose tutorials and workshops co-held at prominent conferences, and industrial participation such as internships.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117814","MRI: Development of X-Mili: An Open, Programmable Platform to Conquer the 5G and 6G Wireless Spectrum","CNS","Major Research Instrumentation, Information Technology Researc","10/01/2021","09/07/2022","Dimitrios Koutsonikolas","MA","Northeastern University","Standard Grant","Deepankar Medhi","09/30/2024","$2,077,596.00","Stefano Basagni, Josep Jornet, Tommaso Melodia, Kaushik Chowdhury","d.koutsonikolas@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","118900, 164000","1189","$0.00","Communication in the millimeter wave (mmWave) frequency bands has emerged as a potential solution to the bandwidth crunch problem by realizing multi-Gbps rates. Communication in the 28 GHz and 38 GHz spectrum bands is a major building block of the 5G cellular architecture, and communication at even higher frequencies is expected to play a major role in the upcoming 6G architecture.  The proposed project will acquire the necessary hardware and software components to build X-Mili, an 8-node mmWave experimental testbed, which would combine the following features:  (i) dual-band operation at both 60 GHz and 28 GHz, enabling both WLAN and5G cellular research, and extensibility towards higher (6G) frequency bands, (ii) practical phased antenna arrays, (iii) bidirectional SISO, 2x2 SU-MIMO, and MU-MIMO operations in both bands, (iv) full programmability at all layers of the protocol stack, and (vi) O-RAN compliance.<br/><br/>Example projects enabled by the proposed infrastructure include characterization of ultra-wideband MIMO (Multiple-Input & Multiple-Output) and OFDM (Orthogonal Frequency-Division Multiplexing) channels and new PHY layer design for 5G and 6G bands, cross-layer design of novel algorithms for single-user and multi-user MIMO and link adaptation, machine learning-driven link and network adaptation, resource allocation, and network management, software defined networking, security in 5G networks, joint sensing and communications, enabling high-bandwidth, low-latency applications over mmWave networks, spectrum sensing, and vital sign monitoring. The proposed development activity would provide the mmWave research community with a unique experimental platform that may be instrumental in advancing research activities in the fields of mmWave communications, networking, and sensing. All measurement data generated during the development phase as well as datasets contributed by research groups using the instrument would become publicly available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126108","CC* CIRA: Shared Arkansas Research Plan for Community Cyber Infrastructure (SHARP CCI)","OAC","Campus Cyberinfrastructure","09/01/2021","04/04/2023","Donald DuRousseau","AR","University of Arkansas","Standard Grant","Kevin Thompson","08/31/2024","$199,592.00","Mansour Mortazavi, Brian Berry, Lawrence Tarbox, Jackson Cothren","drdurou@msu.edu","1125 W MAPLE ST STE 316","FAYETTEVILLE","AR","727013124","4795753845","CSE","808000","102Z, 9102, 9150","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>SHARP CCI creates a statewide research cyber infrastructure (RCI) Plan for Arkansas that is focused on the eight degree granting institutions performing science and engineering research. Participating institutions include: Arkansas State University (ASU); Southern Arkansas University (SAU); University of Arkansas (UA) Fayetteville (UAF); UA Division of Agriculture (UADA); UA Little Rock (UALR); UA Medical Sciences (UAMS); UA Pine Bluff (UAPB); and University of Central Arkansas (UCA). Each school has a growing demand for federated access to high-speed resources, managed services and technical training; however, a coordinated plan for providing these capabilities does not currently exist. Most schools in Arkansas lack the connectivity, budget and staffing to fully utilize these statewide resources, and each school is unique in technical capability and expertise. SHARP CCI assembles the senior administrators and research leaders at each school to document the environments, capabilities, technology needs and resource gaps and create an equitable RCI Plan to inform decision makers in the state. This plan will also provide a necessary resource for each school to apply for additional NSF support through solicitations that expand school infrastructure and training capabilities.<br/><br/>A statewide RCI Plan is requisite for organizing and expanding research collaborations and it fosters an economy-of-scale for building compliant facilities and systems and establishing a managed service environment for schools with limited resources. SHARP CCI includes establishing a comprehensive data science training program, Arkansas Cyber Team (ACT), to provide engineers, educators, researchers and students access to technical expertise in a broad range of scientific domains. The project team includes Arkansas Research and Education Optical Network, Great Plains Network, Open Science Grid and NSF?s Engagement and Performance Operations Center to provide engineering and training expertise, managed service experience and help in implementing a statewide RCI Plan for Arkansas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104068","Collaborative Research: Frameworks: Convergence of Bayesian inverse methods and scientific machine learning in Earth system models through universal differentiable programming","OAC","Polar Cyberinfrastructure, Data Cyberinfrastructure, Software Institutes, EarthCube","08/01/2021","07/15/2021","Sri Hari Krishn Narayanan","IL","University of Chicago","Standard Grant","Varun Chandola","07/31/2025","$800,000.00","Michel Schanen","snarayan@mcs.anl.gov","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","540700, 772600, 800400, 807400","077Z, 1079, 7925, 8004","$0.00","Understanding and quantifying parameter sensitivity of simulated systems, such as the numerical models of physical systems and mathematical renderings of neural networks, are essential in simulation-based science (SBS) and scientific machine learning (SciML). They are the key ingredients in Bayesian inference and neural network training. Seizing on the opportunity of emerging open-source Earth system model development in the Julia high-level programming language, this project is endowing these open-source models with automatic differentiation (AD) enabled derivative information, making these converging data science and simulation-based science tools available to a much broader research and data science community. Enabling a general-purpose AD framework which can handle both large-scale Earth system models as well as SciML algorithms, such as physics-informed neural networks or neural differential equations, will enable seamless integration of these approaches for hybrid Bayesian inversion and Bayesian machine learning. It merges big data science, in which available data enable model discovery with sparse data science, and the model structure is exploited in the selection of surrogate models representing data-informed subspaces and fulfilling conservation laws. The emerging Julia language engages a new generation of researchers and software engineers, channeling much needed talent into computational science approaches to climate modeling. Through dedicated community outreach programs (e.g., Hackathons, Minisymposia, Tutorials) the project team will be working toward increasing equity, diversity, and inclusion across the participating disciplines.<br/><br/>The project is developing a framework for universal differentiable programming and open-source, general-purpose AD that unifies these algorithmic frameworks within Julia programming language. The general-purpose AD framework in Julia leverages the composability of Julia software packages and the differentiable programming approach that underlies many of the SciML and high-performance scientific computing packages. Compared to most current modeling systems targeted for HPC, Julia is ideally suited for heterogeneous parallel computing hardware (e.g., CUDA, ROCm, oneAPI, ARM, PowerPC, x86 64, TPUs). The project is bringing together expertise in AD targeted at Earth system data assimilation in high performance computing environments with SciML expertise. The project team is working with the Julia Computing organization and package developers to ensure sustainability of the developed frameworks. The project?s Earth system flagship applications consist of (i) an open-source, AD-enabled ocean general circulation model that is being developed separately as part of the Climate Modelling Alliance (CliMA), and (2) an open-source, AD-enabled ice flow model. Each of these application frameworks is being made available to the community for science application, in which derivative (gradient or Hessian) information represent key algorithmic enabling tools. These include SciML-based training of surrogate models (data-driven and/or model-informed), parameter and state estimation, data assimilation for model initialization, uncertainty quantification (Hessian-based and gradient-informed MCMC) and quantitative observing system design. Academic and industry partners are involved, who are using the frameworks for developing efficient power grids, personalized precision pharmacometrics, and improved EEG design.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103832","Collaborative Research: ELEMENTS: Tuning-free Anomaly Detection Service","OAC","Data Cyberinfrastructure, Software Institutes","05/01/2021","05/21/2021","Elke Rundensteiner","MA","Worcester Polytechnic Institute","Standard Grant","Varun Chandola","04/30/2025","$259,651.00","","rundenst@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","772600, 800400","077Z, 7923","$0.00","Finding and understanding anomalous behavior in data is important in many applications. A large number of anomaly detection algorithms exist, and it can be difficult to determine which algorithm is best suited to a particular domain.  And once an algorithm is selected, users must tune many parameters manually to get the algorithm to perform well; this requires in-depth knowledge of the machine learning process and an understanding of the trade-offs among different algorithms to select the best performing approach.  To address these difficulties, this team develops a package that can test a range of unsupervised anomaly detection techniques on a dataset, explore options to identify best-fit, and classify anomalies with higher accuracy than manual tuning.<br/><br/>The project will automatically test a range of unsupervised anomaly techniques on a data set, extract knowledge from the combined detection results to reliably distinguish between anomalies and normal data, and use this knowledge as labels to train an anomaly classifier; the goal is to classify anomalies with an accuracy higher than what is achievable by thorough manual tuning. The approach can be applied across of a range of data types and domains. The resulting cyberinfrastructure provides tuning-free anomaly detection capabilities while making it easy to incorporate domain-specific requirements. It enables scientists and engineers having little experience with anomaly detection techniques to steer the anomaly detection process with domain expertise.  Evaluation of the unsupervised anomaly detection package will use data sets and partnerships with collaborators from the Massachusetts General Hospital/Harvard Medical School, Cyber Security research, and Signify (formerly Philips Lighting) to ensure that the utility and usability of the package is verified throughout the development process. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2045679","CAREER: Advancing Remote Collaboration: Inclusive Design for People with Dementia","IIS","Information Technology Researc, HCC-Human-Centered Computing, IIS Special Projects","04/01/2021","12/15/2022","Amanda Lazar","MD","University of Maryland, College Park","Standard Grant","Dan Cosley","03/31/2026","$609,362.00","","lazar@umd.edu","3112 LEE BUILDING","COLLEGE PARK","MD","207425100","3014056269","CSE","164000, 736700, 748400","1045, 7367, CL10","$0.00","Technology increasingly provides opportunities to interact remotely with others. People with cognitive impairment can be excluded from these opportunities when technology is not designed with their needs, preferences, and abilities in mind. Mild dementia is one form of cognitive impairment that is experienced by many individuals worldwide. Little is known about how technology should be designed to support people with mild dementia. In addressing this gap, this project has the potential to benefit not only people with dementia, but also researchers, caregivers, and the general public, who currently lack opportunities to interact with and learn from people with dementia. This project will also train the next generation of technology researchers and designers in two key ways: 1) with a remote dementia lab that scaffolds design activities involving end users with dementia, and 2) by co-developing and disseminating modules with people with dementia, disability advocates, and healthcare professionals that can be flexibly integrated into human-centered design courses.<br/><br/>The goal of this project is to design, develop, and evaluate approaches to support and study inclusive collaborative technologies that support people with mild dementia in remote interaction. The project will investigate: 1) empirical knowledge of preferences, benefits, and barriers experienced by people with mild dementia in using digital collaborative tools; 2) new interaction techniques to engage people with dementia in remote collaborative technology use through the iterative development of three novel applications; 3) a theoretical understanding of how established Computer Supported Cooperative Work (CSCW) theories can be adapted for dementia; 4) a validated framework for designing for people with mild dementia; and 5) innovative methods to remotely include people with dementia in design activities. In order to achieve these goals, this project involves research threads targeting three diverse domains (social, purpose-oriented, and cognitive) and interaction partners (peers with dementia, mixed ability groups of older adults, and caregivers). This project has the potential to transform research, design, development, and deployment of technologies for people with cognitive impairment, advancing collaborative technology design from highly customized and bespoke technology to an area ripe for scalable development and deployment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2045153","CAREER: Leveraging Recommendations for Self-Actualization","IIS","Information Technology Researc, HCC-Human-Centered Computing, IIS Special Projects","04/01/2021","07/17/2023","Bart Knijnenburg","SC","Clemson University","Continuing Grant","Cindy Bethel","03/31/2026","$364,987.00","","bartk@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","CSE","164000, 736700, 748400","1045, 7367, CL10","$0.00","The primary research goal of this project is to leverage recommendation technology to support users in developing, exploring, and understanding their preferences based on their long-term goals and ambitions. Recommender systems provide their users with recommendations based on their expressed preferences, but decision scientists have demonstrated that users' preferences are often constructed on the fly and thereby focused on their immediate desires rather than their longer-term goals. The recommendations, in turn, tend to reflect these short-term likes, ignoring users' ambitions and long-term goals. This proposal will take an important step towards escaping this vicious circle by developing and testing two novel user-adaptive interaction mechanisms that leverage recommendation algorithms to support unique new ways for users to develop, discover, and understand their own preferences. This will turn the users of these systems into confident consumers who are able to construct preferences that focus on their broader long-term objectives.  The decision-support platform and the proposed interaction mechanisms will be made available to academics to support their research on recommender systems and decision-making.<br/><br/>Two innovative interaction mechanisms can advance the field of recommender systems research as well as the field of decision science by extending the reach of personalized decision-support tools beyond ""choice"" towards preference construction: (1) Personalized preference profiles that use recommendation algorithms to visualize users' preferences and highlight surprising preference dynamics. These profiles will induce preference construction by helping users understand their preferences, reflect upon their long-term goals, and make better-informed decisions. (2) Preference-based communities that use recommendation algorithms to automatically create ad-hoc groups of users whose interests overlap enough to generate interesting discussion, yet not so much that they turn into epistemic bubbles. These communities seek to induce preference construction through discussion, thereby reinstating the social and intellectual benefits of active advice-giving practices. This project will integrate these interaction mechanisms into live systems, supporting education and outreach activities where students of all ages will use these systems to make decisions based on their preferences and life goals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346397","CC* Regional Networking:  Creating Opportunities for Research & Education - Multiple Organization Regional - OneOklahoma Friction Free Network - CORE-MORe-OFFN","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","04/01/2024","04/03/2024","Brian Burkhart","OK","OKLAHOMA STATE REGENTS FOR HIGHER EDUCATION","Standard Grant","Kevin Thompson","03/31/2026","$1,014,757.00","Paul Wills, April Goode, Heath Hodges, William Bradford, Sky Pettett","brian@onenet.net","655 RESEARCH PKWY STE 200","OKLAHOMA CITY","OK","731046217","4056309620","CSE","808000, 915000","9150","$0.00","Creating Opportunities for Research & Education Multiple Organization Regional OneOklahoma Friction Free Network (CORE-MORe-OFFN) connects Connors State College, Eastern Oklahoma State College, Northeastern Oklahoma A&M College and Oklahoma Panhandle State University to the OneOklahoma Friction Free Network (OFFN). CORE-MORe-OFFN promotes education and research drivers in biology, social sciences, math, aeronautical engineering, information technology and nursing. Supporting these STEM-related fields is crucial to meeting state workforce needs outlined in the Oklahoma State Regents for Higher Education's Blueprint 2030 strategic plan. A significant percentage of Native American and Hispanic students comprise the populations of these institutions, which span across rural Oklahoma. CORE-MORe-OFFN leverages statewide partnerships to provide faculty and students' access to cyberinfrastructure practitioners and resources, creating cross-institutional academic collaboration opportunities. <br/><br/>CORE-MORe-OFFN extends OFFN at 10GBps to four under-resourced campuses, including three A&M colleges, by implementing Science DMZs, Data Transfer Nodes and network performance monitoring capabilities to initiate new education and research driver capabilities that integrate with existing cyberinfrastructure resources at institutions throughout Oklahoma. The project upgrades connectivity via a new fiber build at one campus and a leased wave service at a second. Campus technical personnel facilitate adoption of new technology and provide access to network infrastructure, computing tools and scientific equipment. The project also engages faculty and students in the larger OneOklahoma CyberInfrastructure Initiative (OneOCII) group, which connects CI practitioners throughout Oklahoma. OFFN is managed by OneNet, Oklahoma's research and education network, with scientific leadership provided by the University of Oklahoma.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346755","CC* Integration-Small: Network-Aware Edge Computing for Real-time Wildfire Detection","OAC","CISE Research Resources, Campus Cyberinfrastructure, EPSCoR Co-Funding","05/01/2024","04/15/2024","Batyr Charyyev","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Nicholas Goldsmith","04/30/2026","$500,000.00","Engin Arslan","bcharyyev@unr.edu","1664 N VIRGINIA ST # 285","RENO","NV","895570001","7757844040","CSE","289000, 808000, 915000","9150","$0.00","The proliferation of Internet of Things (IoT) devices has facilitated the development of various scientific applications, from smart city initiatives to environmental hazard monitoring systems. In most of these applications, swift processing of data captured by IoT sensors is crucial to prevent natural disasters in a timely manner. While conventional cloud-based data processing pipelines offer cost-effective and performance-efficient solutions, it is often challenging to meet stringent performance requirements of hazard monitoring systems such as low latency and high bandwidth. Edge computing emerges as a compelling solution to bridge this gap by bringing computational processing closer to the data source.  This project develops an edge computing framework tailored to address the computational requirements of time-sensitive distributed scientific applications such as wildfire monitoring. The edge computing framework has the potential to benefit other domains beyond wildfire monitoring, such as autonomous vehicles and emergency response systems.<br/><br/>The project develops an edge computing framework that optimizes the task scheduling problem by combining high precision system monitoring with comprehensive application profiling. It develops a scalable resource monitoring system to monitor the status of compute resources (e.g., edge servers and cloud instances) and network resources using lightweight monitoring agents and P4 programmable network devices. Additionally, the project conducts application profiling to extract essential metrics regarding resource utilization and execution time of tasks across various edge server and cloud instance configurations. The scheduling is formulated as a multi-objective optimization problem, and various optimization methods such as mixed-integer linear programming, genetic algorithms, and heuristic methods are explored. Finally, the team targets a wildfire detection project (AlertWildfire) as a use case to demonstrate the effectiveness of the proposed framework. <br/><br/>This project is jointly funded by Office of Advanced Cyberinfrastructure, the Established Program to Stimulate Competitive Research (EPSCoR), and the Division of Computer and Network Systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2144503","CAREER: Conversational User Interfaces to Support  Older Adults' Social Wellness","IIS","Information Technology Researc, HCC-Human-Centered Computing","05/15/2022","03/16/2023","Aqueasha Martin-Hammond","IN","Indiana University","Continuing Grant","Dan Cosley","04/30/2027","$543,539.00","","aqumarti@iupui.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","164000, 736700","102Z, 1045, 7367","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Maintaining social wellness through a sustained network of in-person relationships is essential to preserving health. Yet, as individuals age, remaining socially active can become challenging. For example, many older adults relocate for retirement, experience changes in mobility, or begin losing family and friends, which may weaken the fabric of their social network. Because of this, older adults are at risk of experiencing feelings of loneliness and social isolation, which may lead to declines in mental and physical health. This project will investigate the potential of emerging conversational user interfaces such as voice assistants and chatbots to help older adults stay connected with the world around them, thus improving their social wellness. By engaging older adults in the design process and adapting state-of-the-art conversational design methods, this work will develop empirically validated novel strategies for personalized conversational interfaces that leverage real-world social connections and motivate older adults to engage in social activities with other people. The project will also yield new curriculum, experiential learning opportunities, community engagement initiatives, and tools to aid reflective thinking of the impact of artificially intelligent technologies on an increasingly aging society.<br/><br/>This project leverages social-behavioral theories to identify and examine a new family of conversational interaction approaches for supporting older adults in maintaining and building social relationships with others. To do so, the team will employ participatory methods that leverage close community collaborations with senior organizations and older adults. The research will include three main aims. First, the team will generate an empirical account of the needs of older adults to maintain social wellness in an interconnected world, along with situations that pose risks of isolation. Second, the team will explore conversational design principles to support collaborative two-way dialogues that encourage engagement in social wellness activities, with particular attention to people's concerns about privacy and aspects of the situation where they are using the system. Finally, the team will develop and evaluate a novel platform implementing tested conversational strategies to motivate older adults' participation in socially meaningful activities. This project will advance knowledge of the uses, challenges, and real-world barriers of designing and deploying conversational user interfaces to support older adults' social wellness, contributing to human-centered artificial intelligence, aging, and conversational design. The research is expected to reveal new approaches that can benefit older adults' social wellness while decreasing the risk of social isolation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126285","CC* Regional: Extended Small Institution - Multiple Organization Regional - OneOklahoma Friction Free Network (ESI MORe OFFN)","OAC","Campus Cyberinfrastructure","09/15/2021","01/25/2024","Vonley Royal","OK","OKLAHOMA STATE REGENTS FOR HIGHER EDUCATION","Standard Grant","Kevin Thompson","08/31/2024","$414,595.00","April Goode, William Bradford, Brian Burkhart, Sky Pettett, Jeffrey Price, Kelly McClure","von@onenet.net","655 RESEARCH PKWY STE 200","OKLAHOMA CITY","OK","731046217","4056309620","CSE","808000","9102, 9150","$0.00","The Extended Small Institution Multiple Organization Regional OneOklahoma Friction Free Network (ESI-MORe-OFFN) connects two under-resourced campuses, Oklahoma City University and Fires Innovation Science and Technology Accelerator campus of Cameron University, to the OneOklahoma Friction Free Network (OFFN) in support of research and education drivers in the areas of engineering and economics. The project extends existing and creates new and diverse research collaboration opportunities for faculty and students through fostering broader and deeper collaborations and expanding research and education activities. The project provides cyberinfrastructure resources to these two under-resourced campuses, reaching a diverse student population, including first-generation students. The project has significant educational impact for both undergraduate and graduate students at the campuses, providing them with additional STEM and cyberinfrastructure opportunities as well as exposure to leading researchers and CI practitioners.<br/><br/>The ESI-MORe-OFFN project extends the OneOklahoma Friction Free Network to two under-resourced campuses by implementing Science DMZs, Data Transfer Nodes and network performance monitoring capabilities in support of science application drivers at the two campuses and integrating these capabilities with other similar capabilities across Oklahoma.?In addition, the project enhances the skills and abilities of technical personnel at each campus and facilitates cross-campus collaborations, including shared uses of computing tools, scientific equipment, and educational materials by engaging faculty and students in the larger OneOklahoma CyberInfrastructure Initiative (OneOCII). OFFN is managed by OneNet, Oklahoma's research and education network, and scientific leadership is provided by the University of Oklahoma.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232851","CC* Data Storage: NRDStor: Nebraska Research Data Storage","OAC","Campus Cyberinfrastructure","01/01/2023","08/29/2022","Derek Weitzel","NE","University of Nebraska-Lincoln","Standard Grant","Kevin Thompson","12/31/2024","$496,207.00","Hongfeng Yu, Adam Caprez, Toolika Ghose","dweitzel@unl.edu","2200 VINE ST","LINCOLN","NE","685032427","4024723171","CSE","808000","9102, 9150","$0.00","The Holland Computing Center (HCC) at the University of Nebraska (NU) has a long history of providing high-performance and high-throughput compute capabilities and the associated storage resources. A long-standing hurdle many NU researchers experience is the difference between their traditional work environments (i.e., desktop) and HCC computing environments, often incurring non-trivial data transferring across these environments. The Nebraska Research Data Storage (NRDStor) creates a cyberinfrastructure storage resource that unites the familiarity of a researcher?s desktop environment with HCC?s computing resources. NRDStor is accessible on HCC?s computing clusters, as well as mountable by researchers on their lab computers and laptops. Fast data transfers between research labs and NRDStor are enabled by leveraging recent upgrades to campus networking. Data stored on NRDStor may be shared (at the user?s discretion) via the Open Science Data Federation (OSDF). <br/><br/>NRDStor is available to all HCC users, including the four primary NU campuses in Lincoln, Omaha, the Medical Center in Omaha, and Kearney. The science drivers of NRDStor span the diverse scientific communities that vary in practice from computational biology to wireless networking and encompass many domains in-between. By providing a united filesystem accessible to researchers on both their desktop environment and the clusters, NRDStor can lower a barrier of entry and create a more consistent research environment, lessening their time to science. Moreover, this effort can provide undergraduates with valuable professional development experience by incorporating them to assist HCC professionals in administrating the NRDStor resource and pursuing researcher needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346718","CC* Networking Infrastructure: Enhancing network connectivity for data-intensive, multi-institution collaborative science","OAC","Campus Cyberinfrastructure","04/01/2024","03/13/2024","Arman Pazouki","IN","Purdue University","Standard Grant","Kevin Thompson","03/31/2026","$649,951.00","Troy Hege, Deepak Nadig, Norbert Neumeister, Preston Smith","apazouki@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","CSE","808000","","$0.00","This project enhances Purdue University?s external connectivity to the world, delivers fast networking for data intensive research within Purdue, and extends connectivity to under-served research facilities. These network expansions position Purdue to address data challenges in cutting-edge research and enable emerging research such as artificial intelligence across several disciplines. Several educational and workforce development activities, including formal training and mentorship, are included in this project to engage undergraduate students in the deployment and operation of network infrastructure and support experiential and residential learning programs on Purdue?s campus. <br/><br/>With this project, Purdue connects to Indiana GigaPOP and the ESnet network with a 400 Gbps wide-area network. This upgrade enables Purdue university to fully leverage Internet2?s fifth generation backbone as well as the 400 Gbps transatlantic Large Hadron Collider Open Network Environment that includes Purdue as a participating site. Furthermore, the project provides fast connectivity to Purdue research farms, Agronomy, and Animal sciences to support emerging science in under-served facilities. Finally, the project expands Purdue?s science DMZ to enable high-bandwidth, low latency communication required for real-time video analytics and public-private partnerships within the Purdue Discovery Park District.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018905","MRI: Development of an Integrated Instrument for Testing Safety and Robustness of Robotic Co-Workers in Dynamic Environments","CNS","Information Technology Researc, IIS Special Projects","10/01/2020","11/10/2023","Ioannis Poulakakis","DE","University of Delaware","Standard Grant","Marilyn McClure","09/30/2024","$551,682.00","Guoquan Huang, Panagiotis Artemiadis, Herbert Tanner","poulakas@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","CSE","164000, 748400","1189, 9150","$0.00","Merging human living spaces with robot workspaces sparks the development of novel control, estimation and perception algorithms with theoretically provable performance and safety guarantees. However, to date, rigorous experimental protocols and equipment for validating the theoretically predicted performance of the proposed algorithms does not exist. A major reason for this is the difficulty associated with reproducing?in a systematic fashion?the dynamic conditions encountered by robots operating in typical human-centric and natural environments. The ultimate goal of this project is to promote human-robot co-existence by developing a unique new fully integrated sensing and control instrument, which will enable researchers to rigorously evaluate the performance of robotic algorithms in response to systematically induced dynamic perturbations that emulate real-world conditions. This award enables the initiation of the effort to realize the envisioned instrument. In this project, a component of the instrument will be realized to support research on legged locomotion using a bipedal robot integrated into a controllable environment realized by a novel instrumented variable impedance treadmill capable of emulating a wide variety of terrain realities. The unique configuration of this component will allow researchers to evaluate robot locomotion control and planning methodologies and will be used to generate sufficiently rich and expressive datasets, which will be widely shared through the internet to assist research in training models of locomotion through machine learning algorithms. The proposed effort will pave the way toward the development of similar instruments for experimentally testing and verifying the strengths and limitations of robotic algorithms. <br/><br/>This award will result in the development of a stand-alone component of a unique shared-use research instrument, which will make possible?for the first time?measurements that will contribute to the creation of novel experimental protocols for the rigorous assessment of robotic locomotion algorithms.  This project will involve assessment of the robustness of locomotion controllers to terrain variability and uncertainty by performing synchronous measurements of the (i) motion/kinematics of a novel bipedal robot walking on a unique variable stiffness treadmill, (ii) the interaction forces between the robot?s feet and the treadmill, and (iii) the deflection of the treadmill belt when a compliant environment is simulated. Data of this kind are invaluable to studying and assessing the performance of locomotion control strategies on a wide variety of terrain realities. Perturbations like sudden changes in the stiffness of the terrain underneath a leg?including the case where a leg encounters terrain disruption in the form of step-down or step-up disturbances?can be systematically induced and their effect can be measured by the developed instrument. Such capabilities have never been available in the context of human-sized robotic co-workers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2214980","Collaborative Research: CNS Core: Large: Systems and Verifiable Metrics for Sustainable Data Centers","CNS","Information Technology Researc, Special Projects - CNS, CSR-Computer Systems Research","10/01/2022","03/07/2024","Anshul Gandhi","NY","SUNY at Stony Brook","Continuing Grant","Daniel Andresen","09/30/2026","$479,864.00","Erez Zadok, Dongyoon Lee, Zhenhua Liu, Shuai Mu","anshul@cs.stonybrook.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","CSE","164000, 171400, 735400","7925, 9251","$0.00","Data centers already contribute significantly to the global carbon footprint. However, the rise in popularity of resource-intensive Big Data and Machine Learning workloads is poised to make data center operations unsustainable. This project designs a suite of Sustainability Aware Software SYstems (SASSY) to enable ""sustainable-by-design"" data centers. SASSY focuses on sustainability holistically, considering the lifecycle carbon footprint of computing equipment, cleanliness of energy source, and device reliability. To measure per-job end-to-end sustainability costs, a full-stack measurement framework is developed. To involve end-users in sustainability efforts, new programming models and tools are designed to enable users to specify their sustainability and performance objectives. The metrics and models together guide SASSY to make wise data-center-wide sustainable management choices.<br/><br/>The adoption of SASSY solutions leads to sustainability savings that benefit the society at large. Further, the SASSY programming models and tools allow developers to build more sustainable applications, enabling ""sustainable-by-design"" software development. Data center operators and industry partners can directly benefit from SASSY's open-source software and models, which are made public through the project Website: https://www.pace.cs.stonybrook.edu/sassy.html. The next generation of practitioners and researchers are taught to consider sustainability as a first-class metric via educational and mentoring opportunities that the project generates.<br/><br/>This project was in response to and partially funded by Design for Sustainability in Computing (NSF-22-060)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2215789","MRI: Development of Heterogeneous Edge Computing Platform for Real-Time Scientific Machine Learning","OAC","Major Research Instrumentation, Information Technology Researc, CYBERINFRASTRUCTURE","10/01/2022","08/22/2023","Lifang He","PA","Lehigh University","Standard Grant","Alejandro Suarez","09/30/2025","$999,600.00","Wujie Wen, Joshua Agar, Yue Yu, Jieming Yin, Lifang He","lifanghescut@gmail.com","526 BRODHEAD AVE","BETHLEHEM","PA","180153008","6107583021","CSE","118900, 164000, 723100","1189, 9102","$0.00","This project aims to develop a Heterogeneous Edge Computing platform for real-time Scientific Machine Learning (HEC-SML) at the extreme edge. Such a platform will allow for real-time analysis and control of optical, scanning probe, and transmission electron microscopy. Putting computation at the edge ?- as close to the data source as possible -? circumvents latency and bandwidth challenges when sending data to high-performance computing facilities and will enable real-time data analysis to conduct scientific experiments with creative inquiry. The development of HEC-SML will lead to the convergence of microscopy, machine learning (ML), and heterogeneous computing concepts. In microscopy, advanced control systems will enable new imaging modalities; methods for personalized medicine; and discovery and understanding of functional and quantum materials. In ML, HEC-SML will motivate the design of strategies to impose physics constraints and develop optimization methods for training more efficient algorithms. Combined research in these disparate fields creates new objectives that motivate transformative advances in each discipline. The research enabled by HEC-SML will enable advances to and train scientists that can address edge computing challenges for wireless communication, healthcare monitoring, advanced manufacturing, and multi-agent autonomous systems. The instrument will also enable several student curriculum advances along this convergence of fields. This program will engage interdisciplinary researchers in biological sciences, materials science, machine learning, and heterogeneous computing and lead to novel research thrusts. <br/><br/>High-performance computing (HPC) has made tremendous advances in scheduled, parallelized, and distributed computing. Experimental microscopy requires that voluminous data at high velocity is processed in timescales relevant to the experiment (nanoseconds-minutes). HPC facilities cannot meet these needs as they are not designed for dedicated networking and computation and typically do not have heterogeneous compute nodes for low-latency computation. HEC-SML will be a purpose-built instrument for real-time analysis and control of microscopy. A technical innovation is co-locating HPC with microscopy to enable low-cost, reconfigurable, dedicated high-speed networking. With this, we will develop a centralized edge computing platform for signal processing, data reduction, and ML tools for many microscopy modalities. The instrument will provide a platform for real-time analysis and control of optical, scanning probe, and transmission electron microscopy. HEC-SML will provide new capabilities for counting and sorting cells as well as for the characterization and manipulation of materials for energy conversion, sensing, and quantum materials. HEC-SML will provide a turn-key solution for microscopy and other data-intensive scientific experiments. Furthermore, HEC-SML will enable transformative advances in personalized medicine; sensing, energy conversion, and quantum materials; scientific and physics-informed ML methods using experimental data; and codesign of heterogeneous computing and ML for low-latency data reduction, scientific signal processing, and control systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2123343","Collaborative Research: HDR DSC: Infusion of data science and computation into engineering curricula","IIS","HDR-Harnessing the Data Revolu, Information Technology Researc, Info Integration & Informatics, IIS Special Projects","10/01/2021","08/31/2023","Rebecca Napolitano","PA","Pennsylvania State Univ University Park","Continuing Grant","Sylvia Spengler","09/30/2025","$1,463,807.00","Houtan Jebelli, Gregory Pavlak, Ryan Solnosky, Yuqing Hu, Wesley Reinhart, Robert Kimel, Nathan Brown","nap@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","CSE","099Y00, 164000, 736400, 748400","062Z, 7364, 7484, 9102, CL10","$0.00","The goal of this project is to develop a curricular framework for data science education and workforce development that is transferable between diverse institutions, so STEM-related programs can plug and play data science lessons with existing curricula without much overhead. These lessons will be created in conjunction with community stakeholders and industry partners to ensure a focus on real-world problem solving and include student organizations in course development to promote flexible learning pathways. The proposed additions to undergraduate STEM education will provide an evidence-based blueprint for best practices in integrating data science with existing engineering curricula. Implementation across multiple engineering departments will result in a significant impact on society through the training of a diverse, globally competitive STEM workforce with high data literacy. <br/><br/>The objectives of this project are to (1) facilitate data science education and workforce development for engineering and related topics, (2) provide opportunities for students to participate in practical experiences where they can learn new skills in a variety of environments, and (3) expand the data science talent pool by enabling the participation of undergraduate students with diverse backgrounds, experiences, skills, and technical maturity in the Data Science Corps. This work will support the Data Science Corps objective of building capacity for education and workforce development to harness the data revolution at local, state, and national levels. The institutions gathered for this project will develop training programs and curate datasets that will be made available so they can be included in undergraduate instruction nationwide. Furthermore, the training materials will be shared with industry partners, facilitating workforce development. The project team will develop a website to house data science training programs, didactic datasets, and other resources for educators. These resources are intended to reduce barrier to entry for faculty seeking to incorporate data science into their instruction, as recruiting and retaining faculty to create and teach integrated introductory courses in data science has been recognized as a significant hurdle by the National Academies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1933331","2019 Waterman Award","CCF","ATW-Alan T Waterman Award, Information Technology Researc","05/15/2019","05/22/2019","Mark Braverman","NJ","Princeton University","Standard Grant","Phillip Regalia","04/30/2025","$1,000,000.00","","mbraverm@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","049Y00, 164000","041P","$0.00","The National Science Foundation (NSF) has named Dr Mark Braverman as a 2019 recipient of its Alan T. Waterman Award. This award is NSF's highest honor that annually recognizes an outstanding researcher age 40 years or younger and funds his or her research in any field of science or engineering. This year's awardee will receive a $1 million grant over a five-year period for further advanced study in his field.<br/><br/>Dr Braverman is a computer science professor at Princeton University, and is internationally recognized for numerous contributions for his interdisciplinary research at the boundary of computer science and mathematics. His work has yielded new insights into a wide range of areas including: information complexity, pseudorandomness, algorithmic game theory and economics, continuous computation and dynamical systems, machine learning, and applications of computer science in healthcare and medicine.  His work has reinvigorated two of the hottest current research areas in theoretical computer science: information complexity and mechanism design. Dr. Braverman has used information complexity to study the famous direct sum problem for communication complexity, and has settled a longstanding open question by proving a direct sum theorem for randomized communication complexity. Dr. Braverman has also made breakthrough solutions to longstanding problems open for decades, including proving the Linial-Nisan conjecture on pseudorandomness and disproving Krivine's conjecture about the value of the bound in the Grothendieck inequality. <br/><br/>Dr. Braverman is the recipient of several awards, including Best Full Paper Award in Economics and Computation (2018) for the paper ""Selling to a no-regret buyer,"" the European Mathematical Society Prize (2016), the Presburger Award from the European Association for Theoretical Computer Science (2016), a SIAM Outstanding Paper Prize for ""How to compress interactive communication"" (2016), the Stephen Smale Prize from the Society for the Foundations of Computational Mathematics (2014), a Packard Fellowship in Science and Engineering (2014), a Turing Fellowship Award of the John Templeton Foundation (2012), an NSF CAREER Award (2012) and a Sloan Research Fellowship (2011).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346652","Research Infrastructure: CC*Networking Infrastructure: Deep Soil to Supercomputing - Infrastructure Enhancements as a Force Multiplier for Idaho Research","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","04/01/2024","03/29/2024","Dave Lien","ID","Regents of the University of Idaho","Standard Grant","Kevin Thompson","03/31/2026","$628,415.00","Lucas Sheneman","davelien@uidaho.edu","875 PERIMETER DR","MOSCOW","ID","838449803","2088856651","CSE","808000, 915000","9150","$0.00","This network improvement project enables and accelerates data-intensive research at the University of Idaho and beyond by addressing acute network bottlenecks and improving multi-institutional access to shared research infrastructure. Idaho?s unique model for statewide cyberinfrastructure collaboration stems from a collective need to share and maximize limited resources while advancing multi-institutional science efforts. This project accelerates research and education at the University of Idaho while improving access to shared cyberinfrastructure for our peer research universities, primarily undergraduate institutions, and tribal research partners.<br/><br/>This project achieves two primary objectives: 1) replacing legacy multi-mode fiber optics to enable 10Gbps high-speed access to the Deep Soil Ecotron, a new NSF mid-scale infrastructure project, and 2) end-to-end modernization of 100Gbps campus Science DMZ networks through targeted equipment improvements to remove barriers to efficient scientific data exchange.<br/><br/>The Deep Soil Ecotron represents unique research infrastructure hosted at the University of Idaho, allowing U.S. investigators to conduct soil biochemistry experiments at extreme soil depths. This network improvement project replaces aging multi-mode fiber infrastructure to enable efficient data exchange with the Ecotron facility, multiplying the reach and impact of this unique national research resource. Under University of Idaho leadership, Idaho?s research universities co-manage and co-utilize significant shared cyberinfrastructure such as the Falcon supercomputer. This project provides an order-of-magnitude upgrade in data throughput to existing Science DMZ networks supporting Falcon. As multi-institutional dependency on Falcon increases, these improvements are a necessary step to support a growing set of known and future University of Idaho science drivers and collaborations within the state.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835822","Framework: Data: HDR: Extensible Geospatial Data Framework towards FAIR (Findable, Accessible, Interoperable, Reusable) Science","OAC","Data Cyberinfrastructure","10/01/2018","08/18/2022","Xiaohui Carol Song","IN","Purdue University","Standard Grant","Marlon Pierce","09/30/2024","$4,971,003.00","Jian Jin, Uris Lantz Baldos, Venkatesh Merwade, Jack Smith","cxsong@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","CSE","772600","062Z, 077Z, 7925, 9102","$0.00","This project provides seamless connections among platforms, data and tools, making large scientific and social geospatial datasets directly usable in scientific models and tools.  Users with little or no programming experience will be able to create and build data pipelines that collect and process data at multiple scales and convert such data into usable results.  Four case studies demonstrate the capability of the data framework: flood hazard prediction, plant phenotyping, water quality monitoring and sustainable development.<br/><br/>The project creates an extensible geospatial data framework (GeoEDF) to address prevalent geospatial data challenges, support domain science needs, and contribute to a national geospatial data and software ecosystem. Specific objectives include: <br/> - development of a plug-and-play data framework (GeoEDF),<br/> - use of the framework to address domain science needs, <br/> - development of interoperability with other cyberinfrastructures, and <br/> - dissemination of GeoEDF to the broader community. <br/>Use of modular plug-and-play application programming interfaces (APIs) would enable integration of domain-specific geospatial data in a meaningful and accessible manner, leveraging an existing cyberinfrastructure capability (HUBzero) to facilitate adoption and dissemination without reinventing existing components. The project engages a substantial number of domain scientists from a variety of stakeholder communities; by incorporating the framework into HUBzero-powered sites, the team anticipates having access to more than 750,000 users.  Training will be provided to the next-generation of researchers and professionals; internship programs are planned for undergraduate and underrepresented groups. The project allows users / scientists to connect a range of data sources, potentially increasing interdisciplinary work.  The ultimate goal is to put easy-to-use tools and platforms into the hands of researchers and students to conduct scientific investigations using findable, accessible, interoperable, and reusable (FAIR) science principles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844753","CAREER: Cryptocurrency Forensics Tools","CNS","Information Technology Researc, Special Projects - CNS, Secure &Trustworthy Cyberspace","02/15/2019","04/12/2024","Damon McCoy","NY","New York University","Continuing Grant","Karen Karavanic","01/31/2025","$672,522.00","","dm181@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","CSE","164000, 171400, 806000","025Z, 1045, 7434, 8808, 9102, 9178, 9251, CL10","$0.00","Cryptocurrencies, such as Bitcoin, are growing in popularity. These cryptocurrencies offer the promise of increased efficiency and decreasing frictions in the financial system, such as international money transfer fees and costs associated with raising investment capital. Unfortunately, they are also misused as a payment mechanism for illicit activities such as extortion, drugs, human trafficking, and cybercrime. These illicit activities have likely diminished the reputation of these cryptocurrencies and facilitated large amounts of harm for entities and individual people. Improved forensic tools  that can potentially de-anonymize illicit transactions or capture valuable semantic information about a transaction (for example, what was purchased in a particular transaction), could reduce both of these negative effects. This project will conduct open research that will improve our understanding of how to devise improved cryptocurrency forensic techniques that can be adopted by researchers, companies, and investigators. Much of the research into advanced cryptocurrency forensic techniques is currently being performed by companies and integrated into their closed platforms. This has resulted in a lack of public, generalizable understanding of how to detect and understand the illicit activities occurring within these cryptocurrency ecosystems. The advanced cryptocurrency forensics tools developed in this project will be open and enable improved detection of illicit cryptocurrency transactions. This project will also provide students with expertise in financial technology, security, and machine learning which are all areas of broad national needs.<br/><br/>Achieving the goal of improving the efficacy of cryptocurrency forensics tools requires progress on several key challenges.  These include: (1) Designing and implementing improved and open cryptocurrency forensic data collection and archiving systems. (2) Investigating cryptocurrency address labeling techniques. (3) Designing algorithms for improved clustering of addresses and flow tracing. The approach proposed in this project is unique in that it blends improvements to data collection and archiving with advanced machine learning-based algorithms. The project will deepen the understanding of how to effectively perform forensics of a broad range of cryptocurrencies. The ability to effectively forensically analyze cryptocurrencies fundamentally affects our ability to understand and mitigate illicit activities that impact many United States citizens.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2217770","SaTC: Small: Core: Using Markets to Address Manipulated Information Online","CNS","Information Technology Researc, Secure &Trustworthy Cyberspace","10/01/2022","07/21/2023","Marshall Van Alstyne","MA","Trustees of Boston University","Standard Grant","Sara Kiesler","09/30/2025","$654,087.00","Ran Canetti, Mayank Varia","mva@bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","CSE","164000, 806000","025Z, 065Z, 7434, 7923","$0.00","Societies function poorly without free speech.  They also function poorly when members cannot agree on basic facts.  This research seeks to discover technology-aided social structures that minimize the adverse impact of confusion about facts while promoting free speech. To accomplish these goals, the project develops, prototypes, and tests market mechanisms to dissuade sources of information from dissembling, to decentralize detection of false claims, and to change the incentive structure under which producing false claims is cheaper than producing honest news. It also seeks to decentralize governance so that no single party, neither a government nor a private firm, has content moderation authority. Finally, it provides a principled basis for updating Internet and media law concerning platform liability exemptions for user-generated content.<br/><br/>The proposed mechanism extends established economic theories of signaling and screening that allow authors to credibly signal information regarding the veracity of their claims while helping recipients believe which claims are honest. This mechanism puts the burden of proof on the author, in contrast to extant mechanisms that put the burden of proof on the recipients of information or on the platform. Testing is accomplished in a laboratory setting, using randomized control trials, the gold standard for establishing causality. Experimentation tests, for example, whether more honest candidates are more likely to win a tournament and whether more honest firms can sell more products. The claims made by authors will be decentralized.  Based on market design principles, the detection and the adjudication of false claims will also be decentralized. Security, privacy, and anonymity is proposed to be enforced by technology advances in the developed system.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117681","MRI: Acquisition of Cutting-Edge GPU and MPI Nodes for the Interdisciplinary Pitt Center for Research Computing","OAC","Major Research Instrumentation, Information Technology Researc","10/01/2021","09/07/2022","Geoffrey Hutchison","PA","University of Pittsburgh","Standard Grant","Alejandro Suarez","09/30/2024","$1,187,606.00","Lillian Chong, David Koes, Inanc Senocak","geoffh@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","CSE","118900, 164000","1189, 9102","$0.00","Computational science and engineering spans research and education across many disciplines, and state-of-the-art cyberinfrastructure resources are needed to tackle large problems and enable innovative strategies in data-enabled science and engineering. This project will greatly expand the interdisciplinary University of Pittsburgh Center for Research Computing (CRC), the core facility for scientific computing and research at Pitt. CRC supports the work of over 800 users in 59 departments across the entire university. The new expanded hardware will advance both undergraduate and graduate courses and educational experience across a similarly broad range of departments and courses. These expanded resources will enable Pitt to expand a synergy between research and education at all levels, reaching beyond the university to faculty, staff, and students at Howard University, other historically black colleges and universities (HBCUs), and many undergraduate faculty and students both regionally and nationwide. The new resources will also expand scientific computing to students in the Pittsburgh Public School district, including nearby Pittsburgh Science and Technology Academy and Pittsburgh Public Allderdice, both urban schools with diverse student populations.<br/><br/>The funded resources will consist of 16 state-of-the-art graphics processing unit (GPU) computing nodes including NVIDIA Ampere A100 GPU accelerators. Each GPU node will be ~2x faster than previous generation GPUs and 14-50x faster on scientific computing software than current CPU nodes, and will enable increased machine learning productivity. An additional 36 state-of-the-art MPI nodes containing AMD ?Milan? cores and high system memory will enable complex computational simulations. The availability of the new resource will dramatically expand the access and opportunity to GPU and message passing interface (MPI) computing, offering significant speed improvements for an immense range of scientific computing, from machine learning and big data, to quantum chemistry, protein molecular dynamics, energy conversion, nanoparticle catalysis design, weather/wind forecasting, astronomical data analysis, atomistic tunneling electron microscopy measurements, and computer vision. Beyond simple acceleration, the resources will enable transformative research with vastly more accurate weather grids, new machine learning surrogates for quantum chemical calculations of molecular and materials energies and properties, rare-event sampling in protein folding and binding, fMRI neuroscience, and next generation digital astronomy. The resources will immediately benefit over 30 NSF-funded research groups, leveraging over $18 million in research and training grants. The resources will support research in all areas including Chemistry, Computational Biology, Chemical Engineering, Materials Science, Psychology, Astrophysics, Weather Forecasting, Computer Science, and research centers focusing on energy, sustainability, and other key areas of science and engineering. Workshops and courses associated with the new resources will focus on adapting existing software and developing new software for MPI and GPU-computing including a wide range of machine learning methods enabled by the transformation in numeric processing with these expanded resources. These resources will be shared with collaborators at Howard University, other HBCUs, and with regional and national undergraduate schools to broaden participation in computational science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2215017","Collaborative Research: CNS Core: Large: Systems and Verifiable Metrics for Sustainable Data Centers","CNS","Information Technology Researc","10/01/2022","08/09/2022","Syed Rafiul Hussain","PA","Pennsylvania State Univ University Park","Continuing Grant","Daniel Andresen","09/30/2026","$118,777.00","Syed Rafiul Hussain","hussain1@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","CSE","164000","7925","$0.00","Data centers already contribute significantly to the global carbon footprint. However, the rise in popularity of resource-intensive Big Data and Machine Learning workloads is poised to make data center operations unsustainable. This project designs a suite of Sustainability Aware Software SYstems (SASSY) to enable ""sustainable-by-design"" data centers. SASSY focuses on sustainability holistically, considering the lifecycle carbon footprint of computing equipment, cleanliness of energy source, and device reliability. To measure per-job end-to-end sustainability costs, a full-stack measurement framework is developed. To involve end-users in sustainability efforts, new programming models and tools are designed to enable users to specify their sustainability and performance objectives. The metrics and models together guide SASSY to make wise data-center-wide sustainable management choices.<br/><br/>The adoption of SASSY solutions leads to sustainability savings that benefit the society at large. Further, the SASSY programming models and tools allow developers to build more sustainable applications, enabling ""sustainable-by-design"" software development. Data center operators and industry partners can directly benefit from SASSY's open-source software and models, which are made public through the project Website: https://www.pace.cs.stonybrook.edu/sassy.html. The next generation of practitioners and researchers are taught to consider sustainability as a first-class metric via educational and mentoring opportunities that the project generates.<br/><br/>This project was in response to and partially funded by Design for Sustainability in Computing (NSF-22-060)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2025515","MTM 2: The rules of microbiota colonization of the mammalian gut","EF","URoL-Understanding the Rules o, Information Technology Researc, Info Integration & Informatics, IIS Special Projects","09/01/2020","10/13/2020","Georg Gerber","MA","Brigham & Women's Hospital Inc","Standard Grant","Sylvia Spengler","08/31/2025","$2,900,000.00","Harris Wang","ggerber@bwh.harvard.edu","75 FRANCIS ST","BOSTON","MA","021156110","8572821670","BIO","106Y00, 164000, 736400, 748400","068Z","$0.00","Microbiomes, or the collections of trillions of bacteria and other micro-organisms living on, within and around us, have enormous impact on human life. For example, they help people digest food, promote the growth of farm animals and crops, and degrade pollutants in the environment. Despite the importance of microbiomes, the processes governing their formation and maintenance remain poorly understood. The mammalian gut is a particularly intriguing system for microbiome studies, since a diverse collection of microbes has evolved that specifically colonizes and functions in that environment. The goal of the project is to derive fundamental rules that describe and predict the dynamic process of microbial colonization of the mammalian gut. To achieve this goal, the team of investigators will develop new computer-based methods to automatically extract predictive and explanatory rules from large microbiome data sets. The team will also develop new experimental tools and generate data sets in mouse measuring how microbiomes change over time and across space in the mammalian gut. Overall, the project will further the understanding of the formation of microbiomes in mammals and can provide broader insights into the emergence of other microbial ecosystems, such as those in soil and marine environments. These insights could ultimately help scientists to rationally alter or maintain microbiomes in different environments to benefit human activities. The project will also generate practical resources for the scientific community (computer-based tools and datasets) and provide education on the microbiome to college and elementary school students through courses and hands-on labs.<br/><br/>A wealth of genomic data provides information as to which microbes are present in environments, but little insight into underlying factors that explain or predict complex assemblages of microbial consortia. This project aims to elucidate mechanistic factors that drive the dynamic process of microbial colonization of the mammalian gut. These determinants will be investigated at multiple systems scales, from the level of microbial communities down to the level of individual genes. The project will leverage high-throughput experimental methods developed by the investigators, to generate data characterizing functional genetic selection and spatial organization of microbiota in the mammalian gut. From the Computer Science perspective, the project will develop new computational methods to infer human-interpretable rules and other structured outputs from complex and noisy high-throughput microbiome datasets, using Bayesian and neural-style approaches that incorporate prior biological knowledge while scaling to massive datasets. This project has three main thrusts: 1) Learn microbial community-level rules that quantitatively predict population dynamics of mouse gut colonization and assess these rules across differing ranges of microbial diversity and composition, 2) Elucidate microbial gene-level mechanisms that predict mouse gut colonization dynamics, and 3) Profile microbial spatiotemporal organization and dynamics during gut colonization at the species and gene level to predict microbial community dynamics. The project is expected to establish a set of new computational and experimental tools and principles for understanding the rules of microbial colonization of the gut, with potential applications to other ecosystems including gut microbiota of non-mammalian species as well as complex environmental microbiota.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232803","CC* Data Storage: Institutional Storage for the University of Notre Dame (NDStore)","OAC","Campus Cyberinfrastructure","09/01/2022","08/26/2022","Jaroslaw Nabrzyski","IN","University of Notre Dame","Standard Grant","Kevin Thompson","08/31/2024","$500,000.00","Nitesh Chawla, Jane Livingston, Bradley Smith, Michael Pfrender","naber@nd.edu","836 GRACE HALL","NOTRE DAME","IN","465566031","5746317432","CSE","808000","9102","$0.00","This project equips the Center for Research Computing at the University of Notre Dame (ND CRC) and its scientific users across all Notre Dame colleges and departments to enable transformative research in social and physical sciences and engineering domains through the acquisition of institutional storage called NDStore. Major beneficiaries of NDStore are researchers utilizing various research cores at Notre Dame, such as the Genomics and Bioinformatics Core and the Notre Dame Integrated Imaging Facility, as well as other researchers from University Centers and Institutes, such as the Institute for Data and Society, in addition to the broader national community via the Open Science Grid. Together, these different facilities and researchers generate hundreds of terabytes of data per year, and they enable expert users to address the most complex research problems of today?s world. The major capabilities provided by NDStore accelerate existing research otherwise throttled by insufficient storage capability. They also enable full data lifecycle at previously inaccessible scales, enable new national data-intensive collaborations, and incubate new research projects.  <br/><br/>NDStore brings to Notre Dame an additional 2 petabytes of storage capacity for data manipulation, curation, and long-term preservation, as well as 250 terabytes of fast scratch storage for machine learning-related workloads. NDStore is a highly available solution based on an open-source Ceph-based storage clustering standard. It was designed with the flexibility to meet the needs of researchers at various stages of their research. NDStore provides a clear benefit to researchers who generate data with various instruments in core facilities and need to transfer the data to their home directories for analysis and curation before the data is shared with their communities.  Before this project was funded, the amount of storage provided by ND CRC to each faculty lab was not satisfactory for most of the users dealing with large data coming from instruments at core facilities, such as microscopes, sequencing machines, or other benchtop devices. In addition, ND CRC?s high-performance scratch storage system has been shared between high-performance computing and machine learning workloads; very often, mixing these workloads led to performance bottlenecks, negatively impacting all of the storage system users at Notre Dame. NDStore helps Notre Dame create an independent scratch system for machine learning workloads.<br/><br/>Another important aspect of the intellectual merit of this project is the opportunity for the CRC to deploy NDStore in such a way that the entire data lifecycle at Notre Dame?s research enterprise is supported. Research data use cases at ND are highly diverse, complex, and heterogeneous. They differ in types of data captured, scientific instruments used, data processing and analyses conducted, policies and methods for data sharing and use, and, internal to the lab, cyberinfrastructure-related knowledge. Data life cycle stages include: 1) data capture; 2) initial processing near the instrument(s); 3) central processing at data centers or clouds; 4) data storage, curation, and archiving; and 5) data access, dissemination, and visualization. Until NDStore was deployed, Notre Dame infrastructure could adequately support only stages 1-3 and 5, with very minimal support for stage 4. NDStore fills this gap.  NDStore will also be integrated into classroom and undergraduate internship programs hosted by numerous faculty in colleges and ND CRC. Through user training, research experience for undergraduates, pre-college programs for high school students, workshops, internships, and experiential training programs, ND CRC will ensure that NDStore has the broadest possible impact on the local and national academic community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018308","CC* Integration-Small: Science Traffic as a Service (STAAS)","OAC","Special Projects - CNS, CISE Research Resources, Campus Cyberinfrastructure","10/01/2020","06/15/2020","John Brassil","NJ","Princeton University","Standard Grant","Deepankar Medhi","09/30/2024","$514,208.00","Jennifer Rexford","jbrassil@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","171400, 289000, 808000","9251","$0.00","Future advances in scientific research will require computing on massive datasets and high bandwidth streaming scientific instrument data. New experimental research infrastructures will be required to advance the understanding of the networks capable of supporting these increasingly demanding science data flows. Testing advances in networking technologies and protocols with actual high-speed science data traffic is vital to networking experimenters, scientific instrument users, and data scientists. To address this need, this project will develop a prototype of a decentralized computing and networking system to create, collect and distribute a diverse collection of real and synthetic science traffic flows to the experimental research infrastructure user community. The proposed work will first develop and deploy the Science Traffic as a Service (STAAS) prototype on the Network Programming Initiative testbed connecting two US universities, and then prepare STAAS for later nationwide deployment on the FABRIC midscale networking research infrastructure now under development. The students exposed to research on networking testbeds with demanding science traffic workloads will learn skills to help strengthen a workforce prepared to advance the global-scale computing cloud application service platforms that are increasingly central to the US economy. All documents, software, presentations, and other artifacts created under this project will be made publicly available at http://www.cs.princeton.edu/~jbrassil/public/projects/staas/<br/><br/>The key project insight is that many science flows are already in transit at any moment on or between campuses. Using new campus cyberinfrstucture including passive optical Test Access Points, Network Packet Brokers, and data-plane programmable ethernet switches, STAAS will safely tap and forward copies of these flows onto the experimental testbed, while preserving both the timing integrity of the flows and the data privacy of their payloads. Large scale, high bandwidth experiments will be achieved by enlisting participation of many or all STAAS edge nodes on multiple campuses. By introducing a service-based model, STAAS can help advance the networking research community's transport of emerging science data, and help the operators of scientific instruments increase the amount and quality of data collected by their instruments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346713","CC* CIRA: Bridging the Digital Chasm HPC for ALL","OAC","Campus Cyberinfrastructure","05/01/2024","04/11/2024","Michael Kirby","CO","Colorado State University","Standard Grant","Kevin Thompson","04/30/2026","$200,000.00","Kelly Wrighton, Candace Ramsey","kirby@math.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","CSE","808000","","$0.00","This two-year planning effort develops an inclusive vision for sustainable High-Performance Computing (HPC) at Colorado State University to support knowledge discovery in the era of big data.  To accomplish this, a series of campus workshops and planning sessions are being held to foster engagement with stakeholders, as well as broaden participation in data-driven knowledge discovery.  These activities include planning the development of training and educational materials to enhance accessibility of data-intensive computing to the campus community, addressing the integration of new parallel computing technologies with faculty and student research, and strengthening computing partnerships both regionally and nationally.<br/><br/>In conjunction with these user targeted activities, there is a simultaneous effort to update the university cyber-infrastructure plan including addressing the transmission, storage, and hosting of large data sets.  An overarching goal of this planning phase is the development of a self-sustaining infrastructure that enables the campus community to cultivate expertise and contribute to workforce development in HPC, an area of national need.  The broader impact of fostering a culture of high-performance computing and data driven discovery at CSU is the enhanced ability of faculty, students and graduates to make fundamental discoveries that improve society, e.g., predictive analytics for pandemic preparedness, climate and weather prediction, food supply through digital soil and crop sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209767","Collaborative Research: Elements: Towards A Scalable Infrastructure for Archival and Reproducible Scientific Visualizations","IIS","Data Cyberinfrastructure","09/15/2022","09/06/2022","Jian Huang","TN","University of Tennessee Knoxville","Standard Grant","Cornelia Caragea","08/31/2025","$316,168.00","","huangj@eecs.utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","CSE","772600","077Z, 8004","$0.00","Today?s science revolves around leading edge datasets ? data that scientists need to carefully analyze so that they can draw reliable scientific conclusions. The rate at which these leading-edge datasets are becoming larger and more complex is accelerating every day. In many ways, having access to a dataset does not equal to, or even come close to, having access to the insights in the dataset. This nuanced but crucial difference in accessibility creates a deep barrier to making scientific results reproducible. To this end, ?Accessible Reproducible Research?, published by Science in 2010, presented a system for reproducible research.  A decade later, unfortunately, accessible reproducible research is still in its infancy. It turns out that this barrier is much more fundamental than previously believed, even though on the surface it seems solvable by investing resources and setting guidelines and policies. The real challenge is that the computing toolsets, the working environments, and the work processes of the original team of scientists are very difficult for a different team of scientists to recreate with precision. Such difficulty stems from the rapid speed at which computing technology is advancing; so that freezing a computing environment in a practical manner is nearly impossible. In addition, scientific intuition is difficult to codify, simply documenting a new idea is not enough to communicate what a scientist saw before pursuing that idea. From that respect, making accessible reproducible research a reality requires better methods and tools. In this project, the investigators will focus on the visualization step of data analysis, which is a central component of scientific discovery. This project?s aim is to develop an Archiving Infrastructure for Reproducible Interactive Visualization (AIRIV). Through this infrastructure, the investigators will demonstrate how visual explorations of large and complex data can be reliably captured, efficiently stored, easily shared, and freely reused by any user. This project will improve accessibility of reproducible research and promote the progress of science. For areas such as medicine and pharmaceutical research, this project will provide an unprecedented channel to accelerate translational research and advance the national health.<br/><br/>This project will build upon research funded by a prior NSF CISE Research Infrastructure award. In that previous project, the investigators found a method to capture interactive user experience of visualization tools, and to share the captured experience without the need to share the original software or the original data. Furthermore, during the reuse of a captured experience, the user has freedom to explore beyond the exact sequence of how the previous user has used the tool with a method called Loom. In this new project to create AIRIV, the investigators will focus on web-based visualization dashboards, which represent the standard way for scientists around the world to interact with their data and derive insights. This project will first build a general AIRIV Javascript library that can be imported by any web browser-based application. Using the AIRIV library, developers of web-based visual dashboards can easily implement automatic generation of Loom objects into their dashboards. Developers will be able to instrument their applications to store new provenance information with Loom objects as well. The investigators will then conduct performance and scaling tests to understand the tradeoffs between hosting choices under settings of local, institutional clusters, and community shared data infrastructures. Operators of scientific facilities can use the findings to help science communities make informed choices as to where and how to host scientific visualization archives for better share-ability and cost efficiency. The investigators will also develop machine learning methods that can compare Loom objects and externalize commonalities and patterns in an entire archive of Loom objects. Such new methods will lead to creating a search by example functionality for AIRIV archives. For requirements collection, continuous improvement, and deployment testing, the investigators will engage the Mayo Clinic & Illinois Alliance, which serves as a framework for several technologies in healthcare, many of which center around the research and development of dashboard/analytical tools. We target two such analytics efforts, OmiX and KnowEnG, both of which are developed at National Center for Supercomputing Applications (NCSA).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1231216","A Center for Brains, Minds and Machines: the Science and the Technology of Intelligence","CCF","STC Integrative Partnrshps Adm, Information Technology Researc, STCs-2013 Class","09/01/2013","01/25/2024","Tomaso Poggio","MA","Massachusetts Institute of Technology","Cooperative Agreement","Phillip Regalia","08/31/2025","$48,079,625.00","Ellen Hildreth, Haym Hirsh, Lakshminarayana Mahadevan, Matthew Wilson, Gabriel Krieman","tp@ai.mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","CSE","129700, 164000, 720200","102Z, 8089, 8091, 9171, 9218, 9251","$0.00","Today's AI technologies, such as Watson, Siri and MobilEye, are impressive yet still confined to a single domain or task. Imagine how truly intelligent systems --- systems that actually understand their world --- could change our world.  The work of scientists and engineers could be amplified to help solve the world's most pressing technical problems. Education, healthcare and manufacturing could be transformed.  Mental health could be understood on a deeper level, leading in turn to more effective treatments of brain disorders.  These accomplishments will take decades.  The proposed Center for Brains, Minds, and Machines (CBMM) will enable the kind of research needed to ultimately achieve such ambitious goals. The vision of the Center is of a world where intelligence, and how it emerges from brain activity, is truly understood. A successful research plan for realizing this vision requires four main areas of inquiry and integrated work across all four guided by a unifying theoretical foundation. First, understanding intelligence requires discovering how it develops from the interplay of learning and innate structure. Second, understanding the physical machinery of intelligence requires analyzing brains across multiple levels of analysis, from neural circuits to large-scale brain architecture. Third, intelligence goes beyond the narrow expertise of chess or Jeopardy-playing computers, bridging several domains including vision, planning, action, social interactions, and language. Finally, intelligence emerges from the interactions among individuals ? it is the product of social interactions. Therefore, the research of the Center engages four major research thrusts (Reverse Engineering the Infant Mind, Neuronal Circuits Underlying Intelligence, Integrating Intelligence, and Social Intelligence) with interlocking teams and working groups, and a common theoretical, mathematical, and computational platform (Enabling Theory).<br/><br/>The intellectual merit of the Center is its focus on elucidating the mechanisms and architecture of intelligence in the most intelligent system known: the human brain.  Success in this project will ultimately enable us to understand ourselves better, to produce smarter machines, and perhaps even to make ourselves smarter. The Center's potential legacy of a deep understanding of intelligence, and the ability to engineer it, is tantalizing and timeless. It includes the creation of a community of researchers by programs such as an intensive summer school, technical workshops and online courses that will train the next generation of scientists and engineers in an emerging new field -- the Science and Engineering of Intelligence. This new field will catalyze continuing progress in and cross-fertilization between computer science, math and statistics, robotics, neuroscience, and cognitive science. Sitting between science and engineering, it will attract growing interest from the best students at all levels.  The broader impact of the Center program could be to revolutionize K-12, and also 0-K, and 12-life with a deeper understanding of the process of learning.  The ability to build more human-like intelligence in machines will transform our productivity, enabling robots to care for the aged, drive our cars, and help with small-business manufacturing. The Center team is composed of over 23 investigators, many having already made significant accomplishments in multiple research areas relevant to the science and the technology of intelligence. The Center team has a mix of junior and senior researchers, bringing expertise in Computer Science, Neuroscience, Cognitive Science and Mathematics. The institutional partners include nine institutions (MIT, Harvard, Cornell, Rockefeller, UCLA, Stanford, The Allen Institute, Wellesley, Howard, Hunter and the University of Puerto Rico), three of which have significant underrepresented student populations. The academic institutions are complemented by the Center's industrial partners (Microsoft, IBM, Google, DeepMind, Orcam, MobilEye, Willow Garage, RethinkRobotics, Boston Dynamics) and by world-renowned researchers at international institutions (Max Planck Institute, The Weizmann Institute, Italian Institute of Technology, The Hebrew University)."
"2145640","CAREER: Knowledge-enhanced and interpretable radiology report generation","IIS","Information Technology Researc, Info Integration & Informatics","07/15/2022","08/16/2022","Yifan Peng","NY","Joan and Sanford I. Weill Medical College of Cornell University","Continuing Grant","Sylvia Spengler","06/30/2027","$597,031.00","","yip4002@med.cornell.edu","575 LEXINGTON AVE FL 9","NEW YORK","NY","100226145","6469628290","CSE","164000, 736400","102Z, 1045, 7364","$0.00","The radiology report is the primary mean of communication between radiologists and referring physicians, and which also serve as a legal document. To date, many studies have demonstrated the feasibility of using deep learning to automatically generate radiology reports from chest x-rays. However, existing approaches utilize only current chest x-ray images and do not consider historical images, associated electronic health records (EHRs), and domain-specific prior knowledge. Therefore, the current computer-generated reports are far from accurate and complete. To bridge this gap, there is a critical need to study new report generation techniques to handle large-scale, real-world healthcare data. This project will employ novel informatics and data science techniques to automatically generate clinical reports to improve workflow efficiency and improve healthcare outcomes. From the perspectives of biomedical informatics, our approach will leverage the wealth of information from EHR to profoundly understand the role of natural language, image analysis, and deep learning in report generation. From the perspective of clinical translation, this project will facilitate radiologists? workflow, improve clinical accuracy and efficiency, and enhance decision-making. Additionally, the project will closely integrate research with education, by launching a new graduate Natural Language Processing and Health course and supporting several capstone and specialization projects. It will also broaden the outreach from the investigators to non-computer-science graduate students, who will be exposed to working principles of NLP through our extensive collaborative efforts. <br/><br/>This project will develop and validate a framework to automatically generate radiology reports using longitudinal, multimodal EHR data and domain knowledge. The investigator will attain the overall objective by pursuing four aims. First, the project will build a memory-enhanced report generation system to handle longitudinal chest x-rays and reports. Second, the project will build a radiology-specific knowledge graph from multimodal EHR and inject it into the report generation framework. We will employ a novel approach to construct such radiology-specific knowledge graph, by modeling heterogeneous multi-dimensional EHR data in our model. Third, we will create a new rationale-based model that supports rationale-base interpretabilityFinally, the project will build and evaluate a prototype user-centered reporting system with a user-friendly graphic user interface. The new reporting system will enhance communication between radiologists and referral physicians, particularly in large and heterogeneous EHR. The proposed research is creative and original because it represents a step towards building automatic systems with a higher-level understanding of radiology knowledge and decision-making. It is expected to open research horizons and employ techniques and theories from data science to support next-generation medical diagnostic reasoning from chest x-rays and structured EHR.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1942902","CAREER: Pushing the Limits of Low-Power Wide-Area Networks","CNS","Information Technology Researc, Networking Technology and Syst","02/01/2020","04/09/2024","Swarun Kumar","PA","Carnegie-Mellon University","Continuing Grant","Murat Torlak","01/31/2025","$522,479.00","","swarun@cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","CSE","164000, 736300","044Z, 1045","$0.00","Future smart cities will have to provide Internet connectivity to millions of energy-starved Internet-of-<br/>Things devices - even those several miles away from the nearest wireless base station. To this end, recent<br/>years have seen novel Low-Power Wide Area Networks (LP-WANs) deployed, such as LoRa and NB-IoT.<br/>Unfortunately, today?s LP-WAN technologies require a battery to power these devices, which impacts<br/>their maintenance cost and reliability. This proposal aims to develop a city-scale LP-WAN architecture<br/>that offloads connectivity, sensing and even the source of energy to the more capable base stations and<br/>cloud infrastructure. We develop an architecture where base stations actively collaborate to enable faster<br/>and more reliable connectivity to low-power devices, sense their location and surroundings and provide<br/>energy they require to operate in the first place. The proposed work has a detailed integrated education and<br/>outreach program including course projects that leverage a live testbed and summer labs for K-12 students<br/>in Greater Pittsburgh. The proposed work will be deployed initially to network existing smart sensors at<br/>CMU and later scale to surrounding city infrastructure, through extensive collaborations with the City of<br/>Pittsburgh, forming a first-of-its-kind university-led programmable LP-WAN deployment.<br/><br/>The proposed work enables Low-Power Wide-Area Networking in dense smart-city deployments along<br/>three axes: (1) Long-Range Wireless Charging: First, the proposal investigates a system at the base stations<br/>that beams power to battery-free clients at unknown locations. We explore base station and client designs to<br/>perform wireless charging over large distances, orders-of-magnitude higher than prior work. (2) Pushing<br/>LP-WAN Speed and Scaling Limits: Next, the proposed work explores a solution to decode collisions from<br/>even the weakest of LP-WAN signals that cannot be detected at any single base station. The proposal argues<br/>why power starved LP-WAN clients motivate the need for even mundane PHY-layer functions at the client:<br/>radio configuration, decoding and rate adaptation - to be offloaded to the more capable infrastructure.<br/>The proposal then explores intelligent ways for base stations to collaborate at the cloud, despite limited<br/>backhaul bandwidth. (3) City-Scale Wireless Sensing: Finally, the proposed work investigates novel sensing<br/>solutions that use RF signal measurements received both from city-wide low-power radios as well as<br/>CubeSats in Low-Earth Orbit, collated at the edge and cloud. We evaluate how such a system can sense<br/>properties such as structural health and atmospheric moisture at city-scale, without requiring dedicated<br/>sensors on low-power devices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209630","Collaborative Research: Elements: TRAnsparency CErtified (TRACE): Trusting Computational Research Without Repeating It","OAC","Data Cyberinfrastructure, Software Institutes","07/15/2022","07/08/2022","Thu-Mai Christian","NC","University of North Carolina at Chapel Hill","Standard Grant","Sylvia Spengler","06/30/2025","$99,995.00","","thumai@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","772600, 800400","077Z, 7923, 8004, 9102","$0.00","Research communities across the natural and social sciences are increasingly concerned about the transparency and reproducibility of results obtained by computational means. Calls for increased transparency can be found in the policies of peer-reviewed journals and processing pipelines employed in the creation of research data products made available through science gateways, data portals, and statistical agencies. These communities recognize that the integrity of published results and data products is uncertain when it is not possible to trace their lineage or validate their production. Verifying the transparency or reproducibility of computational artifacts?by repeating computations and comparing results?is expensive, time-consuming, and difficult, and may be infeasible if the research products rely on resources that are subject to legitimate restrictions such as the use of sensitive or proprietary data; streaming, transient, or ephemeral data; and large-scale or specialized computational resources available only to approved or authorized users. The TRACE project is addressing this problem through an approach called certified transparency - a trustworthy record of computations signed by the systems within which they were performed. Using TRACE, system owners and operators certify the original execution of a computational workflow that produces findings or data products. By using a TRACE-enabled system, researchers produce transparent computational artifacts that no longer require verification, reducing burden on journal editors and reviewers seeking to ensure reproducibility and transparency of computational results. TRACE presents an innovative and efficient approach to ensuring the transparency of research that uses computational methods, is consistent with the vision outlined by the National Academies, and enables evidence-based policymaking based on transparent and trustworthy science.<br/><br/>The central goal of the TRACE project is the development, validation, and implementation of a technical model of certified transparency. This includes a set of infrastructure elements that can be employed by system owners to (1) declare the dimensions of computational transparency supported by their platforms; (2) certify that a specific computational workflow was executed on the platform; and (3) bundle artifacts, records of their execution, technical metadata about their contents, and certify them for dissemination. The first phase of the project focuses on the development of a conceptual model and technical specification that can be used to certify the description of a system, termed a Transparency-Certified System (TRACE system), and the aggregation of artifacts along with records of their execution, termed Transparency-Certified Research Objects (TROs).  The second phase focuses on the development of reusable software components implementing the TRACE model and approach. To demonstrate certified transparency, the toolkit is used to TRACE-enable existing platforms including Whole Tale, SKOPE, and the SLURM workload manager. These TRACE-enabled systems produce certified TROs that can be trusted and do not need to be repeated or re-executed to verify that results were obtained as claimed.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Social and Economic Sciences within the Directorate for Social, Behavioral and Economic Sciences; and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2103936","Elements: Collaborative Research: Community-driven Environment of AI-powered Noise Reduction Services for Materials Discovery from Electron Microscopy Data","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure, Software Institutes","06/01/2021","05/24/2021","Carlos Fernandez Granda","NY","New York University","Standard Grant","Varun Chandola","05/31/2025","$299,926.00","","cfg3@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","CSE","171200, 772600, 800400","054Z, 077Z, 094Z, 095Z, 7923, 8004, 8396, 8397","$0.00","The goal of this project is to create cyberinfrastructure (CI) powered by artificial intelligence (AI) for sustained innovation in materials science. Deep understanding of materials is critical for progress in technologies related to energy, communication, construction, transportation and human health. The revolutionary progress of deep learning has been enabled by the availability of open-source AI models and open-access benchmark databases. However, the existing codebases and datasets relevant to image processing focus mostly on photographic images. In order to promote the sustained development of AI technology that can have significant impact in materials science, it is critical to provide data and AI models that are tailored to this domain. The developed CI will address this need by providing software to process images obtained from electron-microscopes, a technique enabling atoms to be visualized, and has the potential to enable transformative breakthroughs in varied and important areas of materials science. The CI is explicitly designed to foster the growth of a sustainable community of users and developers of AI technology at the intersection of the materials and data science communities, and to empower materials scientists to simulate their own datasets and develop their own AI models for scientific discovery. The developed AI-powered CI will therefore enable transformative progress in atomic-level understanding of materials, which will have broader impacts in health, energy, environment, and biotechnology. The CI environment will contribute to training materials scientists in AI technology, connecting them to the AI community, and providing software, data, and support materials to initiate them in AI-powered research. Educational and outreach plans are designed to facilitate interactions between the materials science and AI communities. Outreach activities specifically targeted to the general public, and to high-school teachers and their students, will expose them to materials science, electron microscopy, and AI. The project is committed to providing opportunities to women and underrepresented groups and will prioritize diversity in collaboration with the NYU Center for Data Science diversity committee.<br/><br/>Developing a fundamental understanding of atomic level structure and dynamics is critical for transformative advances in materials science. Aberration-corrected transmission electron microscopy is a primary tool to accomplish this goal. Unfortunately, the information content of microscopy data may be severely limited by poor signal-to-noise ratios. This is particularly true for radiation sensitive materials and experiments where high time resolution is required to investigate dynamic kinetic processes. AI methodology can exploit prior information about material structure by training deep neural nets with extensive simulations. These approaches may significantly outperform existing state-of-the-art methods, especially for non-periodic structures, including defects, interfaces, and surfaces. The developed CI will provide AI noise reduction services which will yield immediate advances and impacts for zeolites, metal organic frameworks, protein-material interfaces, liquid phase nucleation and growth, liquid-solid interfaces, and fluxional behavior in catalytic nanoparticles. In addition, the project will advance methodology for the design of AI-oriented CI. The CI is strategically designed to create a holistic environment for the use and development of AI technology in a specific scientific domain. It will attract domain scientists with little AI expertise, by providing software where the AI technology is transparent to the end user. Exposure to the technology will motivate the scientific community to design and train their own models, which will be facilitated by the open-source codebase in the AI repository. The open-access database combined with the repository will attract AI practitioners with little domain expertise, by giving them access to well-curated data and a clear specification of the relevant AI tasks. These services will be jump-started and supported through multiple educational and outreach activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018811","CC* Regional:  Promoting Research and Education at Small Colleges in Alabama through Network Architecture Enhancements","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/15/2020","08/17/2022","Samuel D'Angelo","GA","Georgia Tech Research Corporation","Continuing Grant","Kevin Thompson","06/30/2024","$730,545.00","Kylie Nash, David Bourrie, Damian Clarke","cas.dangelo@oit.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","303186395","4048944819","CSE","723100, 808000","","$0.00","Advancements in data-intensive scientific instrumentation have greatly surpassed the capability of some campus networking infrastructures to effectively connect data-producing facilities to powerful computing and storage systems. Georgia Tech (GT) working in partnership with Southern Light Rail (SLR) and its high-speed research network, Southern Crossroads (SoX), has established this project to increase connectivity to smaller and HBCU institutions in Alabama. As a result of this project, both Alabama Agricultural and Mechanical University (AAMU) and the University of South Alabama (USA) are able to transition their connectivity from a low-bandwidth ISP to a true high-speed R&E network to help increase their research efforts. Since their IT networking staff and budget are smaller and do not possess the expertise or funding in procuring and managing multiple internet providers, this award is allowing GT to install pre-configured hardware appliances for connectivity, performance management, and large data transfers at SoX.<br/> <br/>With the enhancement of the network at AAMU, they are able to enhance their research on the following: UAVs to manage agricultural data collection and analysis, astrophysics visualization, and virtual mentoring research. Similarly at the University of South Alabama, this project is leading to an increase in the data transfer between their university and industry partners eliminating the need to exchange physical hard drives. This is expanding their research in the field of multi-spectral imaging for medical and other life science applications, as well as analysis of sensor data from airplanes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346712","CC* Networking Infrastructure: Building a Scalable and Polymorphic Cyberinfrastructure for Diverse Research and Education Needs at Illinois State University","OAC","Campus Cyberinfrastructure","05/01/2024","03/29/2024","Yongning Tang","IL","Board of Trustees of Illinois State University","Standard Grant","Kevin Thompson","04/30/2026","$611,586.00","Olcay Akman, George Barnes, Charles Edamala, Craig Jackson","ytang@IllinoisState.edu","CAMPUS BOX 1100","NORMAL","IL","617901100","3094382528","CSE","808000","","$0.00","Faculty researchers and networking experts at Illinois State University are teaming up to create an advanced cyberinfrastructure to meet the evolving and diverse research and educational demands of the campus. This cyberinfrastructure boosts the campus research network to a full 100G capacity, integrates a 100G Science DMZ for better external connections, and introduces a 100G Software Defined Networking (SDN) enabled testbed for creative research opportunities. This enhancement dramatically improves digital interaction among researchers, scientific instruments, visualization stations, high-performance computing (HPC) resources, and external collaborators, elevating research speed and enabling methodologies previously out of reach. For example, large environmental and hydrology data collected by unmanned aircrafts can now be managed through the SDN, quickly processed by HPC clusters, and shared with external collaborators for immediate machine-learning analysis. This infrastructure also benefits students, particularly those from underrepresented groups and first-generation college attendees, by providing them with hands-on experience with top-tier technology in their studies and research projects. <br/><br/>The cyberinfrastructure's modular design caters to varied research and educational requirements. The Science DMZ module, featuring a high-performance switch and a cutting-edge data transfer node, isolates research data traffic from general university operations. The on-campus research network employs a spine-leaf architecture for its scalability and reliability, consisting of two interconnected spines (64x100GbE) using 100Gb links, located in the HPC and another campus data center. The SDN-based testbed adopts a similar spine-leaf architecture with a single Spine for now, prized for its adaptability and equipped with hybrid SDN switches that have built-in controllers for innovative research exploration. Performance monitoring and adjustments are carried out with PerfSonar nodes, in a collaborative effort between researchers and network architects to ensure the cyberinfrastructure's optimal performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2042411","CAREER: Environmentally-Mediated Coordination in Natural and Robot Swarms","CNS","FRR-Foundationl Rsrch Robotics, Information Technology Researc","04/01/2021","08/31/2023","Kirstin Petersen","NY","Cornell University","Continuing Grant","David Corman","03/31/2026","$474,235.00","","kirstin@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","CSE","144Y00, 164000","1045, 6840, CL10","$0.00","Autonomous robot swarms are becoming ubiquitous with thousands of robots and drones operating simultaneously in places from warehouses to entertainment light shows. This technological revolution makes it critical to look beyond robots working in parallel, and towards actual swarm intelligence where the whole is more than the sum of its parts. As a proof of concept, natural swarms exhibit scalable, error tolerant, and adaptive properties by integrating and propagation information into their shared environment over space and time. Adoption of these concepts in robot swarms can complement existing control architectures, and may result in systems that are less efficient than those with centralized control architectures, but are much faster and inexpensive to deploy, are resilient to individual failures, resize easier from initial to full-scale deployment, and can adapt to changing tasks or environments.<br/> <br/>Although gaining in popularity, this type of distributed coordination has many facets and is still poorly understood - especially as a design tool for engineered swarms that aim to achieve biological levels of resilience and adaptability. To address these shortcomings, this Faculty Early Career Development (CAREER) research project will extend upon the concept of environmentally-mediated coordination from working on perfect robots operating in static environments, to include dynamic environments and ways to deal with realistic bounds on error and hardware reliability. The work will result in a model of swarms in dynamic environments that act to integrate, diffuse, decay, and filter information derived from characterization of a biological model system, as well as practical robot experiments. Practical verification involves collective robotic construction, odor plume tracking by honey bees, and strain-mediated behaviors in programmable matter.  This project further involves aims to secure and increase such a diverse workforce through novel methods for inclusive, shared, online robotics curricula; cross-generational outreach programs for the public; interdisciplinary student projects; and workshops for researchers across the fields of robotics, biology, and architecture.<br/><br/>This project is supported by the cross-directorate Foundational Research in Robotics program, jointly managed and funded by the Directorates for Engineering (ENG) and Computer and Information Science and Engineering (CISE).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346667","Research Infrastructure: CC* Data Storage: Broadening UMBCs Data Storage footprint to Advance Scientific Research and Discovery","OAC","NDCC-Natl Discvry Cloud Climat, Campus Cyberinfrastructure","04/01/2024","03/29/2024","Damian Doyle","MD","University of Maryland Baltimore County","Standard Grant","Amy Apon","03/31/2026","$498,122.00","Vandana Janeja, Cynthia Matuszek","damian@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","CSE","295Y00, 808000","","$0.00","The Retriever Research Storage System R-RSTOR enables UMBC researchers and collaborators to address important research in atmospheric science, analyze polar ice caps, and develop AI models for human and robotic interactions.  The system supports increased collaborations with the University of Maryland Center for Environmental Science (UMCES) on research into the ecology of the Chesapeake Bay and its watershed, as well as broader community access through the Open Science Data Federation. Utilizing the open-source Ceph file system that provides a flexible, high-performance, cost-effective, and scalable file architecture that will allow us to greatly expand allocations for all faculty conducting research.<br/><br/><br/>This system integrates with UMBC's existing High-Performance Computing Facility and Science DMZ to streamline interdisciplinary and inter-institutional collaborations. R-RSTOR is designed to utilize low latency non-volatile memory express (NVME) components and large-scale multi-petabyte solid state disc to reduce complexities and costs associated with data manipulation and transfer, which shortens the time to research. The Ceph file system allows researchers the ability to handle file, block, and object-based file structures within a single system. Through the additional gateways running on the system, researchers can utilize application programming interfaces (APIs) for enhanced data mobility between on-premises and cloud environments, accommodating evolving computational requirements as more workloads shift to a hybrid cloud model.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is also supported by National Discovery Cloud for Climate (NDC-C) resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018611","MRI: Development of an Instrument for Student and Faculty Research on Multimodal Environmental Observations","CNS","Major Research Instrumentation, Information Technology Researc","10/01/2020","09/07/2022","Naphtali Rishe","FL","Florida International University","Standard Grant","Deepankar Medhi","09/30/2025","$2,610,307.00","Shahin Vassigh, Daniel Gann, Todd Crowl, Sitharama Iyengar","rishen@cs.fiu.edu","11200 SW 8TH ST","MIAMI","FL","331992516","3053482494","CSE","118900, 164000","1189","$0.00","Focusing on ecological research in the dynamic Everglades Ecosystem, this Instrument provides a platform to aggregate and compare many environmental data sets. It also evaluates the effect of algorithms on ecological model outputs. The instrument leverages experiences and regional access to pilot studies on the Florida Everglades. The expected outcomes are likely to constitute a significant advance in state-of-the-art, novel discoveries in the fields of computing and ecology. By developing a better understanding of spatial and temporal synergies in ecosystem dynamics, scientists will be able to predict complex ecosystem responses and guide risk assessment, planning, and solution development. The enabled research will produce evidence-based science, useful in land-management decisions. The Instrument is a pilot, expandable to other regions and to the national or global scale. All software and FIU-produced data will be open-sourced.<br/><br/>This Instrument supports research that requires the integration of large multimodal heterogeneous datasets to address the challenges with current data-driven ecosystems research and instruments, including scaling of different data; generic global models and algorithms; calibration; and exponentially-growing acquisition of remotely sensed data. Specific enabled research activities involve multi-dimensional fuzzy logic on spatiotemporal data; multimodal image analysis; and machine learning. The system comprises a data repository, a Multimodal Ecosystem Data Aggregator, and analytical and visualization tools. Existing large repositories of global datasets do not readily allow the incorporation of vast amounts of multidimensional, historical locally-acquired datasets. The platform provides comprehensive local data integration and its correlation to global data.  While the Instrument will house a well-curated collection of environmental data, its unique service will transcend mere curation, with built-in tools for data mining, fuzzy-logic reasoning, image analysis, and visualization -- developed and interfaced to empower both student and faculty researchers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018873","CC* Team: CAREERS: Cyberteam to Advance Research and Education in Eastern Regional Schools","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","04/04/2023","Andrew Sherman","CT","Yale University","Continuing Grant","Kevin Thompson","06/30/2024","$1,399,829.00","Vladyslav Kholodovych, Galen Collier, Christopher Carothers, Wayne Figurelle, Gaurav Khanna, Karlis Kaugars","andrew.sherman@yale.edu","150 MUNSON ST","NEW HAVEN","CT","065113572","2037854689","CSE","723100, 808000","","$0.00","Given the pivotal role of data and cyberinfrastructure (CI) in scientific discovery and teaching, it is essential that all small and mid-sized institutions be empowered to fully exploit them. Access to physical infrastructure is certainly required, but researchers also need access to ?Research Computing Facilitators? (RCFs) possessing the mix of technical knowledge and interpersonal skills required to help them use CI resources effectively. This poses challenges for smaller institutions, since RCFs are in short supply and are difficult to recruit and retain. Moreover, institutions can often afford only one or two, making it challenging to support diverse scientific disciplines.<br/> <br/>This project is developing a sustainable distributed approach to address these challenges in six Eastern states, facilitated by the Eastern Regional Network, a nascent, but growing collaboration among this project?s seven anchor institutions, other institutions in the Eastern US, and the area?s regional network providers. The project strategy has two principal legs: (1) expanding the RCF talent pipeline by engaging students at smaller institutions in nearly 70 project-based mentored experiential learning opportunities; and (2) developing a regional RCF pool providing CI facilitation across institutional and geographic boundaries.<br/> <br/>Success of this project directly enhances scientific research at smaller institutions. The regional RCF pool enables researchers to access appropriate expertise without the costs and delays of building an institutional RCF team. The experiential training approach exposes a relatively large, diverse group of students to the RCF profession, yielding opportunities to encourage trainees, especially in underrepresented groups, to pursue RCF careers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2334690","Elements: A Deep Neural Network-based Drone (UAS) Sensing System for 3D Crop Structure Assessment","OAC","Capacity: Cyberinfrastructure, Data Cyberinfrastructure, Software Institutes","02/01/2023","07/18/2023","Guoyu Lu","GA","University of Georgia Research Foundation Inc","Standard Grant","Varun Chandola","05/31/2025","$543,866.00","","guoyulu62@gmail.com","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","CSE","168Y00, 772600, 800400","077Z, 1165, 7923","$0.00","This project develops a 3D reconstruction sensing system that can be installed on unmanned aerial systems (UAS), to be used by agricultural researchers, growers, and service providers to assess crop growth.  Applying Artificial Intelligence (AI) technology for large scale agriculture reconstruction applications, the sensing system would be able to estimate crop structure for a large coverage area at a much lower cost than current standards that rely on light detection and ranging (LiDAR). <br/>    <br/>The project would develop and refine a deep neural network-based 3D assessment workflow, based solely on a low cost and lightweight 2D LiDAR and color camera configuration.  Researchers, growers, and service providers would be able to extract detailed crop structure and forecast yields, based on a 3D time series of crop growth.  The technology would provide a less expensive alternative to the current 3D LiDAR sensor approach, and the sensing system could also be applied to related areas such as high-throughput phenotyping and variation estimation of general terrestrial vegetation.  Outreach and extension activities are included, to deliver research outcomes to the stakeholders, including agricultural researchers, growers and service providers.   PhD students, undergraduates, and high school students will be trained through this project, including a summer activity training high school students through the Rochester Institute of Technology Imaging Science High School Summer Intern Program.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Biological Infrastructure within the NSF Biosciences Directorate, and by the Division of Information and Intelligent Systems within the NSF Computer and Information Science and Engineering Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2216030","MRI: Development of PARAGON: Control Instrument for Post NISQ Quantum Computing","CNS","Major Research Instrumentation, Information Technology Researc, Special Projects - CNS, Special Projects - CCF","10/01/2022","04/26/2023","Lin Zhong","CT","Yale University","Standard Grant","Deepankar Medhi","09/30/2025","$2,516,000.00","Robert Schoelkopf, Abhishek Bhattacharjee, Shruti Puri, Yongshan Ding","lin.zhong@yale.edu","150 MUNSON ST","NEW HAVEN","CT","065113572","2037854689","CSE","118900, 164000, 171400, 287800","1189, 9178, 9251","$0.00","This project will design and implement PARAGON, an instrument of control systems for superconducting circuit-based quantum computers. Using an ultra-low latency, scalable network of Field-Programmable Gate Array (FPGA) accelerators, PARAGON will support real-time measurement, error correction, and control of 100s of qubits. The project will also develop the necessary systems, programming and debugging support for realizing and evaluating new quantum hardware and algorithms with PARAGON.  PARAGON will substantially advance the Nation?s research capabilities in quantum computing, enabling operational tests of error-corrected algorithms and accelerating the arrival of fault-tolerant quantum computing.<br/><br/>Toward cost-effective scalability, PARAGON employs a balanced, fat tree to organize the large number of building blocks and to distribute data, clock, and time (trigger). The leaves of the tree feature Radio Frequency System-on-Chip (RFSoC) for quantum control and the internal nodes of the tree Multiprocessor System-on-Chip (MPSoC) for integration. PARAGON will empower two broad research communities that tackle quantum computing from different fronts. It will allow Physicists to investigate the theory and realization of better qubits, and experiment with sophisticated error correction and fault tolerance methods on real qubits, at a previously impossible scale. It will allow Computer Scientists to experiment with novel architectures and programming schemes for quantum control. Most importantly, it will serve as the meeting place for both communities, fostering cross-pollination and catalyzing collaboration. Through its open design and open-source software, PARAGON will empower the broad community of academic and industrial researchers in superconducting quantum computing to experiment in previously impossible ways. While PARAGON will be implemented for quantum computers based on superconducting circuits, its design can be adapted for those based on other technologies, which also face similar challenges in their control systems. It will provide critical know-how to the budding industry of quantum control systems so that the latter can further lower the cost for wider, commercial availability. The instrument will advance research agendas in multiple disciplines, creating opportunities in cross pollination between applied physics, computer science and engineering. It will create new opportunities to engage both graduate and undergraduate students, especially underrepresented minorities and women, providing unique training for multidisciplinary research. Source materials produced by the project can be found at https://github.com/yale-paragon. The repositories will be actively maintained by the project team during the award period. During the lifetime of PARAGON, the repositories will transition into community-based development and maintenance with the project team being one of the contributors. The project team will ensure the repositories are available at least five years after the lifetime of the physical testbed of PARAGON.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346707","CC* Networking Infrastructure: YinzerNet: A Multi-Site Data and AI Driven Research Network","OAC","Campus Cyberinfrastructure","04/01/2024","03/29/2024","James von Oehsen","PA","Carnegie-Mellon University","Standard Grant","Kevin Thompson","03/31/2026","$647,613.00","Mark Henderson, Stanie Waddell","barr@psc.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","CSE","808000","","$0.00","YinzerNet supports groundbreaking collaborative scientific research and education initiatives between the University of Pittsburgh, Carnegie Mellon University, and the Pittsburgh Supercomputing Center. This network infrastructure eliminates bottlenecks in data movement and sharing and research computing, enhancing the productivity of interdisciplinary collaborations. With its innovative design, YinzerNet integrates modified Data Transfer Nodes at the edge (cloudlets), equipped with Graphics Processing Units (GPUs), Data Processing Units (DPUs), and a basic OpenStack instance, facilitating the deployment of containerized workflows. The network employs Software for Open Networking in the Cloud (SONiC), an open-source network OS overlay, to connect distributed sites, fostering experimental and enterprise network solutions, and adds the ability to expand and connect with other academic institutions in the future.<br/><br/>By advancing network capabilities and supporting cloudlet-based distributed research platforms, YinzerNet enables the exploration of new federated Artificial Intelligence (AI) models and network monitoring tools, crucial as federated services emerge as a standard. It serves a wide range of research domains, including satellite communications, security, life sciences, robotics, high-energy physics, astronomy, and neuroscience, demonstrating immediate impacts on these fields.<br/><br/>YinzerNet not only represents a significant step forward in networking infrastructure but also acts as a catalyst for future technological advancements and connections to the national cyberinfrastructure ecosystem. Its design and implementation underscore the importance of collaborative platforms in accelerating scientific discovery and innovation, showcasing the potential for significant societal benefits through enhanced research capabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346726","CC* Integration-Small: Enhancing Data Transfers by Enabling Programmability and Closed-loop Control in a Non-programmable Science DMZ","OAC","CISE Research Resources, Campus Cyberinfrastructure, EPSCoR Co-Funding","07/01/2024","04/02/2024","Jorge Crichigno","SC","University of South Carolina at Columbia","Standard Grant","Nicholas Goldsmith","06/30/2026","$500,000.00","Andreas Heyden, Paul Sagona, Elie Kfoury","jcrichigno@cec.sc.edu","1600 HAMPTON ST","COLUMBIA","SC","292083403","8037777093","CSE","289000, 808000, 915000","9150","$0.00","Programmable data plane (PDP) switches have recently attracted significant attention from the communications industry. They are network devices that provide unprecedented visibility of events occurring in data networks and enable engineers to write software applications using the P4 programming language. The applications leverage the visibility and performance capacity of PDP switches, where applications can run orders of magnitude faster than those running on general-purpose computers. This project will deploy a ""self-driving"" network ? referred to as Science DMZ ? at the University of South Carolina (USC), using P4 applications running on PDP switches. The enhanced network will support science and engineering projects that foster the progress of science. Specifically, it will enable faculty members, researchers, and students to transfer big science data more efficiently within USC's campus and with external collaborators such as U.S. Department of Energy's laboratories (e.g., Savannah River, Argonne), national computing centers (e.g., San Diego Supercomputer Center, National Energy Research Scientific Computing Center), and international organizations (e.g., European Organization for Nuclear Research ? CERN). <br/><br/>The project has two objectives: 1) Develop a self-driving Science DMZ, capable of enhancing performance and fairness. Traffic will be monitored by PDP switches operating at terabits per second (Tbps) rate, and functions commonly executed on general-purpose CPUs will be offloaded to the PDP switches. The scheme will leverage the granular telemetry and the processing speed of the switches to automate the configuration of the Science DMZ and address challenges observed by USC and the community, such as traffic policing and optimal buffer sizing. Additionally, the project will extend perfSONAR to passively incorporate the telemetry generated by PDP switches, introduce real-time reporting on a per-flow basis, and add new metrics such as queue occupancy and packet interarrival time (i.e., per-packet visibility). 2) Disseminate the P4 technology by developing open-source P4 virtual lab libraries. The libraries will run on USC's cloud platform, referred to as the Academic Cloud, and on FABRIC. The Academic Cloud is used by several colleges and universities, researchers, and the cyberinfrastructure (CI) community for academic courses, workshops, and self-paced training. The libraries on FABRIC will use JupyterHub notebooks, incorporate the learner's customized topology, and embed P4 code alongside explanatory documentation, videos, and other visual aids. Finally, the project will support four PhD students and ten undergraduate students who will deploy the enhanced network under the guidance of the Principal Investigators and CI engineers.<br/><br/>This project is jointly funded by the Office of Advanced Cyberinfrastructure, the Established Program to Stimulate Competitive Research (EPSCoR), and the Division of Computer and Network Systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232810","CC* Data Storage: Flexible Affordable Scalable Technology for Research Storage (FAST-Research Storage)","OAC","Campus Cyberinfrastructure","09/01/2022","08/26/2022","Charles Kneifel","NC","Duke University","Standard Grant","Kevin Thompson","08/31/2024","$500,000.00","Tracy Futhey, John Board, Douglas Boyer, Alberto Bartesaghi","charley.kneifel@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","CSE","808000","9102","$0.00","The Flexible Affordable Scalable Technology for Research Storage (FAST) is a shared research storage system based on commodity data center hardware and open source CEPH software that provides a flexible, high-performance, cost-effective, tiered data storage system in support of research data storage requirements. The FAST system provides both Network Attached Storage (NAS) and Object Store storage models, along with guidance for choosing wisely between them.  Object storage services support efficient packaging of data sets, including the complete set of metadata, for both operational use and long-term data storage.  FAST also includes scripts to package, migrate data between storage tiers, and facilitate the use of appropriate storage to meet researcher?s requirements.  Cloud-based long-term cold storage environments are part of the data lifecycle to support data retention requirements cost-effectively.  By using commodity hardware FAST is easily expanded and capable of incorporating researcher purchased hardware into the environment. <br/><br/>The system (20%) will be shared with the community via the Open Science Data Federation (OSDF) as well as with minority service institutions in North Carolina through a community storage grant program.  Both sharing models allow researchers to publish data for public consumption, stage data to facilitate local analysis and provide robust storage service to schools which may not otherwise have access to these services. The expected impact of FAST is to reduce the amount of time, effort, and money that researchers spend managing their data, ensuring long-term access to data, with the data stored in the right place at the right time and cost.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201106","CC* Compute: COllaborative Next-generation Technology In the Northeast: the UMassUnity Machine (CONTINUUM)","OAC","Campus Cyberinfrastructure","04/15/2022","04/05/2022","Michael Zink","MA","University of Massachusetts Amherst","Standard Grant","Amy Apon","12/31/2024","$399,676.00","","zink@ecs.umass.edu","101 COMMONWEALTH AVE","AMHERST","MA","010039252","4135450698","CSE","808000","","$0.00","COllaborative Next-generation Technology In the Northeast: the UMassUnity Machine (CONTINUUM) extends an existing computational resource, UMass Amherst?s UNITY cluster, with eight novel servers that support cutting-edge IBM POWER, GPGPU and FPGA architectures. This extension enables key computational efforts at UMass Amherst and Dartmouth campuses, and the University of Rhode Island in numerous important science areas, including gravitational wave science, metagenomics, earthquake detection, modeling cancer and others. Based on the science requirements, the existing Unity cluster is extended with eight Power9 nodes, each with dual 2.7 GHz 16-core processors, 256 GB memory, accelerated by dual Nvidia Tesla V100 GPGPUs and a Xilinx Alveo U50 FPGA. Beyond the initial science drivers proposed, the new servers support many more collaborative research teams on the three campuses. Furthermore, CONTINUUM enables scientific development for the entire Open Science Grid (OSG) community, with access to 20% of the compute time.<br/><br/>The new servers also serve as an important tool for training undergraduate and graduate students in the efficient use of cutting-edge HPC systems. In addition, CONTINUUM supports research and education in an EPSCoR state (RI) and a university campus in Massachusetts with 38% students of color. To promote collaborations with researchers in the wider region, CONTINUUM personnel participate in the regional HPC Day conference and collaborate with the Northeast and CAREERS Cyberteams communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2325668","Conference: Inaugural Tribal College and University CI Community of Practice Workshop","OAC","Campus Cyberinfrastructure","07/15/2023","06/26/2023","Carrie Billy","VA","American Indian Higher Education Consortium","Standard Grant","Kevin Thompson","06/30/2024","$99,935.00","Alfred Anderson, Dennis Dye","cbilly@aihec.org","121 ORONOCO ST","ALEXANDRIA","VA","223142015","7038380400","CSE","808000","7556","$0.00","The Tribal College and University Cyberinfrastructure Community of Practice workshop provides an opportunity for the IT Directors, CIOs, and other administrators, IT staff or faculty at TCUs, who are geographically isolated across the United States, to meet in person, exchange best practices, interact with invited national cyberinfrastructure (CI) experts, and collaborate across TCU CI initiatives, academic areas, and/or geographical regions.  The workshop is organized by the American Indian Higher Education Consortium (AIHEC) and led by a program committee consisting of AIHEC, TCU and external experts. Cyberinfrastructure topic areas expected to be addressed through both presentations and in-depth panel discussions include CI Strategic Planning and Academic Engagement, Cybersecurity and Technology, TCU Future Directions and CI Funding Strategies. Workshop panelist and presenters come from the TCUs? IT staff and faculty but also include external subject matter expects and research and education community members. <br/><br/>The intellectual merit of the workshop includes the opportunity for TCU staff knowledge and skill development focused on new and existing cyberinfrastructure technologies in support of science and engineering applications relevant to the tribal community.  Given the important role the TCUs play in their communities, implementation of capacity-building relationships with TCUs will enable them to serve a similar function to their area K-12 schools and Tribal agencies, advancing them through the adoption of the shared CI tools and resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201105","CC* Compute: NCShare Compute as a Service","OAC","Campus Cyberinfrastructure","05/01/2022","11/01/2023","Charles Kneifel","NC","Duke University","Standard Grant","Amy Apon","04/30/2025","$397,557.00","Mohammad Ahmed, Kevin Davis, Tracy Futhey, Joel Faison, Eva Kraus","charley.kneifel@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","CSE","808000","","$0.00","This collaboration among Duke University, North Carolina Central University (NCCU), and Davidson College builds a shared computing environment housed at North Carolina's research and education network provider called MCNC. The shared computing environment supports a common set of software and science drivers, with priority use by Davidson, NCCU, and other North Carolina minority-serving and smaller institutions; OSG and Duke would have lower-priority use, if excess-capacity exists. As a result, faculty at participating institutions, who often juggle high course loads, have access to tools and services that ease the delivery of customized computing systems to meet their research needs and enable easy access to powerful tools for students. <br/><br/>The project's shared computing environment comprises 768 CPUs, 6144GB of RAM, and 250TB of storage and uses federated logins (Shibboleth-enabled) to support a large and diverse set of users at NCCU and Davidson. The environment is extended in a cloud model to other minority-serving and smaller schools in NC and encompasses other options for logins. The project leverages Duke's already mature capability in automated provisioning, software containerization, and advanced networking, which accelerates implementation and makes computation and powerful software environments quickly available to researchers and educators in the region. <br/><br/>By starting from a common base environment and then supporting customizations that can meet the specific research and education demands of participating institutions, this project provides hands-on opportunities for students to use advanced science capabilities and software environments. The result enhances STEM education at participating campuses by enabling  advanced access to science capacity on those campuses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2025676","2020 NSF CISE-SBE Workshop on Harnessing the Computational and Social Sciences to Solve Critical Societal Problems","CNS","Cross-Directorate  Activities, Information Technology Researc","04/01/2020","02/08/2024","Frankie King","TN","Vanderbilt University","Standard Grant","Darleen Fisher","03/31/2025","$99,998.00","","frankie.king@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","CSE","139700, 164000","7556","$0.00","This award funds the 2020 NSF CISE-SBE Workshop on Harnessing the Computational and Social Sciences to Solve Critical Societal Problems. This invitation-only three-day meeting will be to be jointly convened by the National Science Foundation Directorates for Computer and Information Science and Engineering (CISE) and Social, Behavioral, and Economic Sciences (SBE).  It is increasingly apparent that many of the systems on which our society depends for its health, prosperity, and security are neither purely social nor purely technical ones. Rather, they are socio-technical systems. For example workplace relationships, media markets, health delivery systems, and criminal justice organizations are all increasingly characterized by a complex mixture of human actors and institutions on the one hand, and digital platforms and algorithms on the other hand. Efforts to design, manage, audit, and ultimately improve these systems to the benefit of society therefore lie at the intersection of the computational sciences and the social-behavioral sciences. <br/><br/>This workshop will bring together experts from nationally recognized institutions comprising a diverse range of stakeholders and perspectives drawn from academia, industry, government, and philanthropy to discuss three core themes at the intersection of the computational and social-behavioral sciences. This intersection has been an area of increasing research interest and innovation in recent years, generating new conferences, funding opportunities, and jobs, both in academia and industry. In order to generate practical solutions to real-world socio-technical problems, however, a number of challenges must be overcome that will require new streams of funding and also new models of collaboration, between the computational and social-behavioral sciences, and also among academia, government and industry. The workshop is organized around three core substantive themes, each of which corresponds to a contemporary societal concern of widespread interest: increasing opportunity for all Americans, while reducing harmful disparities; improving the trustworthiness of the information ecosystem; and empowering the skilled technical workforce<br/><br/>In addition to sketching out a research agenda for each of these substantive areas, the workshop would also focus attention on four cross-cutting challenges: building large-scale, shared data infrastructures for research purposes; partnering with industry around problems of shared interest; fostering collaborations between CISE and SBE researcher; and developing training and educational programs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2030859","Computing Innovation Fellows 2020 Project","CCF","CISE Education and Workforce, Information Technology Researc, Special Projects - CNS, Special Projects - CCF, IIS Special Projects","05/15/2020","03/25/2024","Ellen Zegura","DC","Computing Research Association","Continuing Grant","Mitra Basu","04/30/2025","$16,737,291.00","Elizabeth Bradley, Mary Lou Maher, Tracy Camp, Ann Schwartz, Andrew Bernat, Ellen Zegura, Mark Hill","ewz@cc.gatech.edu","1828 L ST NW","WASHINGTON","DC","200365104","2022662949","CSE","055Y00, 164000, 171400, 287800, 748400","","$0.00","The current coronavirus (COVID-19) pandemic is disrupting aspects of daily life and work, including having a serious impact on the current faculty-recruiting season in the computing-research community. A Computing Research Association (CRA) survey completed on April 1st 2020 counted about 100 academic-computing job positions being pulled from the market due to hiring freezes, and universities continue to forecast major financial losses for the upcoming academic year, with likely negative impacts on the academic-computing job market for the next year as well. Thus what is needed is a bridge that keeps highly trained researchers in the academic pipeline to preserve future computing innovation and to meet the training needs of future computing professionals, as these will be the backbone of the future economy. CRA and its Computing Community Consortium (CCC) provided such a bridge for the severe economic downturn a decade ago, using NSF funding to administer three cohorts of Computing Innovation Fellows (CIFellows). That postdoctoral project kept 127 young scholars in research with career-enhancing programs.  The current project, CIFellows 2020, is intended to provide similar support for the academic-computing pipeline in light of the damage it is sustaining in the wake of the current pandemic. <br/><br/>The CIFellows 2020 project takes inspiration from the original CIFellows project but adapts it to the current uncertain situation, by incorporating more flexibility, allowing the option of doing a postdoc at the applicant?s current institution, and providing a significant mentoring/cohort-building component that is based on best practices that emerged from the original effort. Fellows may come from any research area under the umbrella of NSF Computing and Information Science and Engineering (CISE). Fellows will engage in a 1-2 year postdoctoral experience that furthers their career development in new ways. An application process will be implemented, and selection of successful applicants will be made using a technical program-committee style with strict adherence to conflicts of interest and based on a holistic evaluation of merit and diversity along many dimensions, with major emphasis on intellectual merit and broader impacts in applicant materials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018851","CC* Compute: High-Memory Compute Resources for Maine","OAC","Campus Cyberinfrastructure","07/01/2020","04/03/2024","Bruce Segee","ME","University of Maine","Standard Grant","Amy Apon","06/30/2024","$399,813.00","Jayendran Rasaiah, Stephen Cousins, Damian Brady, Samuel Roy","segee@maine.edu","5717 CORBETT HALL","ORONO","ME","044695717","2075811484","CSE","808000","9150","$0.00","Maine researchers are advancing the state of the art in areas including landslide prediction, hydrodynamic modelling, fluid-structure interaction and modelling the electro-chemical properties of organic molecules. Strong, scientifically compelling investigations have previously been hampered or stalled by the lack of adequate computational resources.<br/> <br/>This project advances research at the University of Maine in two ways through the addition of approximately 1000 processing cores in high RAM nodes along with a growth in CEPH disk storage. It enables research to move forward in areas such as landslide prediction, coastal modelling, DNA sequencing from single strands of DNA, and high resolution modeling of the cardiovascular system. It facilitates an increase in collaboration with the Eastern Regional Network, the Open Science Grid, the Open Storage Network, and with other institutions, particularly other EPSCoR sites in the Northeast. Data and code from this grant is disseminated to the public through tools such as github and EarthCube. The increase in computational resources as a result of this project allows opportunities for undergraduate and graduate students to engage in state-of-the-art numerical modeling. By having these new resources to meet the needs of researchers, previously existing resources are utilized to offer courses for which there was not previously the capacity. Thus the instrumentation advances research and also enables the project team to better train the next generation of scientists and engineers. The research projects facilitated by this cluster all include plans to distribute, and visualize model output for relevant stakeholders.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112471","AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE)","CNS","GVF - Global Venture Fund, AI Research Institutes, Information Technology Researc, Special Projects - CNS, CISE Research Resources, ","10/01/2021","09/15/2023","Ness Shroff","OH","Ohio State University","Cooperative Agreement","Alhussein Abouzeid","09/30/2026","$13,487,334.00","James Kurose, Elisa Bertino, Robert Nowak, Gauri Joshi","shroff@ece.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","054y00, 132Y00, 164000, 171400, 289000, U31000","044Z, 075Z, 5942, 5976, 5978, 6124","$0.00","Networking and AI are two of the most transformative IT technologies --- helping to better people?s lives, contributing to national economic competitiveness, national security, and national defense. The Institute will exploit the synergies between networking and AI to design the next generation of edge networks (6G and beyond) that are highly efficient, reliable, robust, and secure. A new distributed intelligence plane will be developed to ensure that these networks are self-healing, adaptive, and self-optimized. The future of AI is distributed because AI will increasingly be implemented across a diverse set of edge devices. These intelligent and adaptive networks will in turn unleash the power of collaboration to solve long-standing distributed AI challenges, making AI more efficient, interactive, and privacy-preserving. The Institute will develop the key underlying technologies for distributed and networked intelligence to enable a host of future transformative applications such as intelligent transportation, remote healthcare, distributed robotics, and smart aerospace. It is a national priority to educate students, professionals, and practitioners in AI and networks, and substantially grow and diversify the workforce. The Institute will develop novel, efficient, and modular ways of creating and delivering educational content and curricula at scale, and to spearhead a program that helps build a large diverse workforce in AI and networks spanning K-12 to university students and faculty.<br/><br/><br/>The focus of the AI Institute will be on edge networks, which will constitute the majority of the growth of future networks. This edge includes all devices connected through the radio as well as data centers and cloud computing systems that are not at the core of the Internet. A critical component of the Institute is to shorten the time-scale of interactions between Foundations and use case research across multiple disciplines. This will result in a virtuous cycle that will have a cascading impact dramatically accelerating the time it takes from research to implementation and technology transfer. The research tasks will be enhanced and fleshed out by exploring three wireless edge use cases in depth: (1) Ubiquitous Sensing/Networking; (ii) Human-Machine Mobility and (iii) Programmable/virtualized 6G networks. These use cases are important in their own right and connect the key research thrusts and their validation to specific experimental platforms. The Institute will work with its industry and DoD partners to facilitate translation and adoption.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2030165","Collaborative Research: SWIFT: LARGE: Spectrum Sharing Via Interference-resilient Passive Receivers and Passive-aware Active Services","ECCS","SII-Spectrum Innovation Initia, Information Technology Researc","09/01/2020","08/12/2022","Karl Warnick","UT","Brigham Young University","Standard Grant","Jenshan Lin","08/31/2025","$306,935.00","Philip Lundrigan","warnick@ee.byu.edu","A-153 ASB","PROVO","UT","846021128","8014223360","ENG","151Y00, 164000","044Z, 105E, 153E, 4444","$0.00","Scientists use radio observations of the sky to understand the evolution of galaxies and cosmic structures and answer questions about the origins of the universe. Cellular devices and other wireless technologies are vital to a modern economy but interfere with sensitive astronomical instruments. This project will develop technologies that will allow wireless devices and networks to coexist with scientific uses of the radio spectrum. These technologies include innovations in electronic systems that will make astronomical receivers more immune to interference from wireless devices; and advanced networking methods that allow wireless devices to intelligently cooperate with scientific observatories. <br/>This project includes both fundamental technical innovations and integrative system-level validation. It combines technical innovation at the passive receiver with new ideas on active transmitter management and looks at the problem in an integrative way. Technical innovations include novel hardware architecture and custom analog circuits, machine-learning-inspired design framework, cooperative wireless network protocol design, and testbed verification and evaluation. Breakthroughs in wideband beamforming, beam nulling, and interference cancellation through wideband true-time-delay arrays are proposed, which have the potential to enhance US competitiveness in various markets. A software-defined physical layer protocol brings a new approach to communication between active and passive devices, allowing for coordination between the device types. Industry interaction will enable the discussion of the practicality of ideas, placement of students in internships and permanent positions, and technology transfer. The astronomy application is a significant draw for women and minority engineering students and will allow for a significant increase in the diversity of the students involved in the project relative to historical norms in the engineering discipline. The proposed work will potentially impact the design and conception of future astronomy systems as wideband beamforming and cooperative network protocols will be available to technical standardization committees to enable practical coexistent deployment of active and passive services. The PIs' collaborations with astronomical observatories will allow for knowledge transfer and possible technology adoption in current and future instrument projects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346717","CC* Campus Compute: UTEP Cyberinfrastructure for Scientific and Machine Learning Applications","OAC","Campus Cyberinfrastructure","04/01/2024","03/29/2024","Shirley Moore","TX","University of Texas at El Paso","Standard Grant","Amy Apon","03/31/2026","$498,431.00","Paras Mandal, Deepak Tosh","svmoore@utep.edu","500 W UNIVERSITY AVE","EL PASO","TX","799688900","9157475680","CSE","808000","","$0.00","A major revolution taking place in High Performance Computing (HPC) today is the increasing use of machine learning to complement simulation. This project at University of Texas at El Paso is providing researchers in computational science and engineering fields with access to state-of-the-art GPU-accelerated hardware and software resources. The choice of resources was motivated by the requirements of science drivers that address important societal and national problems in the areas of renewable energy, advanced manufacturing, advanced materials, electric power systems, cybersecurity, and quantum computing using simulation and/or machine learning. <br/><br/>The new hardware is integrated with the existing campus HPC cluster. The hardware is shared with the broader academic research community through participation in the Open Science Grid. In addition to providing a valuable resource for faculty, the augmented HPC cluster is available for use by students for projects related to their thesis and dissertation work as well as for class projects. The software stack on the campus HPC cluster reflects that available on NSF supercomputer systems with similar architectures. Student researchers provide assistance with operation and maintenance of the cluster, thus equipping them with skills needed for future careers in HPC and large-scale machine learning fields.  Expected outcomes of the project include: 1) increased use of HPC across disciplines, 2) improvement in faculty and student skills in using state-of-the art HPC technologies, and 3) increased rate of producing research results and publications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2212427","Collaborative Research: SHF: Medium: Approximate Computing for Machine Learning Security: Foundations and Accelerator Design","CCF","Information Technology Researc","08/01/2022","07/21/2022","Khaled Khasawneh","VA","George Mason University","Continuing Grant","Almadena Chtchelkanova","07/31/2026","$195,362.00","","kkhasawn@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","164000","1640, 7924, 7941","$0.00","Deep Neural Networks (DNNs) are achieving state-of-the-art performance on a large and expanding number of application domains.  However, one of the threats to their wide-scale deployment is vulnerability to adversarial machine learning attacks, where an adversary injects small perturbations to the input data that cause the DNN to misclassify, with potentially dangerous outcomes (for example, mistaking a stop sign for a speed limit sign).  In this project, the researchers will explore how building DNNs with approximate computing elements improves their robustness to these adversarial attacks.  Approximate computing is a technique to build computing elements that are simpler (and therefore higher performing and more sustainable) but do not compute the exact result of an operation.  The investigators will explore how to select approximate computing elements and use them in building sustainable DNN accelerators that balance performance, accuracy, and security.<br/><br/>The proposal's expected contributions include developing new insights into the relationship between approximation and robustness of DNNs.  The project will explore what types of approximation techniques result in effective DNNs that balance accuracy, performance, sustainability, and protection against adversarial attacks and develop optimization frameworks that can find optimal operating points along these dimensions.  It will also explore how to build new approximate computing elements specifically targeted toward this application.  The project will use these findings to build sustainable, performant, and accurate DNN accelerators.  The project will also explore other approximate computing-based techniques to protect against other types of attacks threatening the security and privacy of DNNs, as well as for different deep neural network learning structures.  The project is expected to have significant impacts on security, sustainability, and accuracy of machine learning models.  The research team will share all of the byproducts of the research with the research community.  The project will train graduate and undergraduate students.  The investigators will develop new educational material for use in machine learning, computer architecture, and computer security classes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2144956","CAREER: Cameras and Algorithms that turn Rays Efficiently into Everyday Reconstructions","IIS","Information Technology Researc, Robust Intelligence","06/01/2022","08/24/2023","James Tompkin","RI","Brown University","Continuing Grant","Jie Yang","05/31/2027","$290,765.00","","james_tompkin@brown.edu","1 PROSPECT ST","PROVIDENCE","RI","029129100","4018632777","CSE","164000, 749500","102Z, 1045, 7495, 9150","$0.00","This award is funded in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>This project aims to enable people to capture digital versions of real-world scenes more accurately, efficiently, and flexibly than is currently possible such that it can be a simple everyday task. In computer vision, capture of the real world is called reconstruction, and it is a core challenge that requires estimating the 3D shape, motion, object materials, and lighting within a scene. Successful reconstruction can provide spatial and geometric information of objects independent from lighting conditions to intelligent systems so that they can use the information for reasoning or creating applications of these objects from different viewing angles, e.g., in virtual and augmented reality. This project will scientifically investigate how to overcome the challenges of reconstruction by combining signals from different types of cameras in a way that is consistent with the physics of image formation. The project will integrate research and education by creating new interdisciplinary courses and promoting diversity from different outreach activities, e.g., supporting our K-12 AI4ALL local diversity effort, and attending inclusive teaching workshops at Brown?s Sheridan Center for Teaching and Learning.  <br/><br/>To help overcome the ill-posed problem of scene reconstruction from passive RGB cameras, this project has three areas of focus: 1) Investigate new camera systems that integrate multiple kinds of signals via physically based image formation models. Existing platforms handle typically one modality and frequency (visible light), but the project aims to combine visible light, time of flight, and event cameras to balance the negative effects of each camera and produce a signal of a quality that no individual camera could produce: high spatio-temporal resolution 3D video. 2) Investigate lighting and material decomposition via better capture, sampling, and reconstruction from heterogeneous omnidirectional cameras via new fast view synthesis methods adapted to represent incident illumination. This will use learned material priors from factorizations of physically based reflectance models that can exploit captured full and partial omnidirectional samples. 3) Investigate hybrid representations, optimization, and machine learning methods, including initialization based on reliable sparse sampling from depth sensors, via physically based self-supervised transforms to constrain optimization, and via residual error channels to allow the model to explain all that it can in a physically meaningful way and still train on real-world data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346673","CC* Campus Compute: Building a Computational Cluster for Scientific Discovery","OAC","Campus Cyberinfrastructure","07/01/2024","03/29/2024","Greg Williams","CO","University of Colorado at Colorado Springs","Standard Grant","Amy Apon","06/30/2026","$294,361.00","Yanyan Zhuang, Oluwatosin Oluwadare","gwillia5@uccs.edu","1420 AUSTIN BLUFFS PKWY","COLORADO SPRINGS","CO","809183733","7192553153","CSE","808000","","$0.00","The University of Colorado Colorado Springs (UCCS) aims to strengthen its cyberinfrastructure to meet the evolving needs of research-oriented faculty and to facilitate the increasing demands of externally funded projects. This project aims to address the growing research mission that accompanies the increase in research activities.  The project enables transformative research and supports a multitude of projects in areas such as magnetic dynamics, animal biomechanics, quantum mechanics, super-resolution bio-imaging, mobile hardware architecture exploration, genomics, bioinformatics, and computational fluid dynamics. <br/><br/>The computing hardware consists of a heterogeneous mix of compute nodes to cater to various machine learning (ML) workloads. Components include GPU-accelerated nodes, multi-core CPUs, and specialized hardware that is developed for accelerating ML and AI workloads. A high speed fabric interconnects hardware and software. UCCS shares these computing resources with groups such as Open Science Grid (OSG).  By contributing to the global pool of OSG resources, the system enables advanced research across the nation. The project supports student training in IT and cyberinfrastructure, fostering diversity in the project's personnel and impact. The technical insights gained will be shared extensively within research computing and scientific communities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2125646","CC* Compute: A campus-wide computing resource for research and teaching at the University of Washington Bothell","OAC","Campus Cyberinfrastructure","09/01/2021","08/31/2021","Eric Salathe","WA","University of Washington","Standard Grant","Kevin Thompson","08/31/2024","$394,473.00","Kristina Hillesland, Joey Key","salathe@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","CSE","808000","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>UW Bothell has no general access to research computing resources - this project transforms the capacity for computational science research and education on campus. The University of Washington maintains an integrated, scalable, super-computing infrastructure called Hyak to support research computing across all units and campuses. Academic and research units at the university purchase nodes on Hyak as an alternative to deploying and operating their own high-performance systems. This award allows the small University of Washington Bothell campus to become a participant in the Hyak system creating a dedicated research computing capacity for our faculty and students. The team of researchers participating in this project form the core users of the new system and span a wide range of computational research fields including astrophysics, climate modeling, biochemistry, genomics, machine learning, operations research, and mathematics. The new capacity allows the development of a larger cross-campus initiative for computational science education supporting our undergraduate teaching mission.<br/><br/>This project supports the acquisition and operation of compute nodes and associated hardware in the Hyak system. UW Bothell is a sponsor of the Hyak facility and participates in its governance and decision-making processes. HPC nodes procured include 60 Intel Xeon Scalable Gold 6230 20-core 2.1GHz CPUs for 1,200 total cores and 8 NVIDIA Quadro RTX6000 24GB GPUs. Each node includes 192 GB RAM and a 100 Gbps cluster interface. Data storage is provided by 33 TB high-speed scratch storage and 33 TB of archival tape storage.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104398","Collaborative Research: NGSDI: Foundations of Clean and Balanced Datacenters: Treehouse","CNS","Information Technology Researc, Special Projects - CNS","05/01/2021","06/13/2023","Adam Belay","MA","Massachusetts Institute of Technology","Continuing Grant","Daniel Andresen","04/30/2025","$376,544.00","","abelay@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","CSE","164000, 171400","","$0.00","Computing requires energy. Most computing is done in centralized hubs called datacenters.  Datacenters consume an estimated 1-2% of worldwide electricity production. Datacenter computing, and its energy use, is projected to continue to grow rapidly, perhaps as fast as doubling every few years. This is simply not sustainable. The Treehouse project aims to improve the energy efficiency of datacenter computing by making datacenter computing energy use accountable to users at a fine-grained level and by reducing unnecessary waste in the most frequently used parts of datacenter computation.<br/> <br/>Treehouse improves datacenter energy efficiency in several ways. Treehouse introduces a new computational abstraction that allows new energy optimizations by both application developers (by making application energy use visible at a fine-grained level) and systems designers (by identifying when energy-efficient optimizations can be safely performed without compromising user goals for application performance and reliability). Additional strategies include reducing unnecessary software bloat, reducing resource stranding, and new algorithms to exploit the opportunity posed by new types of hardware with complex tradeoffs between performance and energy use.<br/> <br/>Beyond better energy and resource management, Treehouse provides end users the tools to understand and reduce their individual carbon use from cloud services. This can fundamentally change the way the cloud computing industry thinks about datacenter energy use. Datacenter operators can provide new energy efficient computing models at lower cost. Treehouse software systems and protocols will be open source. Through outreach and new educational materials, Treehouse will pioneer the training of a new type of energy-aware engineer to meet societal needs for an energy-efficient computing infrastructure.<br/> <br/>Treehouse will produce software artifacts, hardware designs, and the results of running those programs and artifacts. These materials will be available for public use under a permissive open source license, archived in multiple locations, and available at the project website treehouse.cs.washington.edu for at least five years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1816379","CSR: Small: System Support to Build Context Aware Applications at the Edge","CNS","GVF - Global Venture Fund, Information Technology Researc, CSR-Computer Systems Research","10/01/2018","03/27/2024","Shivakant Mishra","CO","University of Colorado at Boulder","Standard Grant","Marilyn McClure","09/30/2025","$600,000.00","","mishras@cs.colorado.edu","3100 MARINE ST","Boulder","CO","803090001","3034926221","CSE","054Y00, 164000, 735400","120Z, 7923","$0.00","Smart cities and Internet of Things (IoT) represent a domain of exponential growth, profitability, and immense opportunities in the coming decades, and are expected to revolutionize life. A key roadblock in realizing the immense potential of this disruptive technology is that at present there isn't any integrated system level support to build sophisticated smart city applications or services. Current efforts have mostly been in silos and building applications using the IoT or smart city infrastructure remains an arduous task. Edge computing technology, which allows data produced by IoT devices to be processed closer to where it is created is considered crucial for the success of the smart city and IoT visions, but there are some significant challenges that this project seeks to address.<br/><br/>The project will identify a set of core system level services needed to build sophisticated context-aware applications at the edge, investigate systems and algorithmic challenges in the design and implementation of these services, and build a holistic middleware framework that exposes these services to the edge application developers in an accessible, well integrated manner. The utility of this framework will be demonstrated by developing three novel smart city and IoT applications that are representatives of the type of applications that will be commonplace in the future. Overall, this project will expedite the construction of sophisticated context-aware edge applications and contribute to the growing body of work on smart city and IoT.<br/><br/>The project will integrate research ideas into the educational mission that involves students at high school, undergraduate and graduate levels. The framework will be made available to researchers in academia and industry and contributed to the open source infrastructure for edge computing research and education. As edge computing holds great promise for building transformative context-aware applications for everyday life, the proposed research will contribute to better quality of life and the nation?s economy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2216311","MRI: Acquisition of a Heterogeneous High-Performance Computing Cluster Driven by Computational and Data-Intensive Multidisciplinary Research","OAC","Major Research Instrumentation, Information Technology Researc","10/01/2022","11/24/2023","Kevin Brandt","SD","South Dakota State University","Standard Grant","Alejandro Suarez","09/30/2025","$978,708.00","Joy Scaria, Anne Fennell, Stephen Gent, Larry Leigh, Timothy Hansen","Kevin.Brandt@sdstate.edu","940 ADMINISTRATION LN","BROOKINGS","SD","570070001","6056886696","CSE","118900, 164000","9102, 9150","$0.00","South Dakota State University (SDSU) will enable regional research and educational programs with high computational and data-intensive needs through the acquisition and deployment of an advanced computing instrument. The goals of this initiative will accelerate scientific discovery, promote active engagement, advance collaboration within the research community, and provide a resource within the region to advance and elevate multi-disciplinary STEM education and research. A breadth of science and engineering fields will benefit from this acquisition, including climatology, satellite image processing, national food safety and security, genomics of bacterial infections, antibiotic resistance, and power grid security. This project will also be a focal point for supporting the educational and research needs of underserved communities throughout the region.<br/> <br/>The team within SDSU Research Cyberinfrastructure (RCi) will acquire and implement a robust advanced computing instrument that accomplishes the following key project objectives: (1) Widening and boosting computing core capabilities within research pipelines that harness the power of vector and parallel computing; (2) Advancing the usability of CPU and Graphics Processing Unit (GPU) resources using Open OnDemand and Science Gateways; and (3) Amplifying the accessibility of highly cost-effective computational tools to a larger user base. The system design uses a next-generation computer architecture with hardware resources specifically designed for boosting computing capacity. The instrument will offer the flexibility of scaling research applications, allowing the user to self-provision resources and facilitating ease of access for new researchers. This will be achieved using off-the-shelf open-source software resources and pre-built scientific workflows. This instrument will serve as a computational core for RCi at SDSU, serving researchers, collaborators, staff, and students engaged in research and education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835725","Frameworks: Software NSCI-Open OnDemand 2.0: Advancing Accessibility and Scalability for Computational Science through Leveraged Software Cyberinfrastructure","OAC","Data Cyberinfrastructure","11/01/2018","07/06/2023","David Hudak","OH","Ohio State University","Standard Grant","Alejandro Suarez","09/30/2024","$3,345,802.00","Thomas Furlani, Brad Chalker, Robert Settlage","dhudak@osc.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","CSE","772600","026Z, 062Z, 077Z, 7925","$0.00","Reducing barriers that limit the adoption of high performance computing (HPC) addresses an important problem that broadly affects the science, engineering, and humanities communities.  This effort builds on existing capabilities with large and varied user communities, and on national scale cyberinfrastructure and high-performance computing resources.  The approach has several benefits:<br/> - It increases the use of HPC resources among communities that are not well represented on HPC yet, but have growing needs for HPC.   <br/> - It is also beneficial to HPC providers, by supporting advanced features for monitoring and visualization of the states of systems.<br/> - The resulting framework will be used for training and workforce development, expanding the future ability to use advanced cyberinfrastructure for science.<br/>This project builds on the strengths of existing efforts, and has the potential to benefit a broad user community. <br/><br/>The project develops Open OnDemand 2.0, an open-source software that enables access to high-performance computing, cloud, and remote computing resources via the web, and lower the barriers to access HPC systems. The project combines two widely used HPC resources: <br/> - Open OnDemand 1.0  - an existing open-source, web-based project for accessing HPC services; and <br/> - Open XDMoD - an open-source tool that facilitates the management of high performance computing resources.<br/>Project activities include enhancing an existing web portal-to-HPC system (OnDemand), integrating XDMoD, extending the portal to provide other methods of access for other science domains, and improving the scaling of the system.  The software employs a unique per-user web server architecture. This gives a user full system-level access to an HPC cluster through a web browser.  Job performance visibility is provided by XDMoD, which enables users to make more efficient usage of HPC resources. Innovation and discovery will be integrated through a study which investigates ways to leverage the system-level access provided by Open OnDemand with science gateways.  The integrated platform will enhance resource utilization visibility, extend to more resource types and institutions, and support a smooth and easy utilization of HPC resources with intuitive web interfaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2041952","Virtual Organization for Computing Research in Pandemic Preparedness and Resilience","CNS","Information Technology Researc, Special Projects - CNS, Secure &Trustworthy Cyberspace","10/01/2020","05/25/2023","Madhav Marathe","VA","University of Virginia Main Campus","Standard Grant","Anna Squicciarini","09/30/2024","$1,515,290.00","","mvm7hz@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","CSE","164000, 171400, 806000","025Z, 096Z, 7556","$0.00","This Virtual Organization (VO) will facilitate communication and collaboration among CISE scientists currently involved in pandemic research through the NSF RAPID program. With the guidance of a Steering Committee composed of members from industry, academia, and government agencies, the VO will encourage the sharing of research results in a way not available without a concerted effort. The depth and breadth of multi-disciplinary collaboration enabled by this VO will be extended to include researchers, educators, and students interested in general topics related to pandemic planning and resilience. Utilizing a variety of dissemination platforms, the VO will harness the synergies of the CISE RAPID research programs to facilitate scientific advances and advance public health in the US and around the world.<br/><br/>The VO will: (i) facilitate the collection of a comprehensive collection of data sets, software tools, and documentation that can be shared by the research community; (ii) identification of new research efforts resulting from the cross fertilization of ideas from various subdisciplines of CISE research; (iii) a research roadmap that proposes research directions in CISE sciences that can lead to effective methods to prepare for and recover from future pandemics; and (iv) training and other pedagogic materials needed for educating future generations of scientists in topics related to pandemic preparedness, recovery, and resilience. The increased levels of collaboration fostered by this VO have the potential to result in innovative computational methods and technologies for dealing with future pandemics. The research roadmap will include identification of key research topics, risks, and gaps in the current R&D landscape that will significantly benefit the research community and serve as the blueprint for researchers, funding agencies, and policy makers on the role of information and communication technologies (ICT) in developing break-through solutions for pandemic resilience. While it is not possible to prevent pandemics completely, developing resilience techniques will enable society to prepare for and cope with the aftermath in a more effective manner.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1227110","Network for Computational Nanotechnology Cyber Platform","EEC","EWFD-Eng Workforce Development, ERC-Eng Research Centers, NANOSCALE: SCIENCE & ENGIN CTR, Special Projects - CCF, CYBERINFRASTRUCTURE, NanoSim-Nanosim Groups-Network, ENG NNI Special Studies, Data Cyberinfrastructure, Software Institutes","12/01/2012","11/03/2023","Gerhard Klimeck","IN","Purdue University","Cooperative Agreement","Dana L. Denick","11/30/2024","$30,792,392.00","Michael Zentner, Lynn Zentner, Alejandro Strachan, Michael McLennan, Krishna Madhavan","gekco@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","ENG","136000, 148000, 167500, 287800, 723100, 760400, 768100, 772600, 800400","026Z, 113E, 116E, 124E, 131E, 7219, 7433, 7680, 8004, 8048, 8211, 9178, 9251","$0.00","Network for Computational Nanotechnology (NCN) was founded in 2002 to advance nanoscience toward nanotechnology via online simulations on nanoHUB.org. Not only has nanoHUB become the first broadly successful, scientific end-to-end cloud computing environment, but it also has evolved well beyond online simulation. Annually, nanoHUB provides a library of 3,000 learning resources to 195,000 users worldwide. Its 232 simulation tools, free from the limitations of running software locally, are used in the cloud by over 10,800 annually. Its impact is demonstrated by 720+ citations to nanoHUB in the scientific literature with over 4,807 secondary citations, yielding an h-index of 31, and by a median time from publication of a research simulation program to classroom use of less than 6 months. Cumulatively, over 14,000 students in over 760 formal classes in over 100 institutions have used nanoHUB simulations. <br/><br/>Despite a decade of transformational success for a broad nanotechnology research and education community, significant gaps remain as work is still performed by isolated individuals and small groups. This fragmentation by specialty hinders tool and data sharing across knowledge domains. Nano areas such as bio, photonics, and materials are only beginning to use nanoHUB while manufacturing, informatics, environmental-health-and-safety are to date not even represented on nanoHUB. The NCN Cyber Platform proposes to address these gaps through efforts in three strategic goals to: 1) accelerate research by transforming nanoscience to nanotechnology through the integration of simulation with experimentation; 2) inspire and educate the next-generation nanoscience and nanotechnology workforce; and 3) grow the nanoHUB society that uses and shares nanoHUB content. Five cross-cutting thrust areas focus on the cyberinfrastructure (CI) and social dynamics of the nanoHUB virtual society: CI innovation; content stewardship and node engagement; education research and precollege/college and lifelong learning; outreach, diversity, and marketing; and CI operations. The 10-year NCN nanoHUB Cyber Platform vision is that nanoHUB will be the online nano society that researchers, practitioners, educators and students depend on day-to-day while simultaneously immersed in professional practice and computational resources for a multidisciplinary culture of innovation grounded in cloud services-enabled workflows.<br/><br/>Intellectual Merit: The NCN nanoHUB strategic plan will answer two fundamental challenges to the next-generation nanoHUB experience: 1) development of technologies that enable simple management and publication of scientific data (experimental and simulation) without additional complex steps: and 2) the establishment of a value system that fosters publication of data, tools, and lectures similar to today's rewards for journal publications. CI innovation, developed through the leading HUBzero platform as well as in cooperation with other CI efforts, will enable new connection points for research, education, and commercialization, expanded platform tool features to help users exchange and publish data; combined data and tools for verification, validation, and engineering activities; and increase immersive and pervasive features. Through partnerships with professional societies and commercial publishers, nanoHUB will change how researchers publish their simulation results through novel interactive journals that reflect a user's workflow, link directly back to their data, and make the work reproducible. This value system will drive new content toward nanoHUB, obviating the need for content generation to be monetarily supported by NCN. Through partnerships with the three new NCN content nodes and other NSF-funded nano efforts, NCN will continue to foster content creation to demonstrate value to the authors and will prototype, test, and host the proposed new technologies for broad usage.<br/><br/>Broader Impacts: NCN has developed processes that enabled researchers to rapidly deploy their research codes and innovative tutorials and classes on nanoHUB. To date, these processes harvested research and educational results from 890 contributors world-wide. Expansion into new areas of nano research and education, including pre-college education, represent a huge growth potential for nanoHUB that goes beyond simulation to embracing data management, search, and exploration. Focus on diversity will continue to be an integral part of NCN's outreach program, in particular through focused workshops and new initiatives such as EPICS High. The NCN-pioneered HUBzero already powers 40 HUBs at 12 institutions, serving a broad range of science and engineering disciplines and commercialization. Through impact assessment and continual contributions to HUBzero software stack releases, nanoHUB will continue to drive impact beyond its nano society into other disciplines and institutions."
"2047388","CAREER: SHF: Chiplet-Package Co-Optimizations for 2.5D Heterogeneous SoCs with Low-Overhead IOs","CCF","Information Technology Researc, Software & Hardware Foundation","06/01/2021","09/15/2023","Yarui Peng","AR","University of Arkansas","Continuing Grant","Sankar Basu","05/31/2026","$390,262.00","","yrpeng@uark.edu","1125 W MAPLE ST STE 316","FAYETTEVILLE","AR","727013124","4795753845","CSE","164000, 779800","1045, 7945","$0.00","Design of 2.5D chiplets is becoming increasingly popular as a flexible and scalable More-than-Moore solution to push computational performance of integrated circuit chips. With heterogeneous integration, each IP block can be implemented using the optimum technology node, maximizing design flexibility and performance. However, a drawback compared with a monolithic 2D chip is the large overhead introduced by inter-chiplet communication. On-package wires have much larger parasitics compared with on-chip interconnects. Thus, they may reduce the performance and energy-efficiency of 2.5D systems. Designing these physical IOs are expensive and time-consuming. Heterogeneous components cannot be easily integrated without a commonly used standard, and designers need to conservatively reserve a large design margin, which inevitably results in non-optimum designs and increased costs. The timing, power, and signal integrity properties between chiplets and the package are also not captured by the traditional design flow and CAD tools. The missing low-overhead, low-cost, and customizable IOs and design tools significantly slow down the adoption of these advanced packaging techniques. This project uses a chiplet-package co-design methodology to combine IC and package designs and provide a seamless environment for heterogeneous development. The goal is to minimize inter-chiplet performance overhead, reduce design costs, explore the full potential of 2.5D systems, and demonstrate the highest integration density and energy efficiency. The proposed chip design efforts and CAD tools will be used to provide educational materials, hands-on experience, and guest lectures to students. The project will contribute to the development of much needed US workforce in the area of research and development of semiconductors via a diverse set of plans for industry collaboration, pre-college education, and college and graduate-level education at the PI's institution, and in the state of Arkansas. The design tools resulting from the project will be open-sourced and packaged with design examples and documentation. <br/><br/>This 2.5D chiplet-package co-design flow will eliminate the boundary between chiplets and the package and combine additional design synthesis, extraction, and optimization steps to minimize overhead and costs. The IO synthesis will create and place small IO cells with just-enough sizing based on detailed extraction results. The active package synthesis will generate on-package buffers and re-timers to optimize area, wirelength, and performance. Holistic and In-Context extraction for chiplet-to-package coupling will provide the highest accuracy for both homogeneous and heterogeneous designs. The fabrication and testing of 2.5D systems is intended to ensure realistic validation with measured data. This CAD flow will further break the boundary between low-voltage and high-power engineering and enable computer-on-package with heterogeneous integration of Si logic and GaN/SiC power conversion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346616","CC* CIRA: High-performance computing solutions for small Midwest institutions","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","04/01/2024","03/20/2024","Aleksandar Poleksic","IA","University of Northern Iowa","Standard Grant","Kevin Thompson","03/31/2025","$173,368.00","Timothy Kidd, Pavel Lukashev, Sarah Diesburg, Dheryta Jaisinghani","poleksic@cs.uni.edu","1227 W 27TH ST","CEDAR FALLS","IA","506140012","3192733217","CSE","808000, 915000","9150","$0.00","High-performance computing is one of the main drivers of scientific discoveries and technological innovations. However, due to the lack of high-performance computing resources and the rise of big data, teaching and research in data science and other emerging fields is becoming out of reach for small institutions of higher education. This cyberinfrastructure research alignment project provides a high-performance computing environment to advance research and foster scientific collaborations at small academic institutions in the Midwest. The project establishes partnerships among five non-R1 institutions in Iowa and Minnesota to plan a regional computing network that connects these institutions via research collaborations and outreach activities. <br/><br/>The seed institutions on the project, Macalester College, Grinnell College, Central College, Coe College, and the University of Northern Iowa, are teaming up to allow faculty and students to explore the increasingly relevant field of high-performance computing. This is accomplished by studying how the research and classroom needs at these institutions correspond to hardware and networking requirements and by identifying opportunities for intra-institutional collaborations that will facilitate effective instruction and scientific discoveries. The overarching goal is to create a computing network that combines modern, centralized hardware resources, housed at the University of Northern Iowa, with the interdisciplinary expertise of faculty at the partnering organizations. A significant part of the project focuses on a feasibility plan for including community colleges and other small regional institutions in the network. This will make careers in an exciting, fast-growing, and data-driven world accessible to students of all backgrounds.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835618","Collaborative Research: Framework: Data: Toward Exascale Community Ocean Circulation Modeling","OAC","Data Cyberinfrastructure","11/01/2018","03/20/2024","Christopher Hill","MA","Massachusetts Institute of Technology","Standard Grant","Marlon Pierce","09/30/2024","$728,217.00","Paul O'Gorman","cnh@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","CSE","772600","062Z, 077Z, 7925","$0.00","This project designs and implements a software framework for handling petabyte-scale datasets; the focus is on global ocean circulation.  A team of three universities (Johns Hopkins University, MIT, and Columbia University) builds a unified data system that is capable of delivering global ocean circulation model output at 1 km horizontal resolution. The product will be hosted in an open portal, providing the community with scalable software tools to enable analysis of the dataset. The team will use this data to answer specific questions about mixing and dissipation processes in the ocean. <br/><br/>The goal of this effort is the creation and demonstration of a complete and replicable cyberinfrastructure for sharing and analysis of massive simulations.  The focus is on high resolution ocean circulation modeling, with software tools that will enable efficient storage. Two major challenges to the study of ocean and climate dynamics are addressed: handling large datasets from high-resolution simulations, and understanding the role of small-scale ocean processes in large-scale ocean/climate systems.  Resolving the first challenge would significantly facilitate ongoing and future studies of the ocean/atmosphere/climate system; addressing the second challenge would profoundly improve understanding of ocean/climate dynamics. The project builds a unified data system consisting of high-resolution global ocean circulation simulations, a petascale portal for data sharing, and scalable software tools for interactive analysis.   The software framework from this project is expected to handle petascale to exascale datasets for users.  Several pre-existing capabilities are leveraged for this project: the JHU regional numerical model of the Spill Jet on the East Greenland continental slope, software from the Pangeo project, the SciServer data-intensive software infrastructure, and lessons learned from the North East Storage Exchange multi-petabyte regional data store.  The broader target is next generation simulation software in the geosciences and other disciplines.  <br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Division of Ocean Sciences and the Integrative and Collaborative Education and Research Program within the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2144410","CAREER: Make Them Pay! Algorithms for Securing Wireless Systems","CCF","Information Technology Researc, Algorithmic Foundations","04/01/2022","09/12/2023","Maxwell Young","MS","Mississippi State University","Continuing Grant","Peter Brass","03/31/2027","$236,393.00","","maxwelly@outlook.com","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","CSE","164000, 779600","102Z, 1045, 7934, 9150","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Wireless systems are often populated by energy-constrained devices, and communication occurs over a shared channel. These features make such systems especially vulnerable to jamming, whereby an adversary disrupts the shared channel, forcing devices to expend significant energy in order to mitigate the attack. The goal of this research is to design and analyze novel defenses that are energy-efficient, while still providing performance guarantees against a powerful jamming adversary. The outcomes of this research have the potential to secure systems against these attacks, whose severity is likely to escalate with the continued adoption of wireless technology. This project integrates its objectives with curriculum development and research opportunities for both undergraduate and graduate students at Mississippi State University. <br/><br/>The approach taken by this project differs from much of the prior work on jamming mitigation, which has focused solely on the energy cost incurred by correct devices. In practice, adversarial devices must also expend energy to jam the channel, and the aim of this research is to design defenses that exploit this aspect. Specifically, this project aims to develop algorithms that 1) guarantee correct devices can accomplish computational tasks despite jamming, 2) are energy-efficient in the absence of attack, and 3) impose an asymptotically-higher cost on the adversary relative to the correct devices when an attack is underway; such algorithms are called resource-competitive. Broadly, this project focuses on communication and coordination tasks that serve as fundamental building blocks for many wireless protocols. By providing a resource-competitive treatment of these tasks, this research addresses theoretical problems that have the potential to improve the security for a range of applications<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346668","CC* CIRA: Building Research Innovations in Community Colleges in TX In Collaboration with Universities (BRICCs-TX)","OAC","Campus Cyberinfrastructure","04/01/2024","03/29/2024","Sarah Janes","TX","San Jacinto College District","Standard Grant","Amy Apon","03/31/2026","$199,344.00","James Howell","sarah.janes@sjcd.edu","4624 FAIRMONT PKWY STE 106","PASADENA","TX","775043323","2819986146","CSE","808000","","$0.00","Building Research Innovations in Community Colleges in TX (BRICCs-TX) is designed to help Texas community colleges prepare their students for the research and data analytics that is required to be successful both in the workplace as well as in the attainment of higher degrees. It is more efficient and effective if educational institutions at all levels collaborate to share tools and resources for the common good.  With the Texas Higher Education Coordinating Board (THECB), BRICCS-TX is providing an online platform for sharing ideas, tools and resources, all institutions of higher education have access to the same information. <br/><br/>The goals of BRICCs-TX are to: 1) encourage research at all levels and in both technical and academic areas at community colleges, 2) encourage collaboration between community colleges and universities in research opportunities, 3) share computing power and cyberinfrastructure solutions and opportunities for collaboration in the cloud and in hardware across the state, 4) host workshops around the state and virtually with research, computing power and cyberinfrastructure as the focus, and 5) work with the THECB to establish a hub on their OERTX website that will serve as a repository of information on research collaboration and cyberinfrastructure supporting research.  The project is working with community colleges in various regions in the state, including South Plains College, Houston Community College and Lone Star, as well as universities Texas A&M and Texas Southern University.  San Jacinto College leads a team that exemplifies collaboration and communication to empower students from diverse demographics as better prepared individuals for problem solving using the latest technology available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2044815","CAREER: Extensibility in Theory and Practice","CCF","Information Technology Researc, Software & Hardware Foundation","04/01/2021","09/12/2023","John Garrett Morris","IA","University of Iowa","Continuing Grant","Anindya Banerjee","03/31/2026","$401,951.00","","garrett-morris@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","CSE","164000, 779800","1045, 7943","$0.00","Modern software engineering relies on large ecosystems of independent software components and libraries.  This poses two challenges for the design of programming languages and language tooling, both of which are addressed by this project.  First, languages must support developing independent components, including identifying and abstracting reusable components from existing applications.  Second, languages must provide expressive specification mechanisms to ensure that components are used correctly and that components do not have unintended interactions.  The project's novelties are new programming language features for modular specification and implementation of program data and behavior, at all level of the software stack from user-facing applications to operating system components and hardware support.  The project's impacts are improvements in both software reliability and programmer productivity, as the project enables language tooling to automatically help programmers identify and correctly use software components and libraries.<br/><br/>The project has two primary themes.  The first is modularity and reuse in high-level functional programs.  The project develops extensible types for data and computational effects, supporting high-level abstractions including overloading, generic programming, and extensible effect handlers.  The second is modularity and reuse in low-level and systems programs.  The project develops extensible bit-level specifications of data structures and their layout, targeting applications including operating system kernels and hardware interfaces.  Each of these themes will only have impact if the resulting language features have comparable performance with existing approaches.  The project uses linear typing and compile-type specialization to offset runtime costs traditionally associated with generic and extensible programming techniques.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346635","CC* Regional Networking: Connecting Colorado's Western Slope Small Institutions of Higher Education to the Front Range GigaPoP Regional R&E Infrastructure","OAC","Campus Cyberinfrastructure","04/01/2024","03/22/2024","Marla Meehl","CO","University Corporation For Atmospheric Res","Standard Grant","Kevin Thompson","03/31/2026","$1,103,549.00","Scot Colburn","marla@ucar.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","CSE","808000","","$0.00","High-speed, reliable, networking infrastructure is vital to an organization?s ability to thrive and flourish in today?s rapidly evolving scientific and technical environments.  Such network connectivity is essential for success in business, scientific investigation, communication, global collaboration, education, and outreach.  Western Colorado is geographically challenged for network connectivity, with expansive rural areas, low population density, and natural impediments of mountains and rocky terrain.  Colorado?s ?Front Range? benefits from larger, denser populations where high-speed network connectivity is prevalent and relatively affordable.  A ?continental? digital divide separates the level of Research and Education (R&E) network access available to the Front Range compared to the communities west of the Continental Divide, the ?Western Slope.?<br/><br/>The project extends the Front Range GigaPoP?s (FRGP's) R&E network connectivity to two universities (Colorado Mesa University and Western Colorado University) and to the Rocky Mountain Biological Laboratory on the Colorado?s Western Slopes. This project partners with an established regional non-profit middle mile network provider to create a Virtual Routing and Forwarding network overlay to deliver R&E services. This creates a regional aggregation point with its partners and connects the institutions using 10 Gbps leased fiber circuits. The aggregation point replaces commercial, low-bandwidth internet services.  This project provides R&E organizations on the Western Slope the infrastructure to aggregate their network traffic and fully participate in the FRGP.  This model can be replicated to connect additional communities to the R&E infrastructure, which optimizes the value of partnerships and existing network infrastructure investments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335909","Collaborative Research: NSF Workshop on Automated, Programmable and Self Driving Labs","CNS","Information Technology Researc","10/01/2023","08/17/2023","Thomas Kurfess","GA","Georgia Tech Research Corporation","Standard Grant","Xiaogang (Cliff)  Wang","09/30/2024","$12,048.00","","kurfess@gatech.edu","926 DALNEY ST NW","ATLANTA","GA","303186395","4048944819","CSE","164000","025Z, 7556","$0.00","Laboratory automation increases precision and efficiency of science experiments. Advances in low-cost sensors, actuators, robotic systems, and control systems have lowered the barrier to entry to laboratory automation such that fully self-driving labs will have the potential to enable new practices of science experiment and to accelerate scientific exploration progress. There is a critical need to develop the principles and methodologies for self-driving laboratories. These systems will likely draw from best practices and experiences learned in data science, human-machine interaction, manufacturing and quality control, open-source ecosystems, and laboratory science methods. <br/><br/>The proposed workshop will convene leaders in self-driving laboratories and related areas including data science, robotics, manufacturing, and open-source ecosystems to define a roadmap for self-driving laboratories. Experts will discuss on latest advances and current challenges in automated sample preparation, experiment generation, data collection, and data analysis. The workshop will help identify major themes and assess on how future, interconnected goals can be best supported in a research context. By convening community leader around related topics, the workshop will seed cross-discipline collaboration on infrastructure that can support a broad range of sciences, which may include more complex experiments to accelerate scientific discovery and perform cost effective verification and validation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2143434","CAREER: Enriching Conversational Information Retrieval via Mixed-Initiative Interactions","IIS","Information Technology Researc, Info Integration & Informatics","07/01/2022","08/16/2022","Hamed Zamani","MA","University of Massachusetts Amherst","Continuing Grant","Judith Cushing","06/30/2027","$570,863.00","","zamani@cs.umass.edu","101 COMMONWEALTH AVE","AMHERST","MA","010039252","4135450698","CSE","164000, 736400","102Z, 1045, 7364","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>It has become clear that providing access to information through natural language conversations will play a significant role in the future of search technology. This will be enabled by developing efficient and effective conversational search engines. Existing systems are generally designed based on a query-response paradigm, in which the user initiates the interaction by submitting typing a word or phrase, and the system responds with one or more documents. This process repeats itself until the user either receives a useful response or terminates the search session. This is not an optimal design for interaction. A better approach would be to create search systems that operate like a conversation. In a conversational search systems, for instance, the system may ask a clarifying question or can recommend new information even though it is not an explicit response to the search query. A conversational search system, the conversation should yield the information that is needed to facilitate the ultimate goal of user satisfaction. The mentioned query-response paradigm does not support these natural conversational interactions. This CAREER award aims to advance the state-of-the-art by envisioning solutions that go beyond this  query-response paradigm.<br/><br/>To achieve this goal, this project studies theoretical and machine learning solutions for generating and handling mixed-initiative interactions in information seeking conversations. In more detail, this project explores the following three research thrusts: (1) developing theoretical foundations for measuring mixed-initiative information seeking conversations; (2) developing models for clarifying the user's information needs which is considered as the most common mixed-initiative interaction type; and (3) developing models for proactive informational contributions to ongoing conversations. In addition to these algorithmic and modeling contributions, this project also develops a number of invaluable resources for advancing the field of conversational information retrieval, including a conversational scholarly assistant agent that will be used as a tool for online experimentation and public data creation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201525","CC* Regional: NCShare Science DMZ","OAC","Campus Cyberinfrastructure","04/15/2022","09/13/2023","Tracy Futhey","NC","Duke University","Standard Grant","Kevin Thompson","03/31/2025","$984,868.00","Eva Kraus, Kevin Davis, Tracy Doaks, Deepak Kumar, Joel Faison","futhey@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","CSE","808000","","$0.00","This project creates a shared high-performance network to enhance research and education at minority-serving and smaller institutions in North Carolina. Such networks, typically undertaken by individual universities, establish a parallel physical network infrastructure (a Science network) to interconnect researchers on the campus to external sites. Conversely, this project builds a shared, regionally based, Science network operating on the existing state-wide research and education network, rather than as a separate infrastructure. The resulting Science network is expected to lower costs, require fewer local (campus) support personnel, and provide fast and unrestricted data movement to multiple institutions.<br/><br/>The network is hosted at MCNC, North Carolina?s research and education network operator, and is a collaboration with Duke University, North Carolina Central University and Davidson College. Technically, the project establishes parallel ""friction-free"" paths for 5-7 participating institutions by virtualizing existing last-mile circuits between university networks and the statewide network. Project components include a Data Transfer Node, sufficient storage, and modest computation to demonstrate the efficacy of the shared Science network model. Shared resources are accessed using federated identity management and out-of-band traffic inspection ensures security.<br/><br/>By identifying transmissions between trusted research sites (regionally/nationally) and securely routing that traffic around local campus security inspections that are intended to examine general, untrusted network traffic, the speed of Science network transmissions is significantly improved. This virtualized approach increases accessibility of high-speed data-driven research by democratizing access to advanced Cyberinfrastructure, enhancing research productivity, promoting collaboration, and reducing the time required for scientific discoveries at participating minority-serving and smaller institutions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346752","Equipment: CC* Campus Compute: A High-Performance Computing System for Research and Education in Arkansas","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","07/01/2024","03/20/2024","Salvador Barraza-Lopez","AR","University of Arkansas","Standard Grant","Amy Apon","06/30/2026","$499,113.00","David Chaffin, Tulin Kaman","sbarraza@uark.edu","1125 W MAPLE ST STE 316","FAYETTEVILLE","AR","727013124","4795753845","CSE","808000, 915000","9150","$0.00","This award updates computing resources in the Arkansas High Performance Computing Center (AHPCC) to support research and education in Arkansas, an EPSCoR state. AHPCC is based at the University of Arkansas and supports higher education institutions in the state, including the University of Arkansas at Pine Bluff (an HBCU), Arkansas Tech University, the University of Arkansas at Little Rock, the University of Arkansas for Medical Sciences, Philander Smith University (an HBCU), and others. This acquisition doubles all-user CPU capacity on a much more efficient system, adding  37 dual-socket server computers interconnected with a 100 Gb/s InfiniBand HDR-100 network and including supporting equipment such as an administrative Ethernet network, and two computer racks with power distribution, and water cooling.<br/><br/>The system supports research aligned with data science engineering, artificial intelligence, computational physics, materials science, biology, chemistry, and biochemistry, and continues a tradition of high-impact discoveries enabled by AHPCC. Examples of cutting-edge science enabled by this acquisition include: model order reduction and physics-based machine learning algorithms, new 2D multiferroics and materials for neuromorphic computing, dynamics of soft capsules within nano-channels, and assessing global warming through microbial evolution. Research activities to recruit, train and prepare the STEM workforce are also supported, especially in data-analytics, for which there is a strong need from local companies. The research and technical teams publish research in journals and conferences and deliver lecture series and workshops that train statewide users on High Performance Computing technologies and attract new users. The award ensures an increased participation of highly qualified women and under-represented faculty and students, helping create a more diverse workforce. This instrument is part of the Open Science Grid Consortium.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2344341","Collaborative Research: IMR: MM-1B: Privacy-Preserving Data Sharing for Mobile Internet Measurement and Traffic Analytics","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","09/14/2023","Feng Ye","WI","University of Wisconsin-Madison","Continuing Grant","Deepankar Medhi","09/30/2026","$64,963.00","","feng.ye@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","CSE","164000, 736300","115Z, 7363","$0.00","Mobile Internet measurement is critical to network design, resource allocation, and troubleshooting network issues. However, sharing of mobile Internet measurement data can potentially compromise user privacy. Given the wide introduction of artificial intelligence to mobile Internet measurement and traffic analytics, there is an urgent need for data sharing solutions that provide explainability in terms of the trade-offs among data quality, utility and quantity. To close the gap, the objective of this project is to develop new methods to augment data with explainable data quality and utility, to access and share collected data in a privacy-preserving manner, and to collaboratively analyze Internet data with intelligence and autonomy.<br/><br/>This collaborative project brings together investigators from University of Nebraska-Lincoln, Utah State University, and University of Wisconsin-Madison. It aims to lay a solid foundation for mobile Internet measurement with privacy preservation, collaborative and distributed intelligence, and autonomy. Methodologies and methods will be developed for quality-explainable data synthesis and augmentation; privacy-preserving data sharing; and collaborative and privacy-preserving analysis of Internet measurement data. Moreover, a mobile Internet traffic generator will be developed for evaluating the proposed methods. This project can significantly advance the prior research in Internet traffic analytics, quality-explainable and privacy-preserving data processing, mobile Internet traffic analytics, distributed artificial intelligence and machine learning algorithms, optimizations, modeling, simulations, and testbed experiments. <br/><br/>The research efforts associated with this project will greatly advance the understandings of the critical issues in the next-generation mobile Internet measurement with distributed and collaborative intelligence to provide privacy-preserving data sharing and Internet traffic analytics. The outcomes of the project can potently foster the transition of our society into data sharing with privacy and intelligent era. Research and education will be integrated in this project by introducing emerging mobile Internet measurement and privacy-preserving data processing with advanced topics such as 6G wireless systems, data augmentation, artificial intelligence and machine learning models into the current curricula in the three collaborative institutions.<br/><br/>The project website is hosted at: cns.unl.edu/imr-ppds. The collected data, simulation codes, and publication list will be published on the project website. Copies of technical reports and accepted manuscripts will also be published on the project website. The website will be maintained during the project years, and remain accessible for least 2 years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019012","CC* Integration-Large: N-DISE: NDN for Data Intensive Science Experiments","OAC","CISE Research Resources, Campus Cyberinfrastructure","10/01/2020","08/17/2023","Edmund Yeh","MA","Northeastern University","Standard Grant","Deepankar Medhi","09/30/2025","$1,025,000.00","Lixia Zhang, Harvey Newman, Jason Cong, Susmit Shannigrahi","eyeh@ece.neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","CSE","289000, 808000","9102","$0.00","The project, N-DISE (Named Data Networking for Data Intensive Science Experiments), aims to accelerate the pace of breakthroughs and innovations in data-intensive science fields such as the Large Hadron Collider (LHC) high energy physics program and the BioGenome and human genome projects. Based on Named Data Networking (NDN), a data-centric architecture, N-DISE will deploy and commission a highly efficient and field-tested petascale data distribution, caching, access and analysis system serving major science programs.<br/><br/>The N-DISE project will design and develop high-throughput caching and forwarding methods, containerization techniques, hierarchical memory management subsystems, congestion control mechanisms, integrated with Field Programmable Gate Arrays (FPGA) acceleration subsystems, to produce a system capable of delivering LHC and genomic data over a wide area network at throughputs approaching 100 Gbits per second, while significantly decreasing download time. In addition, N-DISE will utilize NDN's built-in data security support to ensure data integrity and provenance tracing. N-DISE will leverage existing infrastructure and build an enhanced testbed with four additional high performance NDN data cache servers at participating institutions.<br/><br/>N-DISE will provide a field-tested working prototype of a multi-domain data distribution and access system offering fast access and low cost, as well as data integrity and provenance, to many data-intensive science and engineering fields. The project plans to hold annual workshops and hackathons to train students, postdocs, and other researchers on NDN architectural design, algorithms, as well as implementation methodologies for specific data-intensive science environments. The project will undertake initiatives for actively involving under-represented groups, and for educational outreach to K-12 students.<br/><br/>N-DISE will maintain a GitHub repository at https://github.com/neu-yehlab/n-dise. The repository will host up-to-date publications, code, data, results, and simulators. The repository will be maintained by the team for at least three years beyond the duration of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835648","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure","11/01/2018","05/12/2020","Linda Schadler","NY","Rensselaer Polytechnic Institute","Standard Grant","Alejandro Suarez","10/31/2024","$1,459,023.00","Deborah McGuinness","linda.schadler@uvm.edu","110 8TH ST","TROY","NY","121803590","5182766000","CSE","171200, 772600","054Z, 062Z, 077Z, 7925, 9251","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1817105","NeTS: Small: Intelligent Optical Networks using Virtualization and Software-Defined Control","CNS","GVF - Global Venture Fund, Information Technology Researc, Special Projects - CNS, CISE Research Resources, Networking Technology and Syst","10/01/2018","07/31/2023","Byravamurthy Ramamurthy","NE","University of Nebraska-Lincoln","Standard Grant","Ann Von Lehmen","07/31/2024","$665,995.00","","byrav@cse.unl.edu","2200 VINE ST","LINCOLN","NE","685032427","4024723171","CSE","054Y00, 164000, 171400, 289000, 736300","044Z, 120Z, 7363, 7923, 9150, 9251","$0.00","Software-defined networking (SDN) is a promising paradigm that has the potential to enable Internet Service Providers (ISPs) to achieve a high level of network automation and therefore significant cost savings. Indeed software-defined wide area networks (SD-WANs) are emerging in industry and are projected to account for 25 percent of WAN traffic by 2021.  However, many challenges remain. The networks of large ISPs such as AT&T are complex, and most Internet traffic traverses multiple network domains.  It is highly desirable, then, to extend the software control paradigm to enable fast, efficient, cost effective end-to-end provisioning of services and resources over heterogeneous, multi-domain networks in an integrated and unified way.  The goal of this project is to address research challenges in this area, leveraging SDN and investigating novel strategies for expediting efficient data transfer across the fiber optic backbone networks underlying the Internet.  <br/><br/>A unified network architecture for a software defined optical network (SDON) will be developed that features: (1) End-to-end provisioning of services, and (2) Expediting data transfers over multiple domains that use heterogeneous transport layer technologies using inter-domain tunnels. SDONs will usher in virtualization and intelligence as the new features of future networks. The project will explore different strategies for dynamic resource allocation including the creation and use of virtual transport links (VTL) and associated bandwidth-on-demand (BoD) techniques. The project will also explore a Resource Delayed Release (RDR) strategy to reduce the dynamic service provisioning time and multi-domain tunnels to expedite data transfers in multi-layer networks involving Optical Transport Network (OTN), Wavelength Division Multiplexing (WDM) and Elastic Optical Networking (EON) layers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2216084","MRI: Acquisition of a High-Performance Computational System for OAK Region to Enable Computing and Data Driven Discovery","OAC","Major Research Instrumentation, Information Technology Researc, CYBERINFRASTRUCTURE, EPSCoR Co-Funding","08/15/2022","09/07/2022","Pratul Agarwal","OK","Oklahoma State University","Standard Grant","Alejandro Suarez","07/31/2025","$4,000,000.00","Xiuzhen Huang, William Hsu, Janet Twomey, James Deaton","pratul.agarwal@okstate.edu","401 WHITEHURST HALL","STILLWATER","OK","740781031","4057449995","CSE","118900, 164000, 723100, 915000","9102, 9150","$0.00","This project will acquire and deploy a high-performance computing (HPC) system to serve the researchers and educators in the broad Oklahoma-Arkansas-Kansas (OAK) region. Computational modeling, simulations, and data analytics are essential tools for new discoveries across all areas of science, engineering and mathematics. In particular, modern research breakthroughs are being driven by vast amounts of data enabled by computing and memory capacity, often combined with machine-learning/artificial intelligence (ML/AI) techniques. This system will seed the growth of a collaborative HPC ecosystem in the region, serving the needs of experienced users and enabling first time users, especially from small and under-served institutions. Technical know-how exchange, regular seminars, and conferences will promote collaborative research endeavors of regional and national importance. Hands on training of graduate, undergraduate and high-school students in computational and data sciences will broadly improve the skill-sets of the science and engineering workforce in the OAK region.  <br/><br/>The deployed system will harness the power of the latest CPUs, cutting-edge graphics processing units (GPUs), 100 Terabytes of aggregate memory, an HDR InfiniBand interconnect, and Petabyte-scale high-speed storage. This system consisting of several types of nodes (CPUs-only, mid-range GPUs, and high-end GPUs) will provide close to 100 million-core hours of computing and will serve as a vital regional resource for science and engineering research. Initial research areas to be served by the system include biology, human and animal health, agriculture, environment research, chemistry and chemical engineering, semiconductor materials research, cybersecurity and social network modeling, renewable energy research, seismology, high-energy physics, and medical physics. This significantly improved computing and data analysis capabilities provided by this system will greatly benefit scientific productivity. Furthermore, the instrument will make it possible for faculty to include HPC and data analysis techniques in the STEM education curriculum, leading to computational thinking becoming a part of research in the minds of the next generation of scientists. Lowering the barrier of entry to HPC will also help improve the education and training of women, minorities and under-represented groups. This system will also contribute computing resources beyond the OAK region through collaborative mechanisms such as the Partnership to Advance Throughput computing (PATh) and the Open Science Grid (OSG).<br/><br/>This project is jointly funded by the Major Research Instrumentation (MRI) program, the Established Program to Stimulate Competitive Research (EPSCoR), and the Computer & Information Science & Engineering (CISE) Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2044633","CAREER: Fast, Energy Efficient Irregular Kernels via Neural Accerlation","CCF","Information Technology Researc, Software & Hardware Foundation, EPSCoR Co-Funding","04/01/2021","09/13/2023","Joshua Booth","AL","University of Alabama in Huntsville","Continuing Grant","Almadena Chtchelkanova","03/31/2026","$380,475.00","","joshua.booth@uah.edu","301 SPARKMAN DR NW","HUNTSVILLE","AL","358051911","2568242657","CSE","164000, 779800, 915000","1045, 7942, 9150","$0.00","High-performance computing suffers from a performance bottleneck that wastes computation time, money, and energy, as processing cores on multicore systems sit idle waiting for memory accesses from irregular kernels. These irregular kernels normally accomplish little computational work despite the high cost of accessing memory. These costly bottlenecks must be remedied by a new approach to high-performance computing. But, at the same time, computing is evolving and is becoming less dependent on the low-level programming languages that cause these bottlenecks and more dependent on learning algorithms such as neural networks to attain the necessary efficiency. This project builds the foundation for accelerating irregular kernels by replacing them with neural networks that run on accelerators optimized for neural networks. These neural networks offer better performance and energy consumption. Additionally, these networks are tuned in high-level programming languages (e.g., Python) that are easier for novice users to learn. This allows more computer scientists to aid the scientific and high-performance computing communities. This project also builds a new curriculum such as adding neural accelerators and expanding neural network algorithm materials into traditional undergraduate courses. This project, in both its research and educational aspects, significantly reduces the development time and costs of high-performance computing while simultaneously reducing performance bottlenecks. Furthermore, this project will support graduate and undergraduate students as they engage in cross-disciplinary involvement to match accuracy and performance constraints from the scientific-modeling and big-data-analysis communities that currently depend on irregular kernels for areas such as climate modeling, large scale circuit design, and drug analysis on infectious diseases. <br/><br/>The goals and scope of this project are to build a framework that allows irregular kernels to be optimized in terms of both their performance and energy usage using the technique of neural acceleration, i.e., being represented and executed as a neural network. The methods used to meet the project?s goals and scope include the following: 1) The development of an approximation-bound characteristic that quantifies and qualifies acceptable error bars on the developed neural networks along with performance and energy requirements; 2) The development of initial neural networks for commonly used irregular kernels that can be used as starting networks for more complex irregular kernels and be used by individuals tuning their irregular kernels (which will be made available by a public database that is created and maintained by the investigator to support research in this area); and 3) The construction of a toolchain to aid in identifying irregular kernels in code, constructing neural networks based on user input, and deciding how the neural networks should be scheduled. The deliverable toolchain has support for popular libraries like TensorFlow and will be disseminated via an open-source repository. The transformative impact of this project?s effort generates a completely new optimization option for irregular kernels and a base set of tools (i.e., a public database and scheduling toolchain) that will foster future advances into using neural acceleration for various codes and lead to significant advancements in science and engineering. As such, this new optimization option may inspire a new computational model in a post-Moore era that provides timely scientific data for urgent government policy, such as climate change and foreign affairs.<br/>This project is jointly funded by CAREER Software and Hardware Foundations  HPC program and  the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201548","CC* Regional:  Promoting Research and Education at Small Colleges in the Atlanta University Center and at Tuskegee University through Network Architecture Enhancements","OAC","Campus Cyberinfrastructure","04/15/2022","01/25/2024","Samuel D'Angelo","GA","Georgia Tech Research Corporation","Standard Grant","Kevin Thompson","03/31/2025","$1,091,776.00","Abraham George, Bernice Green, James Hunter, Sean Vernon, Charles Cooper, John Wilson, Reginald Brinson","cas.dangelo@oit.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","303186395","4048944819","CSE","808000","","$0.00","The Georgia Institute of Technology (GT) in collaboration with Southern Light Rail and Southern Crossroads (SLR/SoX) provides advanced networking services for the research and education (R&E) communities across the Southeastern United States. This project extends advanced network services and provides cyberinfrastructure (Cl) access, training and support to researchers and educators from four historically Minority Serving Institutions (MSIs), three of them from the Atlanta University Center Consortium, plus Tuskegee University.<br/><br/>GT works with these MSIs to implement robust and secure 10 or 100 gigabit networking that enable and support the computational research and education at these institutions. Furthermore, GT and SoX are establishing and engaging in outreach activities and are identifying other researchers in these institutions who are interested in accessing Cl resources and training. The project addresses MSI?s networking and Cl needs and enhances their connectivity. By enabling, supporting, and promoting connectivity and computational research at MSIs, this project has a transformative impact on these institutions' research and education activities campus-wide. It strengthens the research ecosystem and network of expertise in the Southeastern states and expands the range of institutions involved in computational research.<br/><br/>The network connectivity facilitated by this project increases research training and workforce development for students at four partner Minority Serving institutions and lays the groundwork for project expansion to other MSIs. Additionally, the project team continues training and fostering mentoring opportunities for a diverse group of students, including women and minorities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346088","CC* Planning: Leading Advanced University Computing for Higher Education (LAUNCH) MSU","OAC","Campus Cyberinfrastructure, EPSCoR Co-Funding","04/01/2024","03/18/2024","Michael Navicky","MS","Mississippi State University","Standard Grant","Amy Apon","03/31/2025","$99,310.00","Carolina Siniscalchi, Todd Hall, Lauren Geiger, Dawn Reynolds","navicky@hpc.msstate.edu","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","CSE","808000, 915000","9150","$0.00","Mississippi State University Leading Advanced University Computing for Higher Education, or MSU LAUNCH, is engaging with researchers to make plans to provide comprehensive research computing and data storage support to a broader group of MSU researchers. The team is gathering information about the current cyberinfrastructure on campus and writing a new cyberinfrastructure plan, which will serve as basis for future endeavors. The results of the Research Computing and Data Capabilities Model are the starting point for these efforts.  The initial assessment showed areas in which MSU excels and identified areas in which support to researchers could be improved. <br/><br/>This project will enable interaction of campus IT leadership and researchers across several domain areas to assess the appropriate level of cyberinfrastructure and support services needed at MSU. The outcomes from this planning project will ensure the continued development of a competitive research environment to address the most challenging research questions. This project will also democratize science by including research domain areas that do not have regular and recurring access to computational resources. As Mississippi?s largest research institution, Mississippi State University serves a diverse undergraduate and graduate student population. Of the 21,961 students enrolled on the main campus in the Fall of 2022, 27% were classified as minority students (including Black or African American, American Indian, Asian, Hispanic, Native Hawaiian or Other Pacific Islander, and multiracial students). MSU LAUNCH will improve access to a broad community of users to computational resources and services, an objective that aligns with the long-term strategic goals of Mississippi State University, in an EPSCoR jurisdiction.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104105","Elements: Collaborative Research: Community-driven Environment of AI-powered Noise Reduction Services for Materials Discovery from Electron Microscopy Data","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure, Software Institutes","06/01/2021","05/24/2021","Peter Crozier","AZ","Arizona State University","Standard Grant","Varun Chandola","05/31/2025","$300,068.00","","crozier@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","CSE","171200, 772600, 800400","054Z, 077Z, 094Z, 095Z, 7923, 8004, 8396, 8397","$0.00","The goal of this project is to create cyberinfrastructure (CI) powered by artificial intelligence (AI) for sustained innovation in materials science. Deep understanding of materials is critical for progress in technologies related to energy, communication, construction, transportation and human health. The revolutionary progress of deep learning has been enabled by the availability of open-source AI models and open-access benchmark databases. However, the existing codebases and datasets relevant to image processing focus mostly on photographic images. In order to promote the sustained development of AI technology that can have significant impact in materials science, it is critical to provide data and AI models that are tailored to this domain. The developed CI will address this need by providing software to process images obtained from electron-microscopes, a technique enabling atoms to be visualized, and has the potential to enable transformative breakthroughs in varied and important areas of materials science. The CI is explicitly designed to foster the growth of a sustainable community of users and developers of AI technology at the intersection of the materials and data science communities, and to empower materials scientists to simulate their own datasets and develop their own AI models for scientific discovery. The developed AI-powered CI will therefore enable transformative progress in atomic-level understanding of materials, which will have broader impacts in health, energy, environment, and biotechnology. The CI environment will contribute to training materials scientists in AI technology, connecting them to the AI community, and providing software, data, and support materials to initiate them in AI-powered research. Educational and outreach plans are designed to facilitate interactions between the materials science and AI communities. Outreach activities specifically targeted to the general public, and to high-school teachers and their students, will expose them to materials science, electron microscopy, and AI. The project is committed to providing opportunities to women and underrepresented groups and will prioritize diversity in collaboration with the NYU Center for Data Science diversity committee.<br/><br/>Developing a fundamental understanding of atomic level structure and dynamics is critical for transformative advances in materials science. Aberration-corrected transmission electron microscopy is a primary tool to accomplish this goal. Unfortunately, the information content of microscopy data may be severely limited by poor signal-to-noise ratios. This is particularly true for radiation sensitive materials and experiments where high time resolution is required to investigate dynamic kinetic processes. AI methodology can exploit prior information about material structure by training deep neural nets with extensive simulations. These approaches may significantly outperform existing state-of-the-art methods, especially for non-periodic structures, including defects, interfaces, and surfaces. The developed CI will provide AI noise reduction services which will yield immediate advances and impacts for zeolites, metal organic frameworks, protein-material interfaces, liquid phase nucleation and growth, liquid-solid interfaces, and fluxional behavior in catalytic nanoparticles. In addition, the project will advance methodology for the design of AI-oriented CI. The CI is strategically designed to create a holistic environment for the use and development of AI technology in a specific scientific domain. It will attract domain scientists with little AI expertise, by providing software where the AI technology is transparent to the end user. Exposure to the technology will motivate the scientific community to design and train their own models, which will be facilitated by the open-source codebase in the AI repository. The open-access database combined with the repository will attract AI practitioners with little domain expertise, by giving them access to well-curated data and a clear specification of the relevant AI tasks. These services will be jump-started and supported through multiple educational and outreach activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2243355","CRII: CIF: A Machine Learning-based Computational Framework for Large-Scale Stochastic Programming","CCF","GVF - Global Venture Fund, Information Technology Researc, Special Projects - CCF, Comm & Information Foundations","09/01/2022","08/31/2022","Zhijie Dong","TX","University of Houston","Standard Grant","James Fowler","08/31/2024","$176,340.00","","zdong5@central.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","CSE","054Y00, 164000, 287800, 779700","7936, 8228, 9102, CL10","$0.00","Modeling optimization problems under uncertainty is known as stochastic programming (SP). It has a variety of important applications, including disaster management, supply chain design, health care, and harvest planning. Most real-world problems are complicated enough to generate a very large-size SP model, which is difficult to solve. Quickly finding the optimal solutions of these models is critical for decision-making when facing uncertainties. Existing optimization algorithms have a limited capability of solving large-scale SP problems. Without being explicitly programmed, machine learning can give computers the ability to ""learn"" with data by using statistical techniques. The goal of this project is to create a machine learning-based computational framework to solve large-scale stochastic programming problems effectively and efficiently by integrating machine learning techniques into optimization algorithms. The project will broaden the scope and applicability of machine learning in operations research. Furthermore, this research will support the cross-disciplinary training of graduate and undergraduate students in engineering and computer sciences, as well as the development of new curricula in the interface of machine learning and optimization algorithms.<br/><br/>The project will be the pioneering study of applying machine learning into stochastic programming, while existing works usually focus on using stochastic programming to improve the efficiency of machine learning algorithms. Motivated by the challenges from practices and limitations of current optimization algorithms, two research objectives are proposed: efficient sample generation and convergence acceleration, by taking sample average approximation and L-shaped algorithm as examples. The first research objective is to design a semi-supervised learning algorithm based on solution information to efficiently generate samples for sample average approximation. The second research objective is to develop a supervised learning algorithm to estimate a tight upper bound for expediting convergence of L-shaped method. The two research objectives will be achieved through five tasks: (1) semi-supervised learning-based scenario grouping; (2) supervised learning based representative scenario selection; (3) performance analysis for sample generation; (4) supervised learning based upper bound prediction; and (5) performance analysis for the machine learning-based L-shaped method. The successes of this project will generate a new class of theoretical optimization methods that facilitate various real-world applications in disaster management, supply chain design, health care and harvest planning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322372","Research Infrastructure: CC* Data Storage: Rice Collaborative Object Store","OAC","Campus Cyberinfrastructure","08/01/2023","06/28/2023","Erik Engquist","TX","William Marsh Rice University","Standard Grant","Kevin Thompson","07/31/2025","$206,043.00","Melissa Cragin","erike@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","808000","","$0.00","Research in areas such as modeling and simulation, AI and machine learning, large-scale geospatial analysis, automated data mining of large digital text and image collections, and bioengineering produce terabyte and petabyte-scale data sets that require production-level storage, access, and transfer services. The Rice Collaborative Object Store adds an S3 compatible resource to on-campus cyberinfrastructure services, providing much needed capacity for multi-terabyte data, making that data available for local -or distant- computational analysis, sharing with distributed collaborations, or available for public access. As the research computing and data applications expand across fields, there is a need to create opportunities for workforce development in the area of cyberinfrastructure. This project introduces students to the advanced computing ecosystem, providing training on local storage and compute resources, and user facilitation. These student trainees actively participate in research engagement with the collaborating use case groups to utilize this new resource and the Open Storage Network (OSN).<br/><br/>The Rice Collaborative Object Store node joins the production-level OSN, which is part of the nation?s growing federated data fabric. Providing low-cost, efficient services based on open-source software, the OSN has nodes at many of the national or regional advanced computing centers. The system consists of three compute nodes in a high availability configuration, 1.9PB disk, and Ceph storage software, connected at 100 gigabits from the Rice University ScienceDMZ network. The resource contributes 20% of the available storage to the common OSN allocation pool that is available to researchers across the U.S.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201497","CC* Compute: Augmenting a 2,560-core EPYC2 Computational Cluster with GPUs for AI, Machine Learning, and other GPU-Accelerated HPC Applications","OAC","Campus Cyberinfrastructure","05/01/2022","09/12/2023","Kidambi Sreenivas","TN","University of Tennessee Chattanooga","Standard Grant","Amy Apon","04/30/2025","$415,868.00","Kidambi Sreenivas, Farah Kandah, Eleni Panagiotou, Yingfeng Wang","Kidambi-Sreenivas@utc.edu","615 MCCALLIE AVE","CHATTANOOGA","TN","374032504","4234254431","CSE","808000","9251","$0.00","This project augments an existing computing cluster at the University of Tennessee Chattanooga (UTC) with 36 NVIDIA A100 80GB Graphical Processing Units (GPUs) for 18 of the existing servers. This upgrade provides over a threefold increase in performance for those computers on many workloads, and even higher speed improvement for certain computational areas, such as artificial intelligence problems. GPUs are the best way, at present, to achieve extremely high computational performance cost-effectively on today's servers, workstations, and desktop systems. Adding GPUs to the existing servers is a straightforward upgrade process.  The upgrade enables 13 science drivers, or subprojects, spanning a range of domains and specialties, including researchers at UTC and among collaborating institutions nationwide. Undergraduate and graduate students benefit from using the upgraded computing faculty implemented through this project. <br/><br/>The 13 science drivers pursued in this project support current funded and unfunded research in addition to teaching activities for both undergraduate and graduate students. Also, external collaborators at the University of Alabama, University of New Mexico, and Worcester Polytechnic Institute will utilize a significant fraction of this scalable computing resource, usually in collaboration with UTC researchers. Expected users include the more than 40 computational science PhD students at UTC, plus postdocs, Masters students, and undergraduate researchers. The science-driver projects complement existing uses of the cluster while emphasizing GPU-accelerated research and creative activities, which are specifically enabled by the GPU upgrade supported by this funding. These science drivers include: Data-driven Methods for Predictive Intention Models for Drivers and Pedestrians; Above Ground Carbon Sequestration Using GIS and Remote Sensing for Chattanooga; Portable Performance Optimizations for Irregular Communication; Identifying Metabolites from the Data of Tandem Mass Spectrometry; and Molecular Dynamics Simulations of Active Filaments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346589","Research Infrastructure: CC* Regional Networking: The Pennsylvania Science DMZ supporting under resourced colleges and universities (PA Science DMZ)","OAC","Campus Cyberinfrastructure","12/15/2023","03/15/2024","Wayne Figurelle","PA","Pennsylvania State Univ University Park","Standard Grant","Kevin Thompson","11/30/2025","$1,092,012.00","Frederick Adkins, Wayne Figurelle, Jason Simms, Nathan Flood","wfigure@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","CSE","808000","","$0.00","Contemporary scientific research increasingly relies on robust and comprehensive data networks. Data-intensive instrumentation and methodologies like AI lead to incredible advances for the researchers fortunate enough to have access to the requisite cyberinfrastructure. Unfortunately, even modest deficiencies in capacity and connectivity can sharply limit scientific discovery, especially at under-resourced institutions.<br/><br/>The proposed Pennsylvania Science DMZ (PA Science DMZ) project addresses critical infrastructure and connectivity gaps in five participating institutions: Pennsylvania State University, Indiana University of Pennsylvania (IUP), Lafayette College, The Digital Foundry at New Kensington (DFNK), and Swarthmore College. At IUP, the project will immediately benefit ongoing efforts in data analytics and applied research in worker safety, robotics, and geospatial data, as well as enhancing cybersecurity and STEM education. In Lafayette College, the improved connectivity will support molecular dynamics simulations and biology research, and at DFNK it will allow enhanced external access to a Digital Learning and Demonstration Lab. Swarthmore College will employ the solution in collaborative linguistics and social interactions research.<br/><br/>In addition to these immediate applications, the proposed PA Science DMZ opens the door to a multitude of additional research opportunities, both within the initial consortium and through collaborations with smaller institutions that are currently unable to access critical tools of contemporary science. By implementing a regional model for aggregating cyberinfrastructure hardware and human resources, the project aims to provide a robust and economically efficient template for additional initiatives within the state and nationwide, fostering a more inclusive and interconnected cyberinfrastructure landscape.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2105648","Collaborative Research: NGSDI: CarbonFirst: A Sustainable and Reliable Carbon-Centric Cloud-Edge Software Infrastructure","CNS","Information Technology Researc, Special Projects - CNS","05/01/2021","05/11/2023","Adam Wierman","CA","California Institute of Technology","Continuing Grant","Daniel Andresen","04/30/2025","$361,111.00","","adamw@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","CSE","164000, 171400","","$0.00","Cloud computing platforms continue to grow exponentially, and are becoming the foundation of our information-based economy.  While the cloud?s energy demand grew more slowly than expected over the past decade due to aggressive energy-efficiency optimizations, there are few remaining optimization opportunities using traditional methods. As a result, the cloud?s continued exponential growth will translate into exponentially rising energy demand, which will position it as one of the primary contributors to global carbon emissions. To address the problem, this project elevates carbon to a first-class metric in designing a sustainable and reliable cloud-edge software infrastructure that can enable continued exponential growth.<br/><br/>The project's foundation is a software-defined energy virtualization layer that provides applications visibility into, and control of, their own energy and carbon usage. The project will leverage this foundation to develop higher-level systems abstractions for supporting carbon-efficient applications at different geographical scales including: a cluster balloon technique, which automatically adjusts applications? energy usage to match a volatile clean energy supply at local edge sites; edge hopping mechanisms, which exploit lower regional energy volatility to balance energy across edge sites; and carbon capping policies, which track applications? global grid carbon emissions and restrict grid energy after reaching the cap.<br/><br/>The project has the potential for significant societal impact by enabling commercial cloud platforms to sustainably continue their exponential growth.  The project will conduct outreach by incorporating topics from the proposal into summer programs for local middle and high school students at the partner institutions. The project will also impact the curriculum at these institutions by adopting elements of edge, cloud, and sustainable computing into graduate and advanced undergraduate courses.  Finally, the project will recruit a diverse group of students by leveraging institutional diversity efforts and will involve undergraduate students through Research Experience for Undergraduate (REU) projects.<br/><br/>The project will make its software artifacts, datasets, and research results available to the research community on the project website at http://www.carbonfirst.org and via the UMass Trace Repository  at http://traces.cs.umass.edu. Artifacts derived from this project will be maintained on the project website and the trace repository for a minimum of five years after the project's conclusion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810732","Center for Advancing the Societal Impacts of Research","OIA","Program Planning and Policy De, Genetic Mechanisms, OFFICE OF MULTIDISCIPLINARY AC, SSA-Special Studies & Analysis, Cross-Directorate  Activities, Hist Black Colleges and Univ, Information Technology Researc, GRANTED, Project & Program Evaluation, Eval & Assessment Capabilites, Integrat & Collab Ed & Rsearch","09/15/2018","08/15/2023","Susan Renoe","MO","University of Missouri-Columbia","Continuing Grant","Bernice Anderson","08/31/2024","$5,221,393.00","Diane Rover, Janice McDonnell, Kevin Niemi, Julie Risien","renoes@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","O/D","066Y00, 111200, 125300, 138500, 139700, 159400, 164000, 221Y00, 726100, 755000, 769900","9150","$0.00","The Center for Advancing the Societal Impacts of Research (CASIR) will advance the rigor, relevance, and practice of broader impacts (BI) by  (a) cultivating and strengthening the existent and emerging BI expert community; (b) building capacity of researchers and educators to enhance and articulate the broader impacts of their work; and (c) creating socio-technical infrastructure able to adapt to stakeholder needs as BI continues to grow and evolve. CASIR builds on the foundational work of the National Alliance for Broader Impacts and will advance the practice of translating scientific research for public understanding and meet the growing demand for innovative BI training and resources. <br/> <br/>The Center will develop resources and provide professional development to diverse audiences across multiple institution types and settings, including research-intensive universities, minority-serving institutions, technical and community colleges, and primarily undergraduate institutions in the jurisdictions of the Established Program to Stimulate Competitive Research.  CASIR will directly enhance BI capacity at the individual, departmental, institutional, and national levels. Particular focus will be given to individual researchers and institutions representing and serving traditionally under-served populations.  In addition, CASIR will facilitate dialogue and collaboration around evidence-based approaches to enhancing, evaluating, and documenting research impacts. Overall, the work will be valuable to the community of researchers driving discovery, the community of professionals who provide BI support and collaboration with researchers, and the public which stands to benefit from research and education projects that are well-designed and executed in a way that enhances their broader impacts.<br/><br/>NSF-wide support for this Center augments the Foundation's current efforts to educate research communities about the importance of the broader impacts criterion in the review process and to communicate the societal benefits of fundamental science and engineering research.  CASIR's emphasis on documentation, evidence, and best practices will support an evidence-building approach to investing in discovery and innovation. <br/> <br/>This award is co-funded by the Office of Integrative Activities (OIA) and the following Directorates:  Biological Sciences (BIO), Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), Geosciences (GEO), Mathematical and Physical Sciences (MPS), and Social, Behavioral, and Economic Sciences (SBE).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346097","CC* Planning: Establishing a Sustainable Framework for High-Performance Computing Growth at Wichita State University","OAC","Campus Cyberinfrastructure","06/01/2024","03/11/2024","Terrance Figy","KS","Wichita State University","Standard Grant","Amy Apon","05/31/2025","$100,000.00","Ryan Doll","Terrance.Figy@wichita.edu","1845 FAIRMOUNT ST # 38","WICHITA","KS","672609700","3169783285","CSE","808000","9150","$0.00","Wichita State University (WSU) is embarking on a Campus Cyberinfrastructure Master Plan to:  interconnect campus research labs, create a common campus research storage system and establish a regional compute system for High-Performance Computing (HPC).  This initiative aims to enhance university research and education missions while also contributing to the broader scientific community. The project involves developing strategic plans for HPC infrastructure enhancements, fostering interdisciplinary collaborations, supporting innovative research and education, and ensuring long-term sustainability of WSU's computing resources.  <br/><br/>The first goal involves developing a roadmap for a fixable and scalable next generation HPC system that meets the current and future computational demands. It includes conducting a gap analysis to identify the features and capacity needed to meet WSU's diverse research and educational needs. The second goal is to foster interdisciplinary collaborations. This will be achieved by developing an interdisciplinary computing institute that promotes collaboration between faculty, researchers, and students from diverse departments, organizing regular workshops, seminars, and networking events, and establishing strategic partnerships with national laboratories, government agencies, and industry partners.  The third goal is to support innovative research and education by integrating HPC resources into the curriculum across multiple disciplines, developing a comprehensive training program, and providing consulting services to researchers. The fourth and final goal is sustainability and long-term planning, which involves developing a long-term sustainability plan and strengthening ties with the local community.<br/><br/>The main project deliverables include a Campus Cyberinfrastructure Master Plan and planning for future grant proposal submissions to the NSF, and other funding agencies, including the MRI and CC* programs in support of implementation of the Campus Cyberinfrastructure Master Plan.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2123447","Collaborative Research: HDR DSC: The Metropolitan Chicago Data Science Corps (MCDC): Learning from Data to Support Communities","IIS","HDR-Harnessing the Data Revolu, Information Technology Researc, Unallocated Program Costs","09/01/2021","03/20/2024","Suzan van der Lee","IL","Northwestern University","Continuing Grant","Sylvia Spengler","08/31/2024","$904,339.00","Michelle Birkett, Diane Schanzenbach, Bennett Goldberg","suzan@northwestern.edu","633 CLARK ST","EVANSTON","IL","602080001","3125037955","CSE","099Y00, 164000, 919900","062Z, 9102","$0.00","Diverse experts from five universities in, or with a presence in, the Chicago area are collaborating as the Metropolitan Chicago Data-science Corps to 1) help local non-profit organizations take advantage of increasing data volume and data complexity, 2) train data science students in how to effectively apply their academic knowledge to real data challenges in the non-profit sector, 3) exchange data science curriculum and expertise among these universities and with local community colleges. An organization can submit a Request for Data Services (RDS) and receive help to develop it. MCDC particularly welcomes RDS in areas related to environment, health, and our social well-being. Each RDS is assigned to a team of students. Teams of students with a foundation in data science are formed within a practicum course or as part of a summer internship. MCDC will develop the practicum course, where each team has one or more expert mentors and forms a partnership with the requesting organization. At the end of the term, each team will deliver a solution to each of the requesting organizations.<br/><br/>MCDC is an interdisciplinary partnership between universities, myriad community organizations, and two expansion colleges and aims to strengthen the national data science workforce by integrating community needs with academic learning. By supporting infrastructure to unite diverse students and faculty across institutions and disciplines, by prioritizing the engagement of community, and embedding real-world team-based data science projects into the curriculum, the MCDC will be a uniquely powerful educational experience which will support societal progress. To realize this goal, existing curricula are grouped into multiple pathways to prepare a diverse range of students for participation in MCDC. MCDC students acquire both data acumen and societal knowledge that is intended to lead to a well-prepared and engaged workforce. The MCDC project directors combine extensive, proven, funded, and diverse expertise in curriculum development, inclusive learning practices, integrating real-world data into courses, learning systems, data science, as well as in health, social, and environmental sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2334288","Conference: NSF Workshop on the Convergence of Smart Sensing Systems, Applications, Analytic and Decision Making","CNS","Information Technology Researc","10/01/2023","08/31/2023","Mingyi Hong","MN","University of Minnesota-Twin Cities","Standard Grant","Vishal Sharma","12/31/2024","$100,000.00","","mhong@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","CSE","164000","7556","$0.00","We live in a highly interconnected world, and this interconnectivity is predicted to grow exponentially in the next decade. As a result of this rapid expansion, we anticipate a network of more than 50 billion interconnected smart systems, encompassing intelligent appliances, cars, gadgets, and tools. These systems possess the capability to collect extensive real-time data, perform complex computational tasks, and deliver valuable services and actionable insights, greatly enriching our lives and collective productivity. Essential components of these smart systems include distributed sensors, communication modules, as well as analytical and computational modules. These components are intricately intertwined and rely on each other. Overall, there is a natural synergy between different sensing technologies, their applications, and methodologies developed in analytical areas. The objective of the proposed workshop is to identify significant research and educational challenges pertaining to these areas related to smart sensing technologies. <br/><br/>More specifically, the workshop aims to address significant research gaps in how researchers approach problems and formulate solutions in sensing applications while integrating tools and algorithms from analytical areas. Overall, the workshop aims to foster collaboration, knowledge exchange, and innovation between experts from various fields to advance the understanding and application of analytical tools in sensing systems. The main goals and areas of focus are as follows: <br/><br/>1 )Aligning the State-of-the-Art of Analytical Areas: The workshop aims to bridge the gap between various scientific fields, academia, and industry in data science, particularly machine learning, by presenting the cutting-edge computational and analytical capabilities in a way that is understandable to the sensing community. It seeks to establish effective methods to keep sensing system researchers updated on advancements in analytical areas and make these practices available in a publicly accessible repository. <br/><br/>2) Appreciating Algorithmic Limitations & Application Requirements: The workshop recognizes that the direct application of advanced algorithms from analytical areas to sensing systems can be distracting and limited, especially when considering the physical constraints inherent in sensing systems. Researchers need to develop customized computational and analytical tools that respect these constraints and align with specific application requirements.<br/> <br/>3) Application-Specific Sensor Systems: The workshop acknowledges the broad range of sensors used in various applications and aims to develop new sensors or integrate existing ones to meet specific application requirements. It brings together experts from different fields to define and deliver technologies that address performance, cost, sampling, environmental, deployment, data collection, and ease-of-use needs in sensor systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346737","CC* Data Storage: High-Capacity Active Archive to Enable Economical Data Access and Distribution for Illinois Researchers and the National Community","OAC","Campus Cyberinfrastructure","04/01/2024","03/13/2024","John Maloney","IL","University of Illinois at Urbana-Champaign","Standard Grant","Kevin Thompson","03/31/2026","$500,000.00","Heidi Imker, Timothy Boerner","malone12@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","808000","","$0.00","With continued development of high-resolution instruments, networked sensors, and the ability to deploy these tools in an ever-growing number of capacities, the need for researchers to store and share large datasets has never been more pressing. Additionally, the rise of new data processing techniques leveraging the power of machine learning and AI algorithms allow researchers to glean new insights from existing datasets than was initially thought possible when the data was first captured. Science domains from agriculture to physics and medical imaging to astronomy, require larger and larger data storage options that allow for durable storage with robust data sharing capabilities.<br/><br/>Granite is a cutting-edge tape-based data subsystem that allows researchers to store and retrieve data both via traditional POSIX methods, such as via Globus, and newer, more flexible standards, such as S3. These interfaces connect researchers to Granite?s 19-frame SpectraLogic TFinity dual-robotic tape library with 20 LTO-9 tape drives, 36PB of raw tape capacity, and Versity?s ScoutFS/ScoutAM archive management tools. Connectivity to Granite is provided by redundant 100Gbps paths to the University of Illinois Urbana-Champaign?s Science DMZ, Internet2, the Energy Sciences Network (ESnet), the Big Ten Academic Alliance?s OmniPoP, and the Metropolitan Research & Education Network (MREN). Available to Illinois researchers via the Illinois Data Bank and other Illinois allocation processes, and to the national research community via integration with the NSF Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) capability, Granite serves a large segment of researchers across the United States.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322218","CC* Regional Computing: Great Plains Extended Network of GPUs for Interactive Experimenters (GP-ENGINE)","OAC","Campus Cyberinfrastructure","07/01/2023","10/20/2023","Grant Scott","MO","University of Missouri-Columbia","Standard Grant","Amy Apon","06/30/2025","$981,182.00","Daniel Andresen, Derek Weitzel, Brian Burkhart, Paul Kern","GrantScott@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","CSE","808000","","$0.00","The GP-ENGINE project advances the adoption of advanced computing and data resources in the Great Plains Network region.  This project will increase the number of researchers and students served by both local and national computing resources, strengthen the capacity and capabilities of campus research computing professionals, and expand the regional capacity for research. Researchers will be able to transition nascent ideas and codes into advanced computing code using locally provisioned advanced computing resources.  These codes can be later executed on national high-throughput computing resources. These successes will enhance institutional buy-in for sustainable regional and national research computing systems.<br/><br/>The project leverages strong existing collaborations to provision and manage graphics processing unit resources in Missouri, North Dakota, South Dakota, Kansas, Oklahoma, and Arkansas. It trains and supports researchers to adapt their workbench codes into high-throughput computing codes that can be executed on national platforms such as the Open Science Grid and the National Research Platform. The project addresses computing needs of a diverse set of science drivers from across the consortium, including problems in 3D protein molecule generation, satellite image deep learning for wildfire burn area mapping, dark matter and neutrino detection, real-time monitoring of land surface phenology, cybersecurity attack graph generation, and intelligent manufacturing with digital twins.  The project improves STEM research and education through adoption of JupyterLab computing notebooks. The deployed cyberinfrastructure supports research and education in a region whose sparse population and geographic size are ideal for developing advanced computing, data, and networking in under-resourced EPSCoR states.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346286","CC* Networking Infrastructure: Enabling Big Science and Big Data Projects at the University of Massachusetts","OAC","Campus Cyberinfrastructure","04/01/2024","03/13/2024","Rafael Coelho Lopes de Sa","MA","University of Massachusetts Amherst","Standard Grant","Kevin Thompson","09/30/2024","$337,363.00","James Mileski, Michael Zink","rclsa@umass.edu","101 COMMONWEALTH AVE","AMHERST","MA","010039252","4135450698","CSE","808000","","$0.00","Big science and big data scientific projects rely critically on high speed, reliable internet network connections to share and process scientific data. Data flows through major national and international research networks, which connect national laboratories and research universities across the world. The main goal of this project is to provide a modern network connection between the University of Massachusetts (UMass) Amherst and these networks to enable scientific projects in Astronomy, Computer Science, Life Sciences, and Physics. <br/><br/>Several big data scientific projects at UMass Amherst rely on sharing information with other research institutes across the world via major national and international research networks, such as ESnet and Internet2. The main computing infrastructure for the projects at UMass Amherst is located at the Massachusetts Green High Performance Computing Center (MGHPCC). This project connects the UMass Amherst computing resources at the MGHPCC directly to the major research networks. The connection is carried out as part of the Northeast Research and Education Network (NEREN), which is a consortium of several educational institutions in the northeast and upstate NY. The new 400+ Gbps fiber-based connection between these institutions and the major research networks meets the required modern standards for connection speed and reliability, and enables participation in big data research projects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1931348","CSSI Elements: DataSwarm: A User-Level Framework for Data Intensive Scientific Applications","OAC","Data Cyberinfrastructure, Software Institutes","09/01/2019","04/07/2022","Douglas Thain","IN","University of Notre Dame","Standard Grant","Varun Chandola","08/31/2024","$578,994.00","","dthain@nd.edu","836 GRACE HALL","NOTRE DAME","IN","465566031","5746317432","CSE","772600, 800400","077Z, 7923, 8048, 9251","$0.00","This project creates a capability that will support the construction of large, data intensive scientific applications that must run on top of national cyberinfrastructure, such as large campus clusters, NSF extreme-scale computing facilities, the Open Science Grid, and commercial clouds.  The new capability (DataSwarm) brings data requirements and software dependencies to the target cyberinfrastructure systems, and deploys them as and when required, rather than having these requirements pre-installed on the target systems.  The motivation comes from applications in high energy physics, molecular dynamics, and quantum chemistry.<br/><br/>The main motivation of the work is the challenge of scalable computing frameworks.  Based on a prior development by the Principal Investigator (Work Queue), the current project provides technical innovation in three areas: <br/>(1) Molecular Task Composition.  Molecular task composition is used as an abstraction for the precise construction of tasks that require a custom software environment, large data input, and a scratch data area to capture the outputs. By expressing these aspects explicitly instead of implicitly, the project improves the storage efficiency of large numbers of tasks. <br/>(2) In-Situ Data Management.  In-situ storage management is performed to offset the increased storage consumption likely to occur under molecular task composition, avoiding unpredictable failures of tasks due to storage exhaustion. <br/>(3) Precision Provenance.  Precision provenance of both data objects and task components enables the efficient re-use of resources across multiple runs, as well as precise incremental changes to complex workflows.<br/>For this project, the three key elements addressed are the software environment, input data, and a scratch data area. These elements are usually independently managed; here, they are bound together to form temporary ""molecules"" for task execution.  The three applications included in this project represent three typical types of complex data and complex software dependencies. They include custom late-stage data analysis codes in high energy physics, complex multidimensional optimization, and ensemble molecular dynamics, respectively.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1937599","RTML: Large: Acceleration to Graph-Based Machine Learning","CCF","Information Technology Researc, Special Projects - CCF","10/01/2019","08/27/2021","Jason Cong","CA","University of California-Los Angeles","Standard Grant","Sankar Basu","09/30/2024","$1,799,897.00","Yizhou Sun, Anthony Nowatzki","cong@cs.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244200","3107940102","CSE","164000, 287800","082Z, 2878, 7798","$0.00","Graphs are ubiquitous, and often the fundamental data structure in many applications including bioinformatics, chemistry, healthcare, social networks, recommender systems and systems analysis.  Machine learning (ML) using graphs is receiving increasing attention, both where graphs are a representation of data, as in graph neural networks (GNN) algorithms, and where graphs are an efficient ML model representation, as in arithmetic circuits representation of probabilistic graphical models.  While useful, graph-based ML poses unique challenges to existing computation hardware (Central Processing Units and Graphics Processing Units) due to the combination of irregular memory access and dynamic parallelism imposed by the graph structure and the dense computation required for relevant learning algorithms, though hardware-based implementations are highly desirable to enable real-time processing of streams of data generated by such applications.  The project addresses these challenges with a novel accelerator architecture for graph-based ML, along with a supporting open source software stack, simulator, and field-programmable gate-array (FPGA) prototype.  Beyond the technical contributions, the project will integrate the latest research into several graduate and upper-division undergraduate courses. The project will also work with the UCLA Center for Excellence in Engineering and Diversity (CEED) and Women in Engineering to recruit highly diversified undergraduate and graduate students to participate in the research. <br/><br/>The project targets a programmable and heterogeneous multi-accelerator architecture, with software-controlled compute and memory resources. It is specialized in the following ways to meet the needs of graph-based machine learning.  First, it supports composing accelerator engines for efficient  pipelining of graph-based prefetching with dense computation units. Second, the prefetching hardware will be co-designed with GNN algorithms to support recent and upcoming advances in graph sampling and graph-coarsening algorithms. Third, it will include a high bandwidth scratchpad architecture optimized for indirect access, and spatial compute fabrics (e.g. systolic arrays) optimized for dense computation. Finally, the execution model will be based on an architecture-aware task-parallel model, which has rich-enough primitives to take advantage of heterogeneous hardware, while being flexible enough to load balance for dynamic parallelism.  The key components of the proposed architecture will be prototyped on an FPGA. Overall, the goal of the work is to greatly advance the state-of-the-art of graph-based ML in terms of model accuracy, efficiency, and real-time inference and learning. The project will also collaborate with a synergistic DARPA program for related hardware development.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2215016","Collaborative Research:  CNS Core:  Large:  Systems and Verifiable Metrics for Sustainable Data Centers","CNS","Information Technology Researc","10/01/2022","08/02/2022","Yu David Liu","NY","SUNY at Binghamton","Continuing Grant","Daniel Andresen","09/30/2026","$179,722.00","Kanad Ghose","davidl@cs.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139024400","6077776136","CSE","164000","7925","$0.00","Data centers already contribute significantly to the global carbon footprint. However, the rise in popularity of resource-intensive Big Data and Machine Learning workloads is poised to make data center operations unsustainable. This project designs a suite of Sustainability Aware Software SYstems (SASSY) to enable ""sustainable-by-design"" data centers. SASSY focuses on sustainability holistically, considering the lifecycle carbon footprint of computing equipment, cleanliness of energy source, and device reliability. To measure per-job end-to-end sustainability costs, a full-stack measurement framework is developed. To involve end-users in sustainability efforts, new programming models and tools are designed to enable users to specify their sustainability and performance objectives. The metrics and models together guide SASSY to make wise data-center-wide sustainable management choices.<br/><br/>The adoption of SASSY solutions leads to sustainability savings that benefit the society at large. Further, the SASSY programming models and tools allow developers to build more sustainable applications, enabling ""sustainable-by-design"" software development. Data center operators and industry partners can directly benefit from SASSY's open-source software and models, which are made public through the project Website: https://www.pace.cs.stonybrook.edu/sassy.html. The next generation of practitioners and researchers are taught to consider sustainability as a first-class metric via educational and mentoring opportunities that the project generates.<br/><br/>This project was in response to and partially funded by Design for Sustainability in Computing (NSF-22-060)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1923601","SpecEES: Collaborative Research: DroTerNet: Coexistence between Drone and Terrestrial Wireless Networks","CNS","SpecEES Spectrum Efficiency, E, Information Technology Researc, Special Projects - CNS, Networking Technology and Syst","10/01/2019","09/13/2023","Andreas Molisch","CA","University of Southern California","Standard Grant","Alhussein Abouzeid","09/30/2024","$319,637.00","","molisch@usc.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900890701","2137407762","CSE","059Y00, 164000, 171400, 736300","044Z","$0.00","There is tremendous recent interest in drones with applications ranging from public safety, first responders, surveillance, to package delivery. Drones are also being considered as flying wireless nodes to augment the capabilities of current terrestrial communication networks. Irrespective of the application, drones need radio frequency (RF) spectrum to communicate with their ground control stations as well as with other drones and terrestrial nodes. Since transmissions from higher altitude have the potential of interfering with other wireless services over a large area, it is currently being debated whether and under what rules should drones share spectrum with existing networks or whether it is better to operate them over specifically licensed frequencies. In order to answer such important and timely questions, this project develops a new cross-disciplinary approach to the design and analysis of coexisting drone and terrestrial networks (DroTerNets) by blending ideas from multiple disciplines, such as spectrum sharing, communication theory, propagation science, test-bed development, machine learning, and stochastic network modeling. This research will inform both industry and government on spectrum usage by providing a scientific basis for the high-stakes ruling on spectrum for drones. Further broader impacts will be through student training and wide dissemination of results.  <br/><br/>The overarching goal of this research is to develop a holistic new approach to the spectral and energy efficiency analysis of DroTerNets, yielding the following key innovations: (i) A new learning framework based on the idea of determinantal point processes (DPPs) will be developed to facilitate both simulation-based and analytical characterization of the locations of simultaneously active nodes in a given frequency band for a variety of coexistence schemes, (ii) Drawing on multi-label classification in machine learning, a novel deep DPP-based channel assignment algorithm will be developed by utilizing the structure of DPP kernels to limit the search space, (iii) Non-linear receiver characteristics will be included in the learning framework to both quantify their effect on the energy and spectral efficiency of DroTerNets and to develop novel receiver-aware channel assignment schemes, (iv) Mobility constraints and characteristics of drones that result from the opportunistic access of the channel will be characterized and incorporated in the analysis, (v) Measurements and models of air-to-ground (A2G) channels in a variety of environments with particular emphasis on directional characteristics that determine the effectiveness of multi-antenna receivers will be obtained, and (vi) Experimental investigation and modeling of the correlation between terrestrial and A2G links will be performed to provide a solid foundation for coexistence margins.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1824737","CompCog: A Machine Learning Approach to Human Perceptual Similarity","BCS","Information Technology Researc, Perception, Action & Cognition, Robust Intelligence","10/01/2018","07/26/2023","Robert Jacobs","NY","University of Rochester","Standard Grant","Betty Tuller","09/30/2024","$400,000.00","","robbie@bcs.rochester.edu","910 GENESEE ST","ROCHESTER","NY","146113847","5852754031","SBE","164000, 725200, 749500","075Z, 7252, 7495","$0.00","Similarity is fundamental to nearly all aspects of human cognition. Perception uses similarity: when viewing a person's face, we (unconsciously) calculate its similarity to the faces of people we know in order to recognize who we are looking at. Categorization uses similarity: when judging whether a building was designed by the architect Frank Lloyd Wright, we calculate its similarity to buildings known to have been designed by Wright in order to make our best estimate. Reasoning and problem solving use similarity: when attempting to solve a calculus problem, we calculate its similarity to previous problems that we have encountered in order to determine a good solution strategy. However, how people calculate the similarity of two items is not yet understood. Which features of items do people use to calculate similarity? And how are the feature values of items compared in order to calculate similarity? This research project will use human experimentation and computational modeling to address these questions when items are viewed or grasped. A long-term benefit of the project is that a greater understanding of people's perceptual similarity judgments will provide a foundation for understanding how people calculate and use similarity in other areas of cognition. While conducting the research, undergraduate and graduate students will be mentored in the cross-disciplinary approach embodied in our investigation through participation in both experimental and computational aspects of the research project. <br/> <br/>This project focuses on developing a new empirical and theoretical foundation for understanding people's notions of similarity, particularly in the domain of perceptual similarity.  The field of cognitive science is well aware that understanding similarity is essential to understanding human cognition. Despite this, the primary motivation for this project is the belief that, to date, cognitive science's approach to the study of similarity judgments is much too simple---the restricted class of similarity metrics considered by cognitive scientists is unlikely to scale to large, realistic settings. The primary hypothesis of this project is that the field of machine learning---especially the study of metric learning---can supply cognitive science with a rich array of complex and sophisticated models, models that will be necessary to accurately characterize people's similarity notions in large, realistic domains. Machine learning has pioneered the study of mathematically rigorous linear and nonlinear similarity metrics. We believe that the time is ripe for the field of cognitive science to make use of machine learning's recent advances. Machine learning's metric learning framework extends and elaborates the cognitive science approach in principled and innovative new directions. Indeed, this framework presents an unparalleled opportunity for cognitive science with the potential for transforming this field. Using the empirical and theoretical findings from machine learning, cognitive scientists can now begin to explore human notions of similarity in more complex and sophisticated ways---and in more realistic domains---than has ever been possible. We regard the research project as an early step for cognitive science towards a more sophisticated understanding of people's notions of similarity. Because the project cannot study similarity in all domains of human cognition, it concentrates on perception. Future work will need to develop further the models proposed and evaluated here. If successful, the program will establish an empirical and theoretical foundation that can subsequently be extended to many other areas of human cognition.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2421861","Conference: Research Computing at Smaller Institutions (RCSI)","OAC","Campus Cyberinfrastructure","05/01/2024","03/11/2024","Jason Simms","PA","Swarthmore College","Standard Grant","Amy Apon","04/30/2025","$29,276.00","","jsimms1@swarthmore.edu","500 COLLEGE AVE","SWARTHMORE","PA","190811390","6103288000","CSE","808000","7556","$0.00","This award provides participant support for the Research Computing for Smaller Institutions (RCSI) conference, held June 11-12, 2024 at Swarthmore College in Pennsylvania. Smaller and under-resourced institutions confront challenges related to data management, increasing adoption of computational methods across many disciplines, and support for complex collaborative projects, and must do so with fewer human, financial, and infrastructure resources. The conference addresses research computing challenges unique to such institutions, providing actionable pathways, idea exchanges, and community building for attendees.<br/><br/>The grant supports travel for approximately 26 attendees to attend the conference. The support makes possible the participation of those who otherwise would be unable to attend, an especially important consideration given the target audience of the event. Particular attention will be paid to directing support towards various underrepresented populations and institution types, as well as to those who normally might not attend a research computing conference, including library and appropriate administrative staff. Relevant outputs, such as presentations, will be made publicly available. Given the anticipated diversity and breadth of attendees, broader impacts on research computing practice in support of learning, pedagogy, and research will be extensive, including improved models for supporting emerging research computing integration in pedagogical contexts, guiding additional investment in relevant hardware and software, facilitating skills development for students, faculty, and CI professionals, and laying foundations for subsequent grants and other funding requests.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2212426","Collaborative Research: SHF: Medium: Approximate Computing for Machine Learning Security: Foundations and Accelerator Design","CCF","Information Technology Researc","08/01/2022","07/21/2022","Nael Abu-Ghazaleh","CA","University of California-Riverside","Continuing Grant","Almadena Chtchelkanova","07/31/2026","$388,873.00","Samet Oymak","nael@cs.ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","CSE","164000","1640, 7924, 7941","$0.00","Deep Neural Networks (DNNs) are achieving state-of-the-art performance on a large and expanding number of application domains.  However, one of the threats to their wide-scale deployment is vulnerability to adversarial machine learning attacks, where an adversary injects small perturbations to the input data that cause the DNN to misclassify, with potentially dangerous outcomes (for example, mistaking a stop sign for a speed limit sign).  In this project, the researchers will explore how building DNNs with approximate computing elements improves their robustness to these adversarial attacks.  Approximate computing is a technique to build computing elements that are simpler (and therefore higher performing and more sustainable) but do not compute the exact result of an operation.  The investigators will explore how to select approximate computing elements and use them in building sustainable DNN accelerators that balance performance, accuracy, and security.<br/><br/>The proposal's expected contributions include developing new insights into the relationship between approximation and robustness of DNNs.  The project will explore what types of approximation techniques result in effective DNNs that balance accuracy, performance, sustainability, and protection against adversarial attacks and develop optimization frameworks that can find optimal operating points along these dimensions.  It will also explore how to build new approximate computing elements specifically targeted toward this application.  The project will use these findings to build sustainable, performant, and accurate DNN accelerators.  The project will also explore other approximate computing-based techniques to protect against other types of attacks threatening the security and privacy of DNNs, as well as for different deep neural network learning structures.  The project is expected to have significant impacts on security, sustainability, and accuracy of machine learning models.  The research team will share all of the byproducts of the research with the research community.  The project will train graduate and undergraduate students.  The investigators will develop new educational material for use in machine learning, computer architecture, and computer security classes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019136","CC* CIRA: Building Research Innovation at Community Colleges","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","10/20/2020","Dhruva Chakravorty","TX","Texas A&M University","Continuing Grant","Kevin Thompson","06/30/2024","$250,000.00","Honggao Liu, Sarah Janes, Timothy Cockerill","chakravorty@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","CSE","723100, 808000","","$0.00","Two-year colleges and smaller institutions of higher education play an important role in shaping the country's economic and computing workforce.  Burgeoning interest in fields such as cloud computing and smart manufacturing have paved the path for adoption of advanced cyberinfrastructure practices in research and academic pursuits at these institutions.  Expanding on the collective experience of groups engaged in various aspects of campus computing, the Building Research Innovation at Community Colleges (BRICCs) approach examines the research and educational needs from advanced cyberinfrastructure in such institutional settings. Led by CyberTeams, BRICCs offers an inclusive platform to develop and extend efforts in this space to the national level.  <br/><br/>BRICCs is a unique opportunity to study campus computing characteristics at smaller institutions and community colleges. BRICCs adopts a multi-pronged approach that focuses on learning about the problems at hand, partnering with institutions to enable solutions, and finally communicating its findings to the broader research community. Fostering partnerships between knowledgeable cyberinfrastructure professionals and these institutions is a critical aspect of BRICCs. Toward achieving this, BRICCs will host virtual and in-person community workshops to explore avenues to broaden the impact of advanced cyberinfrastructure on campus computing at all levels. BRICCs will produce learning resources, workshop reports, support future funding efforts, and propose campus networking models that align with the  research and academic pursuits of smaller institutions. As a collective of computing expertise, BRICCs serve as a collaborative space to address similar challenges in advancing cyberinfrastructure adoption in research and educational settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346746","A  Secure Hub for Access, Reliability, and Exchange of Data (SHARED)","OAC","Campus Cyberinfrastructure","04/01/2024","03/11/2024","Torsten Reimer","IL","University of Chicago","Standard Grant","Kevin Thompson","03/31/2026","$499,998.00","Hakizumwami Runesha, John Carlstrom","reimer@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","808000","","$0.00","The University of Chicago's Secure Hub for Access, Reliability, and Exchange of Data, (SHARED), serves as a resource for data-driven research and nucleus of an integrated data management platform. It provides federated data storage for projects across disciplines to facilitate collaboration and best practice in data management. It is integrated with UChicago's institutional repository for data sharing according to FAIR principles. SHARED is led by the University Library and Research Computing Center, with collaboration from University Information Technology Services, the Data Sciences Institute, and the Physical Sciences Division. Aligned with the university's data lifecycle strategy for collection, analysis, publication, distribution, and long-term archiving, it ensures access to data and their integration into broader ecosystems. <br/><br/>SHARED caters to diverse scientific applications spanning cosmology, particle physics, linguistics, and psychology. Through SHARED?s 3.9PB ceph-based storage platform, the project supports projects focused on dark matter searches, physics beyond the Standard Model, simulations of cosmic reionization, surveys of cosmic microwave background radiation, and kinetic simulations of astrophysical plasmas. Additionally, it preserves valuable neural and linguistic data, contributing to understanding cognitive processes and language diversity. The SHARED project promotes interdisciplinary research and provides educational opportunities in data science through collaborations with minority-serving institutions and K-12 student engagement. Leveraging partnerships like the University of Chicago and Chicago City Colleges Data Science Preceptorship program, it enhances workforce diversity and fosters data-related education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346596","CC* Data Storage: Cost-effective Attached Storage for High throughput computing using Homo- geneous IT (CASH HIT) supporting Penn State Science, the Open Science Grid and LIGO","OAC","Campus Cyberinfrastructure","04/01/2024","03/11/2024","Chad Hanna","PA","Pennsylvania State Univ University Park","Standard Grant","Amy Apon","03/31/2026","$489,239.00","David Radice","crh184@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","CSE","808000","","$0.00","Scientific computing needs continue to grow at an exponential rate driven by an exponential growth in data that often outpaces available resources. High performance, cost effective, data storage is a critical component of scientific workflows that produce and analyze large datasets.  Penn State is providing cost-effective data storage to researchers across many scientific domains including physics, astronomy, biology and materials science.  This work supports science at various scales from local Penn State researchers to researchers across the country.  Much of the storage is available to external communities through the Open Science Grid, which allows US researchers to gain free access to computing resources.  The project team is exploring several important scientific topics with storage including searching for ripples in space caused by black holes, searching for planets outside of our solar system that could support life, and studying gene regulation.<br/><br/>Project goals include developing relationships with national cyberinfrastructure efforts such as the Partnership to Advance Throughput computing; building a workforce that is able to leverage cost-effective, open source storage; and enabling research that otherwise would not be possible with existing storage resources and budgetary constraints.  The project aims to provide 7.8 petabytes (PB) of storage.  2.9 PB of the storage (37%) will be available to external communities while the remaining 4.9 PB will be used internally. Among the externally allocated storage, 2 PB will be used for Open Science Data Federation (OSDF) applications and 900 TB will support a new regional initiative to provide storage for underserved institutions in Pennsylvania.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209628","Collaborative Research: Elements: TRAnsparency CErtified (TRACE): Trusting Computational Research Without Repeating It","OAC","Data Cyberinfrastructure, Software Institutes","07/15/2022","07/08/2022","Bertram Ludaescher","IL","University of Illinois at Urbana-Champaign","Standard Grant","Sylvia Spengler","06/30/2025","$349,999.00","Kacper Kowalik, Timothy McPhillips, Craig Willis","ludaesch@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","772600, 800400","077Z, 7923, 8004","$0.00","Research communities across the natural and social sciences are increasingly concerned about the transparency and reproducibility of results obtained by computational means. Calls for increased transparency can be found in the policies of peer-reviewed journals and processing pipelines employed in the creation of research data products made available through science gateways, data portals, and statistical agencies. These communities recognize that the integrity of published results and data products is uncertain when it is not possible to trace their lineage or validate their production. Verifying the transparency or reproducibility of computational artifacts?by repeating computations and comparing results?is expensive, time-consuming, and difficult, and may be infeasible if the research products rely on resources that are subject to legitimate restrictions such as the use of sensitive or proprietary data; streaming, transient, or ephemeral data; and large-scale or specialized computational resources available only to approved or authorized users. The TRACE project is addressing this problem through an approach called certified transparency - a trustworthy record of computations signed by the systems within which they were performed. Using TRACE, system owners and operators certify the original execution of a computational workflow that produces findings or data products. By using a TRACE-enabled system, researchers produce transparent computational artifacts that no longer require verification, reducing burden on journal editors and reviewers seeking to ensure reproducibility and transparency of computational results. TRACE presents an innovative and efficient approach to ensuring the transparency of research that uses computational methods, is consistent with the vision outlined by the National Academies, and enables evidence-based policymaking based on transparent and trustworthy science.<br/><br/>The central goal of the TRACE project is the development, validation, and implementation of a technical model of certified transparency. This includes a set of infrastructure elements that can be employed by system owners to (1) declare the dimensions of computational transparency supported by their platforms; (2) certify that a specific computational workflow was executed on the platform; and (3) bundle artifacts, records of their execution, technical metadata about their contents, and certify them for dissemination. The first phase of the project focuses on the development of a conceptual model and technical specification that can be used to certify the description of a system, termed a Transparency-Certified System (TRACE system), and the aggregation of artifacts along with records of their execution, termed Transparency-Certified Research Objects (TROs).  The second phase focuses on the development of reusable software components implementing the TRACE model and approach. To demonstrate certified transparency, the toolkit is used to TRACE-enable existing platforms including Whole Tale, SKOPE, and the SLURM workload manager. These TRACE-enabled systems produce certified TROs that can be trusted and do not need to be repeated or re-executed to verify that results were obtained as claimed.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Social and Economic Sciences within the Directorate for Social, Behavioral and Economic Sciences; and by the Division of Information and Intelligent Systems within the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2200792","CC* Compute:  The MSU Data Machine - a high-memory, GPU-enabled compute cluster for data-intensive and AI applications","OAC","Campus Cyberinfrastructure","07/01/2022","04/06/2022","Brian O'Shea","MI","Michigan State University","Standard Grant","Amy Apon","06/30/2024","$399,865.00","Phoebe Zarnetske, Arika Ligmann-Zielinska, Junlin Yuan, Matthew Schrenk","oshea@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","CSE","808000","","$0.00","The MSU Data Machine addresses the exponential growth of large and complex datasets in many fields of study, particularly those where computing has not been widely used or where the research and teaching approaches needed to work with ?big data?, which present a different set of computing requirements than in traditional high performance computing.  The Data Machine facilitates data-intensive research by having computing nodes with large amounts of memory, a high speed file system, graphics processing units that are optimized for machine learning and artificial intelligence-based analysis techniques, and a high speed file system.  It also includes software, usage policies, and training that makes it easy for users to interactively analyze and visualize their data. <br/><br/>This project focuses on four specific research areas - in the areas of microbial genomics, social system modeling, spatial and community ecology, and data-driven turbulence modeling - however, the Data Machine broadly enables MSU?s research community to pursue data-intensive research projects by lowering barriers to engaging with these types of resources.  The project also provides a valuable computational resource to the nation via the Open Science Grid and MSU?s NSF-funded Science DMZ project, advancing research in a wide spectrum of areas.  Furthermore, MSU undergraduate and graduate students are participating in the deployment and administration of the Data Machine as well as using it for research and educational activities, contributing to the development of a globally competitive STEM workforce and promoting the advancement of under-represented groups in computational and data science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2016379","CCRI: ENS: Collaborative Research: ns-3 Network Simulation for Next-Generation Wireless","CNS","Information Technology Researc, CCRI-CISE Cmnty Rsrch Infrstrc","10/01/2020","08/22/2023","Sumit Roy","WA","University of Washington","Standard Grant","Deepankar Medhi","09/30/2024","$1,037,657.00","","roy@ee.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","CSE","164000, 735900","044Z, 7359","$0.00","Simulation of emerging next-generation wireless networks continues to be an invaluable component of the design, evaluation and innovation cycle as a complement to other modes such as testbeds of performance testing.  This project will focus on upgrading ns-3, the most widely used open source network simulator, to meet the challenges of efficient yet accurate simulation based performance evaluation of 5G and beyond networks.  It will do this via model building for the evolutions in Wi-Fi (notably Wi-Fi 6) and cellular (notably 5G NR) technologies and incorporating new simulation techniques targeting dense and heterogeneous network use cases.<br/><br/>Network simulation faces fundamental challenges due to inherent increases in complexity, notably at the physical layer due to increasing bandwidth, MIMO (multiple-input, multiple-output) and multi-user operation as well as cross-layer (physical and multiple access) operation, necessary to deal with network scale and heterogeneity.  The primary objective is to develop simulation methods that achieve the desired balance between maintaining simulation run-time efficiency while preserving accuracy of measured network parameters (loss, throughput, latency) - in the face of increasing complexity. The research plan will explore a variety of techniques including efficient link-to-system mappings, pruning of network state representations that do not impact simulation accuracy, and parallelization approaches based on optimistic simulation.<br/><br/>In addition, the project plans to improve ns-3 usability and further adoption through increased community outreach and creation of new educational material to lower barriers to entry for a new generation of ns-3 users.  The recreated ns-3 Consortium hosted by the University of Washington will foster creation of training material for both novice and advanced users, to be archived and distributed online as well as in-person at leading networking/simulation conferences. <br/><br/>New content (educational materials and tutorials, research reports and publications) will be disseminated primarily via the ns-3 website https://www.nsnam.org/ and the two institutional lab home pages (the University of Washington - https://depts.washington.edu/funlab and Georgia Tech - http://blough.ece.gatech.edu/research/) along with other popular channels such as YouTube/Viemo for video.  Additionally, code under development will be preserved in archives such as Gitlab and announced via the ns-3 Users and Developers mailing lists.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019161","CC* Team: Oregon Big Data Research and Education Team","OAC","Campus Cyberinfrastructure","07/01/2020","11/26/2021","Brett Tyler","OR","Oregon State University","Continuing Grant","Kevin Thompson","06/30/2025","$1,400,000.00","Jacob Searcy, Jason Podrabsky","Brett.Tyler@oregonstate.edu","1500 SW JEFFERSON AVE","CORVALLIS","OR","973318655","5417374933","CSE","808000","","$0.00","Today, more than ever before, big data pervade every area of the life, environmental, biomedical, earth, marine, computational, physical, urban, and social sciences, as well as numerous other domains. Increasingly powerful computing technologies have opened the pathway for researchers to address major global challenges through use of large and heterogeneous data sets and through complex models and simulations. This project provides domain scientists, including research students, with the expertise and training needed to collaborate effectively with specialists in these advanced computational and statistical methodologies. The project also provides training and research experiences for students and instructors from small regional colleges, including Hispanic-serving and Native-American-serving institutions. To widely share best practices, it supports a community of practice. <br/><br/>The project employs research and training staff (facilitators) with expertise in data integration, multi-modal data analytics and machine learning. These three related sets of methods enable the analysis of large complex data sets of different types or from different sources, which may or may not have been collected as part of a planned studies. Specifically, a four-person facilitation team is established across Oregon State University, the University of Oregon, and Portland State University. The interdisciplinary, cross institutional team will establish the tools and managements practices to serve the researchers in the state.  The research facilitated by this project will lead to better understanding of earthquakes, diverse ecosystems, and plant and animal form and function. It supports development of faster computing systems, more secure energy systems, and improved environmental health. The data challenges posed by these application areas also motivate new foundational research in advanced data analytics and machine learning. The project also prepares a new generation of students from diverse backgrounds to enter the knowledge economy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209892","Frameworks: Garden: A FAIR Framework for Publishing and Applying AI Models for Translational Research in Science, Engineering, Education, and Industry","OAC","Data Cyberinfrastructure, Software Institutes","07/15/2022","07/14/2022","Ian Foster","IL","University of Chicago","Standard Grant","Varun Chandola","06/30/2026","$3,496,454.00","Rebecca Willett, Eliu Huerta Escudero, Benjamin Blaiszik, Rafael Gomez-Bombarelli","foster@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","772600, 800400","077Z, 8004, 9102","$0.00","Harnessing powerful new advances in machine learning (ML) and artificial intelligence (AI) is key to 1) maintaining and building national competitiveness in the sciences and engineering, 2) realizing breakthroughs in health and medicine, 3) enabling the creation of industries of the future, and 4) increasing economic growth and opportunity. Today, researchers are achieving exciting results with these new ML/AI methods in applications ranging from materials discovery, chemistry, and drug discovery to high energy physics, weather prediction, advanced manufacturing, and health. Yet, much work remains. These new methods and results are not easily applied by others due to the specialized expertise and resources needed to understand, develop, share, adapt, test, deploy, and run the resulting ML/AI models. To overcome these barriers to progress, this project seeks to develop methods and tools for constructing and creating Model Gardens, collections of curated and tested ML/AI models linked with the data and computing resources required to advance the work of a specific research community. Such new methods, software, and tools can make it simple for model producers to publish models in forms that are easily consumed by others, and for model consumers to discover published models and integrate them into their applications in academia or industry. The project connects researchers in materials science, physics, and chemistry enabling the establishment of Model Gardens for their communities and empowering key research centers to collect and provide broad access to new methods and models resulting from their work. Further, the project facilitates the connection of aspiring researchers with scientific problems, engaging hundreds of students from diverse backgrounds (including rural community college partners) in learning and contributing to software development, model publication, development of new AI/ML applications, and training of a next-generation ML/AI-empowered workforce through hosted workshops, open office hours, and development of a new engagement platform.<br/><br/>This project overcomes the barriers to the dissemination and application of new ML/AI methods by creating a new CSSI framework?the Garden Framework to support the construction and operation of Model Gardens: collections of curated models linked with the data and computing resources required to advance the work of specific communities. By reducing the friction associated with model publication, discovery, access, and deployment; providing for the disciplined and structured organization and linking of data, models, and code; associating appropriate metadata with models to promote reuse and discoverability, and applying quality assessment measures (e.g., automated testing, uncertainty quantification) to support model comparison; supporting the development of communities around specific model classes and research challenges; and permitting easy access to models without (and with) download and installation, established Model Gardens reduce barriers to the use of ML/AI methods and promote the nucleation of communities around specific datasets, methods, and models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346701","CC* Regional Computing:  The California State University System Technology Infrastructure for Data Exploration","OAC","Campus Cyberinfrastructure","01/01/2024","12/06/2023","Jerry Sheehan","CA","San Diego State University Foundation","Standard Grant","Amy Apon","12/31/2025","$991,749.00","Gerard Au, Luwen Huangfu, Rafael Espinosa, Bethany Gilden","jjsheehan@sdsu.edu","5250 CAMPANILE DR","SAN DIEGO","CA","921821901","6195945731","CSE","808000","","$0.00","The Technology Infrastructure for Data Exploration (TIDE) creates a pioneering computational core facility within the California State University (CSU) system, focused on machine learning and AI research. The computational core will include substantial GPU, CPU, and storage capacity to be housed at San Diego State University for researchers. <br/><br/>TIDE's primary objective is to enable scientific drivers across the CSU system, including artificial intelligence for humanoid robots, computational chemistry for tetrapyrrolic molecules, hyperdimensional computing for deep neural networks, and digital archeology featuring photogrammetric analysis. This foundational computational asset also addresses the lack of equitable access to cyberinfrastructure, a significant barrier to the democratization of science identified by the National Science Foundation. <br/><br/>The CSU system is the largest and most diverse higher education system in America.  It enrolls 460,000 students, provides 50% of all bachelor degrees in California, and 21 of the 23 CSU campuses are Hispanic-serving institutions.  The broader impact of TIDE includes democratizing GPU access, thus facilitating full participation in the AI revolution. The initiative will also help diversify the field of science by engaging ethnically diverse and first-generation college students. Additionally, TIDE is expected to attract faculty, boost research productivity, and enhance student engagement and success, amplifying CSU's core educational and research missions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232872","CC* Data Storage: Software Defined Storage for Composable and HPC Workflows","OAC","Campus Cyberinfrastructure","09/01/2022","08/29/2022","Erik Gough","IN","Purdue University","Standard Grant","Kevin Thompson","08/31/2024","$494,600.00","Elizabett Hillery, Wen-wen Tung","goughes@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","CSE","808000","9102","$0.00","High Performance Computing (HPC) resources on university campuses have traditionally provided multiple tiers of storage (scratch, project and archive) to accommodate the various I/O patterns, data sizes and retention requirements of scientific computing workflows. Traditionally, these have been provided by POSIX based storage systems and robotic tape libraries. Recent campus level adoption of cloud native technologies like Kubernetes have produced scientific computing workflows that require additional data access methods not possible via traditional methods. Campus storage offerings must evolve to support non-POSIX data access methods and provisioning of purpose-built storage devices, providing additional storage types alongside traditional POSIX based ones, for a holistic offering. <br/><br/>This project closes the gap between campus and cloud storage through the deployment of a central, shared, multi-petabyte Ceph distributed storage system that: 1) Enhances campus shared storage capabilities by establishing a model for Software Defined Storage as a Service (STaaS) at the campus level, offering unified access to and on-demand provisioning of block, object and filesystem storage across composable and HPC resources. 2) Supports Science Domains via innovative storage infrastructure, providing new data access methods between instrumentation, campus composable and HPC resources and the public cloud. 3) Enables education and workforce development by engaging with undergraduate students in deployment, operation and integration of storage infrastructure and supports cloud native applications used by researchers and experiential and residential learning programs on the Purdue University campus.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2238553","CAREER: Closed-loop Health Behavior Interventions in Multi-device Environments","CNS","Information Technology Researc, CPS-Cyber-Physical Systems","02/15/2023","02/15/2024","Shubham Jain","NY","SUNY at Stony Brook","Continuing Grant","David Corman","01/31/2028","$255,284.00","","jain@cs.stonybrook.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","CSE","164000, 791800","1045, 7918, CL10","$0.00","Motivated by the rising caregiver burden and challenges in remote health behavior monitoring, the proposed research will enable effective assistive interventions in response to dynamically changing health behaviors for target populations. To be effective and impactful, assistive mechanisms need to capture and respond to the subtle and changing context of the human. Human behaviors, however, are challenging to learn due to their complexity and the constantly changing physical, social, and environmental context. Recently, wearables have emerged to fill this gap as users are adopting a variety of devices to help them monitor health related parameters. Given their ubiquity, wearables are positioned ideally to deliver persuasive content aimed at improving users? health outcomes.  However, there is a need for a holistic approach to infer human health behaviors, even as the user's context and the devices measuring their behavior vary over time. The proposed research has the potential to transform human health outcomes by capturing and responding to fine-grained behavioral information continuously, inexpensively, and unobtrusively. This human-in-the-loop system will facilitate rapid development of Health applications by providing the foundations for using adaptive and personalized interventions for diverse health populations to enable assistive care for all.<br/><br/>The objective of this research is to develop human-in-the-loop cyber-physical systems that can model human behaviors and enable assistive interventions in sparse multi-device environments. This research will engender: (i) modeling human motion in sparse multi-device environments; (ii) learning motion-derived behavioral measures (verbal, physical, and psychological); (iii) a human-in-the-loop model that delivers interventions to the human and solicits their feedback when needed; and (iv) development and evaluation of the proposed techniques with target health populations. Our key idea is to develop novel techniques for learning coarse and fine-grained human motion in sparse multi-device environments, and infer physical, verbal, and psychological behaviors from human motion. This ultimately feeds into a human-in-the-loop CPS model to deliver the right interventions at the right time for target behavioral outcomes. The research outcomes from this work will be integrated into our comprehensive education plan and will influence pedagogy at the intersection of multiple disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2045885","CAREER: Unifying Millimeter-wave Networking and Sensing using Commodity Backscatter","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2021","07/18/2023","Parth Pathak","VA","George Mason University","Continuing Grant","Murat Torlak","09/30/2026","$391,214.00","","phpathak@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","164000, 736300","044Z, 1045","$0.00","Internet-of-Things (IoT) devices are being integrated into virtually every aspect of our daily lives with applications in logistics, supply chain, healthcare, smart cities, human-computer interaction, tracking, sensing, etc. However, providing high-speed wireless connectivity to these IoT devices remains a challenging open problem. While very high-speed wireless networks such as the next generation 802.11ad/ay WiFi and 5G cellular networks are being deployed using the high-frequency millimeter-wave (mmWave) spectrum, today?s IoT devices (such as Bluetooth and RFIDs) still primarily operate at lower frequencies (sub-6 GHz) due to their low power requirements. The aim of this project is to develop methods and tools for commodity mmWave backscattering and enable IoT devices to operate at mmWave frequencies. The proposed mmWave backscattering will enable high-speed, low-power, and low-cost mmWave wireless connectivity to millions of IoT devices.<br/><br/>The project will investigate commodity mmWave backscatter (called mmIDs) and realize a unified mmWave networking and sensing framework where (i) the mmID devices can be integrated into today?s mmWave networks for their seamless high-speed connectivity and (ii) robustness of the mmWave networks can be improved through the presence of densely deployed mmID IoT devices. The project includes four research thrusts: (1) Robust backscatter communication techniques will be developed where mmID tags can exploit the existing mmWave networking protocol messages to modulate their data; (2) Densely deployed mmWave backscatters will be leveraged for proactive blockage mitigation and mobility resilience in WLANs; (3) A high-speed commodity mmWave backscatter will be devised through symbol translation techniques and self-interference mitigation; (4) The commodity mmWave backscatter will be exploited to enable high-accuracy and low-cost sensing with applications in accessibility. The proposed techniques and protocols for mmWave backscattering will be designed, implemented, and evaluated for emerging commodity mmWave networks such as 802.11ad/ay WLANs and 5G NR. The project presents integrated research and education plan with outreach activities involving high school students from underrepresented minority groups and undergraduate students with disabilities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2144403","CAREER: Enabling Progressive Data Analytics for High Performance Computing: Algorithms and System Support","OAC","CAREER: FACULTY EARLY CAR DEV, Information Technology Researc","04/01/2022","09/12/2023","Qing Liu","NJ","New Jersey Institute of Technology","Continuing Grant","Juan Li","03/31/2027","$403,882.00","","qliu@njit.edu","323 DR MARTIN LUTHER KING JR BLV","NEWARK","NJ","071021824","9735965275","CSE","104500, 164000","026Z, 102Z, 1045","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Rapidly extracting new knowledge from simulation output is critical to the computational sciences at high performance computing (HPC) facilities across the country. However, this has become increasingly challenging due to the growing disparity between the volume of data produced by simulations and the ability to post process the data at the rate it is produced. This project aims to explore reduced representations of data with the overarching goal of achieving science aware and highly adaptable data analytics for HPC applications. The project will create new algorithms and software systems, and benefit the current and future cyberinfrastructure in the U.S. as well as numerous data intensive scientific applications, such as nuclear fusion, astrophysics, combustion, earth science, and others, thus reinforcing the competitiveness and leadership of the United States in this area. Success in the project goals will greatly reduce the time to new knowledge from scientific simulations across various science and engineering disciplines at HPC centers and significantly enhance HPC research and education. The project will contribute to society through engaging underrepresented groups and a set of integrated research and education activities.<br/><br/>The project will develop algorithms and system support centered on the idea of leveraging multilevel data representations to enable progressive data analytics on HPC systems. The proposed work fundamentally differs from conventional lossy data compression in that it can guarantee and enforce scientific constraints and augment accuracy based upon applications needs and system state. The project has integrated research and educational activities in algorithms, systems, and applications, taking into account application requirements and architecture trends in large-scale storage to advance the field of scientific data management. More specifically, the project will make contributions in several areas: 1) constraint-based data decomposition; 2) exploiting error-controlled multilevel representations for performance optimization on HPC storage systems; 3) providing a cross-layer solution to mitigate performance variation in containerized environments, with multiprocessor and multi-application coordination achieved through a probabilistic method for selecting the number of levels to retrieve; and 4) integration and evaluation on production science applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2345749","CC* Planning: Strengthening Central Michigan University's Cyberinfrastructure","OAC","Campus Cyberinfrastructure","05/01/2024","03/07/2024","Ben Andera","MI","Central Michigan University","Standard Grant","Amy Apon","04/30/2025","$99,999.00","Michael Molter","ander1b@cmich.edu","119 BOVEE UNIVERSITY CTR","MOUNT PLEASANT","MI","488583854","9897746777","CSE","808000","","$0.00","With support from this CC* Planning award, Central Michigan University (CMU) is investigating its cyberinfrastructure (CI) capabilities to identify areas of strength, weakness, and opportunity. The rapid evolution of technology and the increasing complexity of research questions necessitate proactively anticipating future needs and planning. A modern CI is essential for CMU to support innovative faculty research and enhance student learning experiences that are otherwise unattainable with the existing infrastructure. With CMU?s active 5-year strategic plan as a guide, this project aligns with institutional priorities to inspire students, foster scholarly success, and engage communities. This project enables CMU to provide increased contributions to broader scientific advancements and expand interdisciplinary collaborations. Sharing experiences and supporting institutions embarking on similar initiatives are critical aspects of the project?s aspirations.<br/><br/>This project is structured around a series of CI planning activities aligned with four strategic aims: (1) Evaluate existing CI capabilities through a comprehensive needs assessment. (2) Forge trust-based partnerships with CI professionals and regional leaders such as Michigan State University and Merit Network, Inc. (3) Develop a forward-thinking Campus CI Plan aligned with institutional goals and key science drivers. (4) Operationalize the newly developed plan by seeking future National Science Foundation (NSF) Campus Cyberinfrastructure (CC*) support. These four aims are the blueprint for enhancing CMU?s ability to drive scientific knowledge forward through a comprehensive CI.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018575","MRI: Acquisition of a High-Performance GPU Cluster for Research and Education","CNS","Major Research Instrumentation, Information Technology Researc","10/01/2020","10/20/2020","Jiayin Wang","NJ","Montclair State University","Standard Grant","Marilyn McClure","09/30/2024","$300,079.00","Chunguang Du, Avrom Trubatch","wangji@montclair.edu","1 NORMAL AVE","MONTCLAIR","NJ","070431624","9736556923","CSE","118900, 164000","1189","$0.00","Graphics Processing Unit(GPU)-acceleration) has become a useful, if not essential, tool for computational tasks in support of investigations across a wide range of disciplines, from data science to biological sciences to physical and social-science areas. Therefore, the addition of GPU-accelerated computing capacity comprising a cluster of hybrid GPU/CPU compute nodes to the HPC system, Hawk at Montclair State University provides critical support for research and education activities, across the Institution, which share the need for this capacity. The impact of each of these investigations is magnified by users? access to the GPU-accelerated computing capacity provided by this System. Moreover, by serving as a focal point for high-performance, GPU-accelerated computing, the system stimulates and supports collaboration across disciplines by investigators as well as their external collaborators. In the setting of the university, a designated Hispanic-Serving Institution with a diverse student body, PIs, as well as other major users utilize this instrument in both undergraduate and graduate education activities that enhance the STEM education for these students, including a significant proportion who are first-generation students in higher education.  The instrument serves to both attract the interest of --and provide training in leading-edge computing to-- a diverse group of students in order to inspire and prepare them to be part of the future STEM workforce by providing `hands on? access to GPU-accelerated HPC and the computing tools and techniques enabled by this platform.<br/><br/>The cluster of hybrid GPU/CPU (Computer Processing Units) compute nodes acquired under this award are integrated to the Hawk system. These nodes constitute a substantial and transformative expansion of computing capacity for research and education at Montclair State, which previously had no generally accessible GPU-accelerated HPC.  The utility of the cluster of compute nodes is significantly enhanced by leveraging the resources of the Hawk cluster. The System is configured to be accessible to not only PIs, but also to users across the Institution, as well as their external collaborators and supports multiple ongoing GPU-accelerated research activities including investigations in  (1) computing and data science, (2) biology and genomics, (3) geomorphology and land use, (4) computational mathematics, (5) applied mathematics, (6) business analytics, and (7) linguistics. These collaborations include the implementation of new techniques and approaches (e.g., machine learning and deep neural networks) to both emerging and long-standing problems in these disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232862","CC* Data Storage: KoaStore: A High Performance and Flexible Research Storage Resource","OAC","Campus Cyberinfrastructure","09/01/2022","05/03/2023","Sean Cleveland","HI","University of Hawaii","Standard Grant","Kevin Thompson","08/31/2024","$516,000.00","David Eder, Alice Koniges, Istvan Szapudi, Stanley Curtis Dodds","seanbc@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","968222247","8089567800","CSE","808000","9102, 9150, 9251","$0.00","The University of Hawai?i?s ?CC* Data Storage: KoaStore -  A High Performance and Flexible Research Storage Resource supports data and computationally intensive research, education and practice across the University of Hawai?i (UH) ten-campus system. KoaStore provides UH faculty, researchers and students state-of-the-art data infrastructure focused towards machine learning, artificial intelligence and large scale simulation. KoaStore enables a larger scope and scale of analysis by providing 4.4PB of high speed Lustre storage platform accessible from all local, regional and national compute resources. KoaStores?s 200Gb HDR infiniband network enables KoaStore to achieve I/O speeds up to 96Gb/s and leading to increased computational throughput for I/O heavy workflows on local Mana and Koa high performance computing resources.  KoaStore?s external network connections increase data transfer speeds to national, commercial cloud and academic resources via the combination of 100Gb/s data transfer node connection and high speed parallel file system, enhancing end-to-end big data workflows. The strategic partnership with the Open Science Data Federation allows for KoaStore to serve the national research community as well as provide a gateway to a compute platform UH researchers can access their data on with national computing resources.<br/><br/>Integration of KoaStore?s high speed file system with the regional Jestream2 NSF Cloud infrastructure hosted at the University of Hawaii allows researchers to easily span computing environments and modalities between the cloud and local computing resources to support new deep learning and artificial intelligence workflows, visualizations and applications. Further, the KoaStore platform aids hands-on training in data science and computational science for the next generation of researchers and data scientists through partnership with the Hawaii Data science institute and local community for workshops and classroom access.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1763423","SHF: Medium: Fairness in Software Systems","CCF","Information Technology Researc, Special Projects - CCF, Software & Hardware Foundation","09/15/2018","05/28/2021","Yuriy Brun","MA","University of Massachusetts Amherst","Continuing Grant","Sol Greenspan","08/31/2024","$1,107,329.00","Alexandra Meliou","brun@cs.umass.edu","101 COMMONWEALTH AVE","AMHERST","MA","010039252","4135450698","CSE","164000, 287800, 779800","1640, 2878, 7924, 7944, CL10","$0.00","Software impacts society in many ways and increasingly automates decision-making. For example, software transcribes videos, translates documents, selects what news articles are promoted, and determines who gets a loan or gets hired. It is possible for software to exhibit bias in its operation, whether or not it is intended by the customers or developers of the software. For example, software might be more accurate at transcribing male voices than female ones. Or software may inject societal stereotypes into automated translations, and risk-assessment computations may exhibit racial bias. As more societal functions operate in cyberspace, the importance of software fairness increases. In these settings, data-driven software has the ability to shape human behavior: it affects the products we view and purchase, the news articles we read, the social interactions we engage in, and, ultimately, the opinions we form. Biases in data and software risk forming, propagating, and perpetuating biases in society. This project develops theory, techniques and tools to enable software designers and engineers to describe fairness requirements, test the software for fairness properties, and debug fairness defects. The outcomes of this project will help increase the society's trust in software decisions and in the data the software uses, in turn, increasing potential impact and benefits the software can bring to society.<br/><br/>The project addresses scientific questions behind efficiently and effectively measuring potential bias and helping stakeholders make informed decisions about software. It is not the project's aim to devise policies or eliminate bias in software. Instead, the aim is to provide software testing tools and measures that can be validated for formally specified software fairness properties. To measure bias, the project develops a novel approach for measuring causal relationships between program inputs and outputs. Software testing enables conducting causal experiments consisting of running the software with nearly identical inputs that vary only in a key input characteristic under test. Variations in an input characteristic that affect execution behavior provide evidence of a causal relationship. The project identifies when causal relationships are appropriate for measuring potential bias, develops efficient testing methods for measuring these relationships, and creates tools and techniques to help engineers identify and modify the causes of these relationships.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1845893","CAREER: Systematic Software Testing for Deep Learning Applications","CCF","Information Technology Researc, Software & Hardware Foundation","05/01/2019","06/02/2023","Baishakhi Ray","NY","Columbia University","Continuing Grant","Sol Greenspan","04/30/2025","$555,065.00","","rayb@cs.columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","CSE","164000, 779800","1045, 7944, 9102, CL10","$0.00","A paradigm shift is underway in software development, where decision making is increasingly shifting from hand-coded program logic to a reliance on Deep Learning (DL) --- popular applications of Speech Processing, Image Recognition, Robotics, etc.  are using DL to implement their core components. Deep Neural Networks (DNNs), a widely used form of DL, is a key behind much of this progress. With such spectacular growth in traditional applications, DNNs and other DL technologies are also increasingly being used in safety-critical systems such as autonomous cars, medical diagnosis, malware detection, and aircraft collision avoidance systems. Such a wide adoption of DL techniques carries with it concerns about the reliability of these systems, as several high-profile instances of DL-based behavior have already been reported. Thus, it has become crucial to rigorously test these applications with realistic corner cases to ensure high reliability. However, due to the fundamental architectural differences between DL implementations such as DNNs and traditional software, existing software testing techniques do not apply to them in any obvious way. In fact, companies like Google, Tesla, etc. are increasingly confronting software testing challenges to ensure reliable and safe DL applications.  Therefore,  systematically testing DL-based software systems will be a significant step towards increasing safety and reliability of sensitive and safety-critical DL systems.<br/><br/>This project will design, implement, and evaluate a novel software testing framework to assess the reliability of the Deep Learning applications and detect buggy behaviors during the application development and maintenance phase. In particular, the proposed framework will develop novel white-box testing strategies, realistic test-case generation techniques, and regression testing techniques to assess DL applications. A unique characteristic of the DL-based programming paradigm is that the end applications highly depend on the training data. Therefore, the research will build novel white-box testing strategies to evaluate both the model and the training data together as a whole system. In addition, this research will design and deploy techniques to generate new test cases that capture the real-world corner-case behavior where the DL applications may fail. The project will also investigate how any changes in data or model architecture can impact a pre-trained model in order to guide regression test case selection and prioritization process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232817","CC* Data Storage: Distributed, Fast, Scalable Infrastructure for Emerging Media Research Data","OAC","Campus Cyberinfrastructure","09/01/2022","08/29/2022","Qi Sun","NY","New York University","Standard Grant","Kevin Thompson","08/31/2024","$499,907.00","Robert Pahle","qisun@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","CSE","808000","","$0.00","The project develops a fast, scalable, distributed cyberinfrastructure (CI) across New York University?s New York City buildings to meet the specialized demands of laboratory and classroom media researchers, including: 1) large-scale storage; 2) fast, low latency, two-way edge-server communications; and 3) scalable system design to rapidly adapt to emerging research frameworks, such as virtual/augmented reality (VR/AR), machine learning, and data science. The project architecture is based on research with colleagues operating the large Flatiron Institute Ceph clusters. The CI fosters and supports data-driven, high concurrency, and collaborative research and educational activities. With VR/AR as an example use case, the CI generates a new service to provide an immediate enabling impact on real-time cloud-edge data transmission and storage, relaxing the computational load on low-power untethered headsets. The high concurrency data volume support enables remote virtual environment co-development by synchronously sharing data and models. <br/><br/>The strategic CI design supports other broad emerging research disciplines, including robotics, educational technologies, biomedical experimentation, and wireless networks. To broaden participation, the CI is accessible via the New York State Education and Research Network, as well as to other NYU collaborating K-12 educational institutions and government agencies. The technology helps to support the training and education of students from low-income households and backgrounds underrepresented in STEM. Through the integration of ultra-large and instantaneous public and citizen data analysis, it also supports just-in-time data-driven urban policy-making and public emergency response.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019129","MRI: Acquisition of FASTER - Fostering Accelerated Sciences Transformation Education and Research","OAC","Major Research Instrumentation, Information Technology Researc, CYBERINFRASTRUCTURE","09/01/2020","09/07/2022","Honggao Liu","TX","Texas A&M University","Standard Grant","Alejandro Suarez","08/31/2024","$3,090,000.00","Theodora Chaspari, Zhe Zhang, Raymundo Arroyave, Dilma Da Silva, Zhangyang Wang","honggao@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","CSE","118900, 164000, 723100","075Z, 1189","$0.00","The project funds the acquisition of a composable high-performance data-analysis and computing instrument, named FASTER (Fostering Accelerated Scientific Transformations, Education, and Research). FASTER will enable transformative advances in scientific fields that rely on artificial intelligence and machine learning (AI/ML) techniques, big data practices, and high-performance computing (HPC) technologies. The FASTER platform removes significant bottlenecks in research computing by leveraging a technology that can dynamically allocate resources to support workflows. It will support researchers from across the Texas A&M University System and their collaborating institutions.  Thirty percent of FASTER?s computing resources will also be allocated to researchers nationwide by the National Science Foundation (NSF) XSEDE (Extreme Science and Engineering Discovery Environment) program. FASTER?s composable interface allows it to simultaneously support both emerging and traditional workloads in research computing. Transformative research projects benefiting from FASTER will include the development of AI/ML models, cybersecurity, health population informatics, genomics, bioinformatics, computer-aided drug design, agricultural sciences, life sciences, oil and gas simulations, de novo materials design, climate modeling, multi-scale simulations, quantum computing architectures, biomedical imaging, geosciences, and quantum chemistry. In addition to supporting a wide-range of fields of research, the project contributes to code development, education, and the workforce development goals of several NSF Big Ideas.<br/><br/>FASTER adopts the innovative Liqid composable software-hardware approach combined with cutting-edge technologies such as state of the art CPUs and GPUs, NVMe (Non-Volatile Memory Express) based storage, and thigh speed interconnect. Workflows on FASTER will be able to dynamically integrate disaggregated GPUs and NVMe to compose a single node, allowing them to scale beyond traditional hardware limits. The composable and configurable techniques will allow researchers to use resources efficiently, enabling more science. Best practices gathered from managing the resource will be shared with the community. FASTER will coordinate a three-pronged effort to effectively broaden participation in computing by focusing on training, education and outreach. FASTER will leverage existing efforts that promote STEM (Science, Technology, Engineering and Mathematics) and broaden participation in computing at the K-12, collegiate, and professional levels to have a transformative impact nationally. FASTER activities are designed to expand the participation of traditionally underrepresented groups in computing and STEM, particularly at minority-serving institutions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2300842","CISE-RV: Computing Community Consortium","CCF","Information Technology Researc","01/15/2024","03/06/2024","Daniel Lopresti","DC","Computing Research Association","Cooperative Agreement","Mitra Basu","12/31/2027","$2,499,488.00","Nadya Bliss, Tracy Camp, Mary Lou Maher, Daniel Lopresti","lopresti@cse.lehigh.edu","1828 L ST NW","WASHINGTON","DC","200365104","2022662949","CSE","164000","1640","$0.00","The Computing Community Consortium (CCC) is established through a Cooperative Agreement between the National Science Foundation (NSF) and the Computing Research Association (CRA). Since its inception, the purpose of the CCC has been to catalyze the development of bold, far-reaching visions for computing research, and to facilitate the communication of those research visions to stakeholders both within and beyond the computing research ecosystem. The CCC fosters proactive engagement with both computing and non-computing research stakeholders -- agencies, industry, professional societies, consortia, and philanthropic stakeholders -- to ensure that it continues to expand its network and impact.<br/><br/>The CCC provides the continuity and capacity to bring together stakeholders that depend on computing innovation as part of increasingly diverse and multi-disciplinary national and societal needs. The CCC plays a vital and effective role in catalyzing the computing research community to articulate far reaching and strategic research visions that will continue to shape the field for decades to come. The majority of CCC visioning activities and white papers bring together multidisciplinary expertise and needs that combine to create important disciplinary and interdisciplinary challenges for the computing field. The results of these activities are disseminated back to the community to ensure continued revitalization and advancement of new areas and topics for future computing research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335910","Collaborative Research: NSF Workshop on Automated, Programmable and Self Driving Labs","CNS","Information Technology Researc","10/01/2023","08/17/2023","Ian Foster","IL","University of Chicago","Standard Grant","Xiaogang (Cliff)  Wang","09/30/2024","$12,047.00","","foster@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","164000","025Z, 7556","$0.00","Laboratory automation increases precision and efficiency of science experiments. Advances in low-cost sensors, actuators, robotic systems, and control systems have lowered the barrier to entry to laboratory automation such that fully self-driving labs will have the potential to enable new practices of science experiment and to accelerate scientific exploration progress. There is a critical need to develop the principles and methodologies for self-driving laboratories. These systems will likely draw from best practices and experiences learned in data science, human-machine interaction, manufacturing and quality control, open-source ecosystems, and laboratory science methods. <br/><br/>The proposed workshop will convene leaders in self-driving laboratories and related areas including data science, robotics, manufacturing, and open-source ecosystems to define a roadmap for self-driving laboratories. Experts will discuss on latest advances and current challenges in automated sample preparation, experiment generation, data collection, and data analysis. The workshop will help identify major themes and assess on how future, interconnected goals can be best supported in a research context. By convening community leader around related topics, the workshop will seed cross-discipline collaboration on infrastructure that can support a broad range of sciences, which may include more complex experiments to accelerate scientific discovery and perform cost effective verification and validation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232628","CC* Regional Computing: Helping Our Researchers Upgrade their Science (HORUS)","OAC","Campus Cyberinfrastructure","09/01/2022","08/29/2022","Shawn McKee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Amy Apon","08/31/2024","$1,000,000.00","Robert Thompson, Robert Stovall","smckee@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","CSE","808000","","$0.00","The HORUS project combines existing large-scale scientific data storage from the previously funded NSF OSiRIS project with a broad range of computational resources to enable accelerated scientific discovery for universities, colleges and community colleges in Michigan and the surrounding regions.   The project focuses on providing easy access to diverse computing and storage resources, both of which are required for scientific research and analysis.  This will be especially beneficial for researchers at smaller institutions, enabling them to more effectively contribute to society by  expediting their research.  In addition, through its connection to OSG and the PATh project, it will serve as an on-ramp to even larger scale resources across the nation when that is needed.<br/><br/>The project establishes a set of compute servers to complement the existing OSIRIS storage system attached to the MERIT statewide research and education network in Michigan. The system consists of 4 GPU nodes (each with two A100 GPUs), 10 Large Memory compute nodes (each with two AMD Epyc 7F72 3.2 GHz 24 core processors) and 10 compute nodes (each with 2 AMD Epyc 7H12 2.6GHz 64 core processors). This resource and associated services provide computational services to a suite of universities including University of Michigan, Wayne State University, Oakland University, Eastern Michigan University, and Oakland Community College.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2212345","Collaborative Research: SHF: Medium: Automated energy-efficient sensor data winnowing using native analog processing","CCF","Information Technology Researc","10/01/2022","07/21/2022","Sachin Sapatnekar","MN","University of Minnesota-Twin Cities","Continuing Grant","Sankar Basu","09/30/2026","$436,410.00","Ramesh Harjani","sachin@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","CSE","164000","1640, 7924, 7945","$0.00","As computing becomes pervasive in all aspects of daily life, computing hardware must allow for increasing interaction with the physical world. This interaction takes the form of sensed signals that are analog in nature, e.g., a sensor may output a voltage that can take on a continuous range of values. Traditional mainstream computing digitizes this data, converting it to digital 0s and 1s for efficient analysis and processing. However, as the amount of sensed analog data is growing exponentially, digital processors will be faced with a data deluge from external sensors.  For these vast volumes of data, even the cost of converting analog input data to digital signals, prior to any processing, can be prohibitively expensive.  Native analog processing (NAP) negates the need for analog-to-digital conversion by working in the analog domain. NAP can be used to implement data processing functions inexpensively, but can achieve only limited accuracy; on the other hand, digital processing can achieve high accuracy, but requires the overhead of analog-to-digital conversion. This project presents a methodology for mixed-signal processing that hybridizes digital and analog subcircuit implementations to achieve the best of both worlds. The effort intends to actively engage with the semiconductor industry, and will train graduate and undergraduate students in the area of semiconductor design, thus alleviating the national skills shortage in this area.<br/><br/>In the first step, computing tasks are automatically partitioned into hybrid analog/digital segments, with the goal of meeting system-level constraints on throughput, power, and noise/error. The computations associated with a task are abstractly represented by a dataflow graph (DFG). This representation is widely used to model a variety of tasks, including those commonly used in digital signal processing and machine learning. The nodes in the DFG are mapped to analog or digital implementations, using cost functions that represent the cost of implementation, as well as the cost of any required analog-to-digital or digital-to-analog conversion. Next, the analog and digital circuitry is optimized to build a silicon implementation at the layout level, based on cutting-edge transistor technologies, which involve restrictive design rules that impose limitations such as unidirectional routing and gridded layout. The optimizations are facilitated by novel techniques for back-end analysis, synthesis, and implementation developed in this project, including transistor and interconnect optimizations, placement and routing techniques that are specifically targeted to mixed-signal designs, and compact performance machine-learning-based model generation that efficiently predicts circuit performance. The project thus automatically translates the system-level DFG specification of a computing task to an optimized mixed-signal silicon implementation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2346636","Research Infrastructure: CC* Data Storage: Foundational Campus Research Storage for Digital Transformation","OAC","Campus Cyberinfrastructure","03/15/2024","03/05/2024","Michael Kennedy","CA","University of California-Riverside","Standard Grant","Amy Apon","02/28/2026","$498,553.00","Matthew Gunkel, Charles Forsyth, Matthew Barth, David Lo","mikek@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","CSE","808000","","$0.00","The University of California, Riverside (UCR) is engaged on a transformative project to revitalize its research storage infrastructure with a Ceph-based solution. This aligns with UCR's recent ascension to the Association of American Universities (AAU) in July 2023 and commitment to supporting its diverse research community as a distinguished Hispanic-Serving Institution (HSI). Modernizing research storage is crucial for maintaining AAU standing as one of only four HSIs in this elite group, and empowering UCR to serve the needs of its students and the Inland region. The enhanced infrastructure will catalyze innovation and interdisciplinary collaboration across campus.<br/><br/>The project establishes a state-of-the-art, integrated storage system built upon Ceph's capabilities. The initial system will consist of 20 nodes, each with 160TB of physical storage across 8 spinning disks. Additionally, there will be another three nonvolatile memory express (NVMe) solid-state drives, providing 23TB of high input/output operations per second (IOPS) storage per node for a total of 460TB, yielding a resilient, scalable, and high-performance environment supporting diverse research endeavors. Integration with the National Research Platform's Nautilus Cluster furthers UCR's national impact, propelling advancements in data-intensive fields. This initiative will fuel groundbreaking discoveries in several scientific areas, including Deep Learning, metabolomics, critical air challenges, big data problems in agriculture and hydrology, honeybee research, neurocognitive imaging in aging, and more.  The storage system will advance multidisciplinary projects, and position UCR as a front runner in inclusive, cutting-edge research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2228982","Collaborative Research: HCC: Small: Toolkits for Creating Interaction-powered Energy-aware Computing Systems","IIS","Information Technology Researc, HCC-Human-Centered Computing","01/01/2023","03/17/2023","Yang Zhang","CA","University of California-Los Angeles","Standard Grant","Dan Cosley","12/31/2025","$315,000.00","","yangzhang@ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244200","3107940102","CSE","164000, 736700","7367, 7923, 9251","$0.00","The explosion in the number of ?smart? computing devices has led to the need to design power solutions that reduce their ecological footprint and electronic waste. One possible answer is ?self-sustaining? systems that harvest power from electromagnetic, solar, and other sources, reducing the need for batteries and external charging. This project looks at a novel kind of self-sustaining power source, one where systems draw energy from how people use them, for instance through button clicks or motion of the device. These user interactions are a promising source of power, but require careful management on the part of device designers since both the timing and types of interactions can be unpredictable and must be designed primarily to meet users? needs rather than power goals. The research team seeks to deepen the understanding of information and resource needs of novice designers of self-sustaining systems and to create tools that help developers manage those needs. This in turn will improve sustainability in the design of computing systems, with the goal of developing power sources and development methods that generalize to a wide set of device design contexts. <br/><br/>The project is structured around three research thrusts. The first is to better understand user needs, through a combination of observations of novices and experts programming self-sustaining systems, interviews, and surveys. The second is to create modules to harvest energy from interactions, providing developers with profiling tools and composable mechanisms enabled by a generic backbone system. The third is to develop toolkits that link the first two thrusts: supporting the development, deployment, and assessment of self-sustaining computing systems by providing guidance to developers on harvester selection, mechanism design, energy profiling, debugging, and beyond. These three research thrusts will be complemented by a comprehensive evaluation with a series of technical validations and user studies. This project will adopt common qualitative and quantitative evaluation techniques including A/B tests, Likert-scale questionnaires, semi-structured interviews, experimenters? observations, walk-through demonstrations, evaluation through demonstrations, and pair programming. Overall, this research effort will guide the creation of tools to support future developments of self-sustaining computing systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2146492","CAREER: Coresets for Robust and Efficient Machine Learning","IIS","Information Technology Researc, Robust Intelligence","10/01/2022","04/21/2023","Baharan Mirzasoleimanbarzi","CA","University of California-Los Angeles","Standard Grant","Vladimir Pavlovic","09/30/2027","$543,447.00","","baharan@cs.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244200","3107940102","CSE","164000, 749500","1045, 1640, 7495, 9251","$0.00","Large datasets have enabled modern machine learning models to achieve unprecedented success in various applications, ranging from medical diagnostics to urban planning and autonomous driving, to name a few. However, learning from massive data is contingent on exceptionally large and expensive computational resources. Such infrastructures consume substantial energy, produce a massive amount of carbon footprint, and often soon become obsolete and turn into e-waste. While there has been a persistent effort to improve the performance and reliability of machine learning models, their sustainability is often neglected. This project aims to address sustainability, reliability, and efficiency of machine learning, by selecting the most relevant data for training. The resulting algorithms will be broadly applicable for learning from massive datasets across a wide range of applications, such as medical diagnosis and environment sensing. The outcomes of this research will be incorporated into curriculum development, to train a new generation of machine learning and data mining practitioners.<br/><br/>The main objective of this project is to develop a new generation of theoretically rigorous methods that enable {efficient and robust learning from massive datasets. To achieve this goal, this project will develop scalable combinatorial optimization algorithms to extract weighted subsets (coresets) of data that guarantee similar training dynamics to that of training on the full data. This enables sustainable, efficient, and accurate learning from massive data. As datasets grow larger, maintaining their quality becomes very expensive. Hence, mislabeled and malicious examples become ubiquitous in large datasets. To ensure reliability in addition to sustainability and efficiency, the developed techniques will be leveraged to extract coresets of data points that enable provably robust learning against noisy labels and adversarial attacks. This project will also seek to learn better objectives to automatically extract the most valuable data for efficient and robust learning from massive data. Finally, this research will enable efficient and robust learning frameworks that can be applied to many real-world applications through interdisciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232816","CC* Data Storage: High Volume Data Storage Infrastructure for Scientific Research and Education at Kennesaw State University Shared as Open Science Data Federation Data Origin","OAC","Campus Cyberinfrastructure","09/01/2022","08/26/2022","Ramazan Aygun","GA","Kennesaw State University Research and Service Foundation","Standard Grant","Kevin Thompson","08/31/2024","$500,000.00","","raygun@kennesaw.edu","1000 CHASTAIN RD NW","KENNESAW","GA","301445588","4705786381","CSE","808000","","$0.00","This project develops a high-capacity storage system with around 4.3PB overall usable storage as an Open Science Data Federation (OSDF) data origin to support scientific research and education activities on both campuses of Kennesaw State University (KSU). 30% of the storage is to be allocated for hosting datasets of external researchers and sharing KSU spawned datasets to empower national research projects.<br/><br/>The project enables a storage platform for KSU?s researchers and students to maintain and share their datasets thriving their collaborations without worrying about deleting files, transferring to alternate storage devices, or mailing hard drives while augmenting KSU?s research expertise into the national data ecosystem. This storage architecture cultivates twenty-four science, technology, engineering, and mathematics projects at KSU in a broad range of disciplines. The project also supports new Ph.D. programs in Computer Science, Interdisciplinary Engineering in addition to the first Ph.D. program in USA on Analytics and Data Science along with 20 master?s degrees in areas including civil engineering, mechanical engineering, chemical sciences, computer science, and cybersecurity. To facilitate the storage system effectively, KSU?s Center for Research Computing organizes workshops for their faculty and students on utilizing national open science resources and findable, accessible, interoperable, and reusable (FAIR) principles.<br/><br/>This effort will enable Kennesaw State University to participate in national research and discoveries by connecting it to OSDF. This integration into an open science network will present national technical services as KSU develops research computing and data expertise while cyberinfrastructure professionals at KSU adapt the state-of-the-art storage technologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2118453","NSF INCLUDES Alliance: The Alliance for Identity-Inclusive Computing Education (AIICE): A Collective Impact Approach to Broadening Participation in Computing","EES","Eddie Bernice Johnson INCLUDES, CSforAll-Computer Sci for All, Information Technology Researc, Special Projects - CNS, Special Projects - CCF, IIS Special Projects","08/01/2021","02/20/2024","Alicia Washington","NC","Duke University","Cooperative Agreement","Subrata Acharya","07/31/2027","$6,233,345.00","Shaundra Daily","nicki@cs.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","EDU","032Y00, 134Y00, 164000, 171400, 287800, 748400","031Z, 1714","$0.00","While nationwide enrollment in undergraduate computing programs continues to increase, computer science (CS) is still overwhelmingly dominated by white and Asian, able-bodied, middle to upper class, cisgender men. Effects of this lack of diversity are evident in academic/workplace cultures and biased/harmful technologies (e.g., facial recognition, predictive policing, public services decisions, healthcare, and financial software) that negatively impact and exclude non-dominant identities. Despite this, identity (as defined in social science) is rarely, if ever, included in CS curricula, pedagogy, research, and policies. As computing becomes more ubiquitous, it is imperative that technology creators from a diverse range of identities are in development and leadership positions to ensure that harmful technologies are avoided. This requires creating academic cultures in computing that emphasize the importance of identity, its societal impacts, and the impacts of technology on people from non-dominant identities. The Alliance for Identity-Inclusive Computing Education (AIICE) aims to increase the entry, retention, and course/degree completion rates of high-school and undergraduate students from groups that are historically underrepresented in computing through evidence-based, identity-inclusive interventions. AIICE?s collective impact approach to broadening participation convenes national leaders in K-16 CS education to transform high-school and postsecondary CS education using innovative strategies that target the people (educators), policies [state (K-12) and institutional (postsecondary) policies, as well as postsecondary accreditation criteria], and practices (classroom/department cultures) that directly impact student entry, retention, and course/degree completion.<br/><br/>AIICE is founded upon evidence that student-focused (and often deficit-based) strategies do not adequately address institutional cultures, policies, and practices that have marginalized people from non-dominant identities. AIICE will collectively create systemic change by blending aspects of social science with CS to 1) increase CS student and educator knowledge and use of identity and related topics, 2) support CS educators and leaders in fostering academic cultures that are more inclusive of non-dominant identities, and 3) increase K-16 policy-driven changes to CS education that infuse identity-inclusive strategies. The Alliance leverages the constellation model for assembling diverse partners to solve complex and pressing social problems through a common agenda, shared measurement system, mutually reinforcing activities, continuous communication, and dedicated backbone support. Each Alliance domain (training, curricula & pedagogy, research, and policy) represents a permeable, action-focused working group (constellation) comprised of various activities at the high-school and postsecondary levels. This flexible structure supports governance and communication that leverages the strengths of each organization and allows for several ways to join, support, and scale the Alliance at the individual, department, and organization levels. Successful implementation of the Alliance will directly impact a total of 7,000 high-school CS teachers, 2,000 postsecondary CS faculty/staff, 5,000 teaching assistants, and 500 U.S. computing departments. This will, in turn, impact a total of 525,000 high-school and 35,000 undergraduate CS students nationwide. AIICE will also directly impact industry diversity, equity, and inclusion efforts, as better-trained graduates will enter technical positions more aware of identity-related issues during development, properly advocate for and implement change, and decrease the development of harmful technologies. This effort will also increase the number of computing departments implementing identity-related interventions, as well as create new annual data and reports on identity-inclusive computing (including a repository of resources and best practices).<br/><br/>This NSF INCLUDES Alliance is funded by NSF Inclusion across the Nation of Communities of Learners of Underrepresented Discoverers in Engineering and Science (NSF INCLUDES), a comprehensive national initiative to enhance U.S. leadership in discoveries and innovations by focusing on diversity, inclusion and broadening participation in STEM at scale, with co-funding from the CS for All program and the Directorate for Computer and Information Science and Engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2233894","CNS Core: Small: Repurposing Smartphones to Minimize Carbon","CNS","Information Technology Researc, Special Projects - CNS","01/01/2023","03/03/2024","Patrick Pannuto","CA","University of California-San Diego","Standard Grant","Jason Hallstrom","06/30/2026","$632,000.00","Ryan Kastner","ppannuto@eng.ucsd.edu","9500 GILMAN DR","LA JOLLA","CA","920930021","8585344896","CSE","164000, 171400","7923, 9251","$0.00","More than a billion smartphones are sold each year and most are decommissioned less than two years later. The majority of these unwanted smartphones are neither discarded nor recycled, but languish in junk drawers and storage units. This computational stockpile represents a huge wasted potential; these smartphones have a high-performance and energy-efficient processor, extensive networking capabilities, and a reliable built-in power supply. This project gives a second life to older, discarded, idle smartphones.  The research addresses key technical questions of how to transform a user-optimized, interactive device into a robust, reliable device capable of long-term, unattended operation.  It develops new metrics to capture both manufacturing and operational carbon costs, couples these with  economic models, and establishes a roadmap for the best opportunities for old electronic devices.  Finally, the project tests these ideas at scale, to empirically establish how best to use phones-as-compute and phones-as-sensors.<br/><br/>Repurposing smartphones to extend their lifetime is a key facet of reducing their carbon footprint.  Integrated circuit manufacturing is an extremely carbon-intensive process and dominates the overall carbon cost for smartphones.  Repurposing extends smartphone lifetimes to better amortize their manufacturing carbon costs.  A repurposed phone can displace the manufacture of a new electronic device and lower overall carbon consumption.  This project trains undergraduate and graduate students to build sustainable computing systems out of old, unwanted smartphones.<br/><br/>This project is funded by Design for Sustainable Computing (NSF 22-060)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319410","Collaborative Research: IMR:MM-1B: Privacy in Internet Measurements Applied To WAN and Telematics","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","07/28/2023","Christos Papadopoulos","TN","University of Memphis","Continuing Grant","Deepankar Medhi","03/31/2026","$91,208.00","","christos.papadopoulos@memphis.edu","115 JOHN WILDER TOWER","MEMPHIS","TN","381520001","9016783251","CSE","164000, 736300","115Z, 7363","$0.00","The PIMAWAT (Privacy in Internet Measurements Applied to WAN And Telematics) project will demonstrate new methods to provide data networking datasets that respect end-user privacy, while still being able to support new research in network protocols, security, privacy, and machine learning. The main insight is that *most data today sent over the wide-area network (WAN) is encrypted*; thus, the challenge is to demonstrate what data is encrypted, detect and scrub any remaining leaks, and finally anonymize the metadata (who talks to whom) before sharing data.<br/><br/>The intellectual merit of PIMAWAT will be to develop new methods to anonymize network traffic at scale, then use those new algorithms to evaluate potential data leakage, and demonstrate that real-world data sources can be scrubbed for sharing while respecting privacy.  PIMAWAT plans to focus the investigator?s prior work on wide-area network data traffic. As possible, it will also explore vehicle telematics as a recently developing dataset that poses unique privacy opportunities and challenges, with a device (not person) focus, yet with geolocation and application details.<br/><br/>The broader impacts of PIMAWAT will be to democratize the potential to collect and share network data through new tools and best-practices for privacy-respecting data scrubbing.  Data from this project will enable new approaches in computer science for protocol design and cyber-security, applying AI and machine learning, and will provide early results in the rapidly evolving field of vehicle telematics. PIMAWAT will provide new tools, data, and practices, and encourage use of these methods by other researchers, in classrooms, and by industry.<br/><br/>The PIMAWAT project website will be https://ant.isi.edu/pimawat and its tools and datasets will be provided through https://ant.isi.edu/datasets/ as they are developed during project and after it completes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126253","CC* Compute: RAPTOR - Reconfigurable Advanced Platform for Transdisciplinary Open Research","OAC","Campus Cyberinfrastructure","10/01/2021","04/07/2023","Jason Liu","FL","Florida International University","Standard Grant","Kevin Thompson","09/30/2024","$416,000.00","Yuepeng Li, Julio Ibarra, Jayantha Obeysekera, Keqi Zhang, Fahad Saeed, Cassian D'Cunha","liux@cis.fiu.edu","11200 SW 8TH ST","MIAMI","FL","331992516","3053482494","CSE","808000","9102, 9251","$0.00","Building resilient, sustainable, livable, and environmentally safe dynamic systems (natural or human built) requires on-demand computing resources, facilitating machine learning, data processing, and data analytics. These systems rely on computation to support simulation, modeling, and analyses to enable discovery, facilitate understanding, and make decisions. This project implements RAPTOR (Reconfigurable Advanced Platform for Transdisciplinary Open Research), a reconfigurable compute environment to address dynamic and diverse computing needs of science drivers??coastal resilience, sustainable environmental research, and systems biology.<br/><br/>The goal of RAPTOR is to increase research production by enhancing computing capabilities at Florida International University (FIU), both at the campus level and through participation in a resource-sharing federated distributed computing community. For that, RAPTOR integrates with the Chameleon Cloud Infrastructure for on-demand resource allocation and the Open Science Grid (OSG) for opportunistic preemptible resource sharing allocation. The result is a platform capable of connecting with a rapidly deployable sampling system that can assimilate and transmit data in real-time to facilitate actionable intelligence and drive adaptive environmental monitoring decisions, and respond as conditions change. The integration of knowledge, tools, and modes of computation proposed by RAPTOR builds upon the expertise of domain researchers, computer scientists, and IT practitioners towards a ?smart cyberinfrastructure? to foster and develop transdisciplinary approaches. RAPTOR not only introduces new computing modalities at FIU to support its researchers, but also benefits a broader federated community of researchers and scholars sharing resources through OSG and Chameleon Cloud.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2203189","EAGER: CAS-Climate: Research Coordination Network on the Indirect Environmental/Energy Impacts of Digitalization","CBET","Information Technology Researc, Info Integration & Informatics, EnvS-Environmtl Sustainability","01/01/2022","10/11/2022","Kasantha Moodley","DC","Environmental Law Institute","Standard Grant","Bruce Hamilton","12/31/2024","$299,959.00","","moodley@eli.org","1730 M ST NW","WASHINGTON","DC","200364553","2029393800","ENG","164000, 736400, 764300","090Z, 7916","$0.00","This project is designed to build a community of researchers with a focus on improving society's understanding of the indirect energy and environmental (E&E) impacts of the digital economy. There is a slowly increasing body of research that seeks to understand and quantify the direct E&E impacts of digitalization, including the increase or reduction of energy use, emissions arising from the use of devices (and/or their supporting infrastructure), and environmental impacts from operating infrastructure such as data centers. However, far more complicated are estimates of digitalization's indirect E&E impacts. Indirect impacts may include those associated with the substitution of digital for physical products, or rebound effects that occur if savings (in time or money) from digitally-mediated behaviors induce energy-intensive activities (like short term rental platforms increasing long distance travel). Ensuring the long-term sustainability of digital infrastructure and the human behaviors it enables will require greater research efforts at the intersection of multiple and often disparate disciplines along with increased scientific cooperation across national borders. This RCN seeks to close disciplinary and knowledge gaps, increase the quality and quantity of research, and begin to grow a new field. <br/><br/>Project goals are to: increase needed multi-disciplinary interactions, especially between the physical and behavioral sciences; facilitate greater collaboration between U.S. and European researchers; identify knowledge gaps and high-value research; improve scholarly impact; and advance work to secure greater research support. By fostering collaboration between disciplines and providing a better sense of research priorities, a research coordination network can be instrumental in leveraging funds from existing and new sources to grow support of research in the topic area.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019131","MRI: Development of a Miniaturized Molecular Beam Epitaxy Setup for Direct Printing of Quantum Circuits","CNS","Major Research Instrumentation, Information Technology Researc","10/01/2020","09/07/2022","Shuolong Yang","IL","University of Chicago","Standard Grant","Marilyn McClure","09/30/2024","$447,086.00","David Schuster, Tian Zhong, Supratik Guha","yangsl@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","118900, 164000","1189","$0.00","This Major Research Instrumentation (MRI) grant will support the development of a nanoscale deposition tool ? a miniaturized molecular beam epitaxy (MBE) system ? for direct fabrication of nanoscale structures for quantum applications. As the era of quantum engineering emerges, there is a pressing need to rapidly design, prototype, and test nanoscale structures for quantum information processing. This is critical not only to the investigation of fundamental questions in quantum information sciences, but also to the development of quantum sensors, quantum computing, and quantum networking. Current technologies separate material synthesis and nanoscale engineering.  To enable more rapid development, researchers will pursue an innovative design based on nanoscale scanning probe technologies and develop a deposition tool that combines advanced synthesis and nanofabrication in one step. This instrument will have the following impacts: i) facilitate rapid development of quantum technologies; ii) develop a versatile tool to serve the broad quantum research and engineering community at the University of Chicago; iii) promote education and outreach at the graduate, undergraduate, and high school level. Outreach programs will engage the surrounding community in the Chicago South area. The investigators will allocate 40% of machine usage to the broader quantum research community at the University of Chicago, which will serve as an intellectual hub for training future quantum scientists. <br/><br/>The team includes experts on nanoscale materials science, microelectronics, quantum sensing, quantum computing, and quantum information.  It has the goal of developing a miniaturized MBE system. Traditional MBE is a state-of-the-art deposition tool to produce wafer-scale thin films; nanofabrication such as photo- or electron-beam-lithography is used as the second step to engineer the nanoscale structures and devices. Supported by this MRI grant, the investigators shrink the lateral scale of deposition to tens of nanometers. This is achieved by customizing a scanning probe into a nano nozzle, which is integrated into a commercial scanning tunneling microscopy system. This new tool is developed in a dedicated laboratory space at the University of Chicago, with established microscopy and spectroscopy tools available to characterize the structures fabricated by the miniaturized MBE system. This instrument will enable research in a number of new scientific areas including but not limited to: 1) two-dimensional superconducting quantum devices based on nanoscale superconductor deposition; 2) delta doping of rare earth elements in oxide materials to host individual to few-body entangled quantum bits; 3) hybrid quantum sensors composed of rare-earth oxide islands and nitrogen-vacancy centers. This will be a field-defining development which aims at expediting the investigation of fundamental quantum phenomena and quantum technology developments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2233871","Collaborative Research: EAGER: Towards a Design Methodology for Software-Driven Sustainability","OAC","Information Technology Researc","09/01/2022","08/17/2022","Eunsuk Kang","PA","Carnegie-Mellon University","Standard Grant","Varun Chandola","08/31/2024","$99,998.00","","eskang@cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","CSE","164000","7916, 9102","$0.00","With the proliferation of computing technologies in our society, software plays an increasingly prominent role in contributing to and solving sustainability challenges. Despite its importance, however, sustainability remains a poorly understood concept among developers, with a general lack of tools, knowledge, and techniques that can be used to design and validate software systems that account for sustainability. This project aims to elevate sustainability as a first-class quality attribute of software that can be explicitly analyzed and designed for and to empower developers with methods for building sustainability into their systems. In addition, this project contributes novel design strategies and patterns that can influence the behavior of users towards sustainable use of computing products. The outcome of this project lays a foundation for further research in software engineering techniques and methodologies for sustainable computing.<br/><br/>To develop an in-depth understanding of sustainability challenges in software engineering, the first step in this project involves an empirical investigation of software applications in emerging, sustainability-relevant domains such as cyber-physical systems, IoT and mobile systems. Next, based on the outcome of this study, the project employs well-established requirements engineering methods to define sustainability as a collection of quality attributes, scenarios, and metrics. Finally, building on this definition, the project develops a catalog of design strategies and patterns that developers can use to build sustainability into their system as an explicit goal. This project also aims to establish a community of researchers in software engineering and other disciplines to encourage increased future activities in sustainability research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319343","Collaborative Research: IMR: MM-1A: Functional Data Analysis-aided Learning Methods for Robust Wireless Measurements","CNS","STATISTICS, Information Technology Researc, Networking Technology and Syst","10/01/2023","08/18/2023","Xuyu Wang","FL","Florida International University","Continuing Grant","Deepankar Medhi","09/30/2026","$196,569.00","","xuywang@fiu.edu","11200 SW 8TH ST","MIAMI","FL","331992516","3053482494","CSE","126900, 164000, 736300","079Z, 115Z, 7363","$0.00","With the increasing growth of large-scale, heterogeneous, dynamic, and complex wireless networks, how to achieve accurate and robust measurements in 5G networks and beyond becomes a challenging and important problem. Most existing data-driven solutions are black-box approaches, which may not be robust and adaptive, and work only for low-dimensional and discrete data. In fact, wireless data belong to the class of functional data, which can be represented by curves or functions. High-dimensional wireless datasets can be better handled by functional data analysis (FDA). Recognizing the significance of the aforementioned problems, this project aims to bridge the gap between FDA-based learning and wireless measurement. <br/><br/>The proposed research falls into the following four interwoven thrusts. (i) Functional Data Regression for Sparse Wireless Measurements: to develop a deep learning based approach to address fundamental regression problems of functional data. (ii) FDA-based Transfer Learning for DynamicWireless Measurements: to study transfer learning for functional data regression and classification under the distribution shift between test data and training data for effective wireless measurements in dynamic environments. (iii) Quantile FDA-based Learning for Robust Wireless Measurements and Control: to develop a deep learning-based approach to address the fundamental bottleneck of quantile regression-based methods. (iv) Wireless Measurement Applications for Integration and Validation. <br/><br/>If successful, this research will greatly advance the practice and understanding of functional data for wireless measurement and related fields. The educational and outreach components include: (i) Curriculum enhancement with learning theory and FDA, and joint developing a graduate course on FDA-based learning for wireless measurements. (ii) Engaging undergrads with hands-on projects. The existing outreach programs will be leveraged to offer research opportunities and seminars to undergrads, with emphasis on engaging underrepresented students. (iii) Outreach activities to increase public awareness, include journal publications, conference presentations, seminars, IEEE distinguished lectures, journal special issues, and workshops and special sessions at major conferences.<br/><br/>The code produced from this project will be disseminated at the public repository GitHub (https://github.com/). A project website will be maintained at Auburn University with URL: https://www.eng.auburn.edu/~szm0001/proj_lMR23.html. This project website will be frequently and regularly updated for dissemination of the outcomes from this project, including a description of the project, project team, major outcomes such as publications, codes and datasets, as well as an acknowledgement of NSF support to this project. This website will be managed/updated by the PI for the three-year project period.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201445","CC* Compute: An Open Science Grid shared computing platform at Penn State","OAC","Campus Cyberinfrastructure","07/01/2022","03/30/2022","Chad Hanna","PA","Pennsylvania State Univ University Park","Standard Grant","Amy Apon","06/30/2024","$399,664.00","Sarah Shandera, Eric Ford, Cindy Gulis, Donghui Jeong","crh184@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","CSE","808000","","$0.00","This project establishes a new state-of-the-art high-throughput computing cluster for researchers across the 24-campus Penn State University system.  The cluster is composed of 10 AMD EPYC nodes, each with 128 cores and one A40 Nvidia GPU. Researchers in the Institute for Gravitation and the Cosmos and the Institute for Computational and Data Sciences, priority users for the cluster, use this computer cluster to: study the universe by analyzing data to find ripples in space called gravitational waves; investigate novel ways to track down the universe?s missing matter called dark matter; and search for planets around other stars.<br/><br/>The new computing cluster improves access to computing for all Penn State researchers in the hopes of accelerating the pace of discovery across a broad spectrum of Science, Technology, Engineering, and Math through projects engaging the Penn State ?Research Innovation with Scientists and Engineers (RISE)? CyberTeam (NSF OAC-2018299). The cluster is connected to the Open Science Grid bolstering the fabric of national cyberinfrastructure that supports competitive US research through sharing resources across universities. Penn State also plans to use this resource for annual summer schools for students focused on data analysis, computing, physics, and astronomy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335908","Collaborative Research: NSF Workshop on Automated, Programmable and Self Driving Labs","CNS","Information Technology Researc","10/01/2023","08/17/2023","Nadya Peek","WA","University of Washington","Standard Grant","Xiaogang (Cliff)  Wang","09/30/2024","$75,903.00","","nadya@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","CSE","164000","025Z, 7556","$0.00","Laboratory automation increases precision and efficiency of science experiments. Advances in low-cost sensors, actuators, robotic systems, and control systems have lowered the barrier to entry to laboratory automation such that fully self-driving labs will have the potential to enable new practices of science experiment and to accelerate scientific exploration progress. There is a critical need to develop the principles and methodologies for self-driving laboratories. These systems will likely draw from best practices and experiences learned in data science, human-machine interaction, manufacturing and quality control, open-source ecosystems, and laboratory science methods. <br/><br/>The proposed workshop will convene leaders in self-driving laboratories and related areas including data science, robotics, manufacturing, and open-source ecosystems to define a roadmap for self-driving laboratories. Experts will discuss on latest advances and current challenges in automated sample preparation, experiment generation, data collection, and data analysis. The workshop will help identify major themes and assess on how future, interconnected goals can be best supported in a research context. By convening community leader around related topics, the workshop will seed cross-discipline collaboration on infrastructure that can support a broad range of sciences, which may include more complex experiments to accelerate scientific discovery and perform cost effective verification and validation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319511","IMR: MM-1C: Enabling Continual Passive Estimation of Performance of Internet Transfers: Online Measurement and Classification Methods","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","08/15/2023","Jasleen Kaur Sahni","NC","University of North Carolina at Chapel Hill","Standard Grant","Deepankar Medhi","09/30/2026","$400,000.00","Vladas Pipiras","jasleen@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","CSE","164000, 736300","115Z, 7363","$0.00","This project focuses on developing techniques that can monitor network traffic and reliably estimate the performance experienced by Internet users, while only looking at headers of network packets and not looking at any privacy-violating information. They main novelty is in also inferring what type of network connection, what type of application, and what type of device the user is using for accessing the Internet, and in being able to do all of the analysis fast enough to keep up with high speed with which traffic is carried by the Internet. <br/><br/>The innovations proposed in this project include: (i) The design of deep learning frameworks for classifying end-user segments, based on access networks, client platforms, and application usage, which will enhance understanding of Internet performance. (ii) The design of online sampling, hashing, and sketching techniques, which will enable other types of passive analysis to also be conducted in an online and light-weight manner. Currently, most passive analysis studies are relegated to storing and working with large sets of traces, which limits the their deployment. (iii) Passive estimation of whether network bandwidth constrains an Internet transfer, which has never been attempted before in a setting as diverse and evolving as the Internet.<br/><br/>This collaborative project brings together investigators from the fields of Computer Science and Statistics, and is expected to transform several domains: (i) The proposed efforts in user segmentation will also aid in understanding how other properties of the Internet (not just performance) differ across different user segments. This will aid in ensuring equitable and ubiquitous Internet access for all citizens.  (ii) The proposed monitoring techniques will be an important source of insights on network systems management that can help alleviate system bottlenecks and improve performance.  (iii) Experience in experimentation, measurements, and scientific analysis of big data is invaluable to federal, commercial, and academic institutions that are involved in mining for information in large data-sets. The proposed efforts in emulation and analysis of massive amounts of traffic data will be an excellent source of undergraduate and graduate students trained in these aspects. (iv) The proposed efforts in involving minorities and undergraduates in research will help broaden the diversity and capabilities of the Data Science (Computer Science/Statistics) work force. (v) The proposed outreach efforts (to middle- and high-schoolers) will help increase community engagement with science and technology.<br/><br/>The website for this project can be found at: https://sites.google.com/cs.unc.edu/real-time-passive-traffic-anal/home. This website will be updated with new developments and resources, until the end of the project activities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2045803","CAREER: Contextually Informed Autonomous Robotic Surgery","IIS","FRR-Foundationl Rsrch Robotics, Information Technology Researc, IIS Special Projects","03/01/2021","01/09/2023","Michael Yip","CA","University of California-San Diego","Continuing Grant","Juan Wachs","02/28/2026","$647,400.00","","yip@ucsd.edu","9500 GILMAN DR","LA JOLLA","CA","920930021","8585344896","CSE","144Y00, 164000, 748400","1045, 6840, CL10","$0.00","Timely and widespread access to medical care has become one of the greatest societal concerns today, pushed to the forefront by the Covid-19 outbreak. Exacerbating the problem is that there is a growing shortage of general surgeons in the US, and hundreds of rural hospitals are projected to close in the coming years. This Faculty Early Career Development (CAREER) project produces much-needed research towards realizing autonomous surgery in the future to address these broad societal issues of healthcare access and equity challenges. Specifically, the project involves taking a big step of realizing autonomous surgical robots is imbuing them with knowledge of human anatomy and physiology in a transcribed form that can be leveraged by artificial intelligence for performing procedures autonomously. As part of the project, an educational plan that integrates closely with the research plan is planned. An interactive lesson-game for high school students is to be developed, and an experience in a classroom setting is planned in which they will explore programming and biology in a video game. <br/><br/>To achieve the overall goal of this project, dense and reduced-order models for representing multimodal, deformable tissue environments with semantic attributes will be investigated and neural networks to approximate the forward and inverse problem. An anatomical roadmap is defined that presents the human body as a semantic navigation problem, and local context can be leveraged to perform semantic localization and environment mapping. For semantic localization, theory and strategies for leveraging semantic labels in deformable scenes and methods will be defined, utilizing geometric priors and active manipulation to identify features and classify scene objects. Finally, given contextually informed observation models, anatomical context may be used to evaluate and optimize safe plans and robust trajectories as well as higher-level decision making such as behavior trees. The educational activity involving teaching students about programming and biology will focus on teaching anatomical context from a biological perspective and basic sequential logic from a block-programming perspective. As part of an interactive lesson, the activity includes a hands-on programming portion that involves building an autonomous agent to solve an educational surgical simulator videogame. Following the interactive lesson, survey data on their perceptions regarding STEM and the potential interdisciplinary nature of biology and computer science will be collected and parsed. Then, by making the activities open-sourced and incorporating our lessons learned, we aim for our videogame to reach underrepresented minorities and provide them an opportunity to experience the broader, multi-disciplinary opportunities that computer science, engineering, and biology have to offer. <br/><br/><br/>This project is supported by the cross-directorate Foundational Research in Robotics program, jointly managed and funded by the Directorates for Engineering (ENG) and Computer and Information Science and Engineering (CISE).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835974","ABR: Collaborative research: Smart earpiece for supporting healthy eating behaviors","CNS","Information Technology Researc, Special Projects - CNS, CSR-Computer Systems Research","10/01/2018","07/17/2023","Jacob Sorber","SC","Clemson University","Continuing Grant","Marilyn McClure","09/30/2024","$506,881.00","Kelly Caine","jsorber@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","CSE","164000, 171400, 735400","9150, 9251, CL10","$0.00","Obesity is one of the most pressing health challenges faced by our country, and has been the target of much attention in the mobile health (mHealth) community. While the science of obesity indicates that diet is a major factor in healthy weight management, scientists are still not able to effectively, quickly and easily measure eating behavior. This project's goal is to develop a digital earpiece comfortable enough to wear (on or near the ear) that can sense and detect eating behavior. The project's long-term vision is to enable health researchers to better understand eating-related behaviors and, subsequently, to support the development of effective interventions that promote healthy diet and behavior.<br/><br/>Ultimately, a better understanding of eating-related behaviors, and better design of effective interventions regarding eating behavior, will have profound impact on personal and public health as well as the national economy. The project's hardware and software prototypes will be shared widely in the research community to enable experimentation around the sensing and interaction opportunities possible in an earpiece device. Furthermore, the project directly involves undergraduate and graduate students in interdisciplinary research, and outreach to middle-school students, expanding the supply of scientists educated in this important emerging topic.<br/><br/>The project will build a prototype wireless earpiece, with low-power (microwatt-scale) electronics and software sufficient to allow for the battery to last a full waking day; to develop efficient algorithms for detecting and distinguishing health-related behaviors; and to develop easy and effective means for the wearer to interact with the earpiece and its applications.<br/><br/>The team expects to answer scientific questions important to achieving the above goals. Specifically, they seek to advance scientific knowledge through the design and development of a wireless earpiece capable of sensing behavior and interacting with its wearer; develop novel low-power analog electronics and distributed software algorithms for inferring relevant behaviors from sensor data; develop novel interaction modalities involving bone-conduction audio between the earpiece and its wearer, complemented by tactile interfaces on the earpiece, on the skin, or on auxiliary devices like a wristband or smartphone; and validate these approaches through user studies and experiments inside and outside the lab.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107434","Collaborative Research: AF: Medium: A Unified Framework for Geometric and Topological Signature-Based Shape Comparison","CCF","Information Technology Researc, Algorithmic Foundations, EPSCoR Co-Funding","06/01/2021","09/13/2023","Carola Wenk","LA","Tulane University","Continuing Grant","Peter Brass","05/31/2025","$489,744.00","","cwenk@tulane.edu","6823 SAINT CHARLES AVE","NEW ORLEANS","LA","701185665","5048654000","CSE","164000, 779600, 915000","7924, 7926, 7929, 9150, 9251","$0.00","A fundamental aspect for data analysis is the ability to compare data sets, in order to measure (dis)similarity and quantify patterns present in the data.  However, data is often too large and complex to analyze in its entirety, and therefore different techniques are used to summarize the data in order to work with smaller, more manageable representations of it.  This project studies the data-comparison problem through the lens of mathematics, using geometric and topological signatures to represent these shapes concisely. This project will consider a variety of different kinds of shape data which live in some larger geometric or topological space (e.g., GIS trajectories, point sets, meshes, 3d scans, or graphs), and consider classes of algebraic, geometric, and graphical signatures which can be used to represent these shapes concisely.  The project draws primarily upon the nascent yet rapidly developing area of topological data analysis, where tools from topology like homology or homotopy are combined with geometric measures to create robust analysis tools for analyzing the shape of data. Graduate and undergraduate students will be tightly integrated into the project, and special efforts will be made to involve students from underrepresented groups. Additional efforts by the research team include planning a workshop focused on women in this field, as well as broadening diversity and inclusion efforts in their own universities.<br/><br/>The project focuses on shapes that have some common underlying annotation framework on top of the signature, which is usually additional structural or geometric information from the original embedding. The research consists of two major components.  In the first, the investigators are initiating a principled study of algorithms and approaches to develop a unified framework which leverages multiple signatures for shape comparison. The goal of this phase is to provide theoretical results as well as empirical evaluations on a variety of data sets and signatures. The second major component of the project studies inverse problems, which aim to reconstruct shapes from a combination of signatures.  Such problems are notoriously difficult for geometric or topological signatures, as they are necessarily lossy and remove certain types of information.  During the course of the project, the investigators are also developing a shape signatures toolkit that enables computation of a range of signatures and distances, adding to the software both existing notions of distance and new ones developed over the course of the project.<br/><br/>This project is jointly funded by the Algorithmic Foundations Core Program and by the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2127309","Computing Innovation Fellows Project 2021","CNS","Information Technology Researc","05/01/2021","02/29/2024","Elizabeth Bradley","DC","Computing Research Association","Continuing Grant","Mitra Basu","04/30/2025","$19,998,659.00","Mary Lou Maher, Tracy Camp, Ann Schwartz, Ellen Zegura, Kenneth Calvert, Randal Bryant","lizb@cs.colorado.edu","1828 L ST NW","WASHINGTON","DC","200365104","2022662949","CSE","164000","097Z, 102Z","$0.00","Due to the pandemic and the resulting uncertain circumstances, academic institutions are facing major hiring disruptions. This leaves the computing research community at risk of losing yet another class of young researchers who cannot afford to wait out the current disruption and thus may leave the research career path permanently. It is not a stretch to anticipate that the loss of this workforce in the research pipeline will have long-lasting downstream effects on computing innovation and impact. The Computing Innovation Fellows (CIFellows) Program addresses these issues by providing funding for two-year postdoctoral positions for recent PhD graduates in computer information science and engineering to provide a career-enhancing bridge experience that will give them the opportunity to remain in the academic research community and retain a cohort of early career professionals in areas under the umbrella of NSF CISE. <br/><br/>With funding by the National Science Foundation, the CIFellows 2021 program will offer two-year postdoctoral opportunities in computing, with cohort activities to support career development and community building. This program is open to researchers whose work falls under the National Science Foundation Computing and Information Science and Engineering Directorate and who have completed or plan to complete their PhD between 1/1/20 - 12/31/21. Applicants will work with a mentor from a US academic institution for their postdoc.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413978","Research Coordination Network (RCN) for Privacy Preserving Data Sharing and Analytics","CNS","Information Technology Researc","07/01/2024","02/22/2024","John Verdi","DC","FPF Education and Innovation Foundation","Standard Grant","Anna Squicciarini","06/30/2027","$490,133.00","","jverdi@fpf.org","1350 EYE ST NW STE 350","WASHINGTON","DC","200054860","2026429142","CSE","164000","025Z","$0.00","The advance of information technology, including artificial intelligence, has made the collection, analysis, and use of data central to research and economic activity. However, when those data are about people and what they do, there are risks to privacy that can grow with the sophistication of the analysis.  In response, the Future of Privacy Forum Education and Innovation Foundation (FPF) is convening a Research Coordination Network (RCN) for Privacy Preserving Data Sharing and Analytics.  The RCN is bringing together experts from academia, industry, and government to support the development, deployment, and scaling of Privacy Enhancing Technologies (PETs)?tools that allow for data analysis without sacrificing privacy.   While PETsthey can mitigate risk, there are many factors holding back these technologies? widespread use.  One of the biggest stumbling blocks to the broader adoption of PETs in research is the current lack of clarity regarding how regulators will interpret and enforce privacy rules when organizations use them. The RCN is working to resolve this issue by bringing stakeholders together to discuss the use of PETs and explore how rules and standards can promote their appropriate use. <br/><br/>The project team is convening a multidisciplinary, cross-sector, and international expert group of scholars and practitioners who focus on PETs development and use to understand the risks of data sharing and analytics for marginalized and vulnerable groups, along with civil rights and civil liberties writ large. This work is in direct response to recommendations from the National Strategy to Advance Privacy Preserving Data Sharing and Analytics. Further, the team is convening a secondary sub-network of high-level regulators from around the world that will inform and respond to the primary network, addressing the legal frameworks relevant to PETs adoption. With input from both groups, the project team is developing and disseminating new guidance to accelerate progress toward a privacy-preserving data-sharing and analytics ecosystem that advances democratic values and will foster convergence, characterize and strive to narrow persistent differences, and illuminate options to support broad deployment of PETs. The team is examining multiple mechanisms for this deployment, including via new technology, law and regulation, and/or standards and certifications. The team is particularly focused on use cases for PETs that support privacy-preserving machine learning and PETs that U.S. federal agencies may need to support the equitable use of AI.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2347372","RAPID: Tuning and Assessing Lahaina Wildfire Models with AI Enhanced Data","CCF","Information Technology Researc, Special Projects - CCF","10/01/2023","09/18/2023","David Eder","HI","University of Hawaii","Standard Grant","Almadena Chtchelkanova","09/30/2024","$200,000.00","Sean Cleveland, Neil Lareau, Hamed Ebrahimian, Negar Elhami-Khorasani","dceder@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","968222247","8089567800","CSE","164000, 287800","139Z, 7914, 9150","$0.00","There is an urgent need to collect data from the Lahaina Fire on Maui that is critical for on-going wildland and urban fire modeling efforts. Being an isolated location with limited wind and environmental observations, other data sources must be tapped to advance modeling and simulation research before these sources are lost. The data capture from multiple sources including social media and time-stamped photos, organized with AI-enhanced methods for data gathering, processing, and infusion will be led by Maui-based researchers working with Maui students. The work will show the importance of data in the understanding of fire propagation inside the community and interaction with urban structures with an additional goal of educating the public and enabling the Hawaii government and emergency response personnel to make decisions to counteract the disaster. This will aid in the development of policies to reduce the likelihood of major loss of life and property damage in the future.<br/><br/>Advanced AI techniques deployed on High Performance Computing (HPC) resources at the University of Hawai?i, the NSF?s Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program and other national infrastructure will be used to process the large volumes of data to obtain required information needed to tune and validate fire propagation and atmospheric simulations. The collected data will be archived and made publicly available in the Data Depot repository supported by NSF?s Natural Hazards Engineering Research Infrastructure (NHERI) program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319315","IMR: MM-1C: Methods and Metrics for IPv6 Internet Scanning","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","07/25/2023","Paul Pearce","GA","Georgia Tech Research Corporation","Continuing Grant","Deepankar Medhi","09/30/2026","$166,835.00","Frank Li","pearce@gatech.edu","926 DALNEY ST NW","ATLANTA","GA","303186395","4048944819","CSE","164000, 736300","115Z, 7363","$0.00","Technology to quickly map (i.e., scan) the Internet and explore its characteristics is of the utmost importance to researchers, governments, policy makers and businesses. Existing technologies rely on exhaustively enumerating the entire Internet address space. With the advent and recent widespread adoption of a newer Internet addressing scheme (called IPv6), such exhaustive enumeration is impossible, preventing our existing technology from being applied to future Internet growth. This project will create new Internet mapping technologies that are suitable for the newer IPv6 addressing scheme through the careful exploration of how IPv6 is deployed in practice, the development and evaluation of key mapping metrics, and the use of reinforcement learning on known addresses.<br/><br/>This project will enable the next generation of generative IPv6 Internet scanning, and provide a platform for extending existing IPv4 research to the IPv6 realm. Developing IPv6 scanning metrics and methods requires understanding detailed behaviors of the IPv6 protocol, service providers, address allocation, and machine-learning methods.  The project will first construct a set of comprehensive metrics for understanding IPv6 scanning performance that can serve as a benchmark for evaluating current and future scanning techniques. Next the project will characterize the substantial impact of large-scale IPv6 aliasing on IPv6 scanning, and the inadequacy of prior solutions, especially in the context of existing metrics and methods. From this the project will develop an online IPv6 dealiasing strategy and evaluate it over existing methods, quantifying its improvement on IPv6 Internet scanning. Lastly, guided by the aforementioned metrics and understanding of aliasing, the project will develop a new generalized online approach to IPv6 address generation and scanning via levering reinforcement learning from a set of known seed addresses to generate a diverse set of candidate IPv6 addresses which it subsequently scans.<br/><br/>Extending whole-Internet scanning to IPv6 is critical to a range of networking topics, from censorship measurement to network outages to vulnerability detection. Further, it is necessary to ensure results do not bias against certain regions without significant IPv4 infrastructure. Thus the value of IPv6 measurement and Internet scanning as a whole is key to researchers, policymakers, and governments. This project will produce the foundation for supporting a wide range of IPv6 Internet measurement research in the future. <br/><br/>More information about the project, including all research papers, datasets, code, repositories, tools, results, and outreach activities can be found on the project's website: https://cc.gatech.edu/~pearce/imr_metrics_methods_ipv6/. The website will be maintained for a period of at least 5 years after completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232917","CC* Regional Computing: Building Cyberinfrastructure to Forge a Regional Research Computing Alliance in Southern California","OAC","Campus Cyberinfrastructure","01/01/2023","08/30/2022","Carl Kesselman","CA","University of Southern California","Standard Grant","Amy Apon","12/31/2024","$999,970.00","Byoung-Do Kim","carl@isi.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900890701","2137407762","CSE","808000","","$0.00","Educational institutions in Southern California, particularly those focused on undergraduate studies, face challenges in accessing essential high-performance computing (HPC) resources and cyber-infrastructure. This lack of accessibility leads to decreased knowledge and experience with HPC among students and faculty and inhibits advancements in data-intensive research activities. The University of Southern California?s (USC) Center for Advanced Research Computing (CARC) houses one of the largest HPC facilities in the region and is deploying new cyberinfrastructure to expand their services and expertise to under-resourced universities in the area. The implementation of this new HPC system with corresponding user support services further solidifies the existing collaboration between USC and the Los Nettos consortium, comprised of 6 member universities and 30 associate networks.<br/><br/>The new system offers a similar computing environment to national HPC centers with advanced computing capacity, high-speed network, and abundant research applications and tools in its software stack. Additionally, the system leverages existing cyberinfrastructure at USC such as central file systems, HPC cluster interconnection, and federated authentication. This project significantly increases the accessibility of advanced cyberinfrastructure to students and researchers of regional institutions while catalyzing multi-institutional research collaborations, thereby forming a computational research and education hub in the region?the Southern California Research Computing Alliance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2233873","Collaborative Research: EAGER: Towards a Design Methodology for Software-Driven Sustainability","OAC","Information Technology Researc","09/01/2022","08/17/2022","Pooyan Jamshidi","SC","University of South Carolina at Columbia","Standard Grant","Varun Chandola","08/31/2024","$100,001.00","","pjamshid@cse.sc.edu","1600 HAMPTON ST","COLUMBIA","SC","292083403","8037777093","CSE","164000","7916, 9102","$0.00","With the proliferation of computing technologies in our society, software plays an increasingly prominent role in contributing to and solving sustainability challenges. Despite its importance, however, sustainability remains a poorly understood concept among developers, with a general lack of tools, knowledge, and techniques that can be used to design and validate software systems that account for sustainability. This project aims to elevate sustainability as a first-class quality attribute of software that can be explicitly analyzed and designed for and to empower developers with methods for building sustainability into their systems. In addition, this project contributes novel design strategies and patterns that can influence the behavior of users towards sustainable use of computing products. The outcome of this project lays a foundation for further research in software engineering techniques and methodologies for sustainable computing.<br/><br/>To develop an in-depth understanding of sustainability challenges in software engineering, the first step in this project involves an empirical investigation of software applications in emerging, sustainability-relevant domains such as cyber-physical systems, IoT and mobile systems. Next, based on the outcome of this study, the project employs well-established requirements engineering methods to define sustainability as a collection of quality attributes, scenarios, and metrics. Finally, building on this definition, the project develops a catalog of design strategies and patterns that developers can use to build sustainability into their system as an explicit goal. This project also aims to establish a community of researchers in software engineering and other disciplines to encourage increased future activities in sustainability research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322308","CC* Storage: EnviStor: A Repository for Supporting Collaborative Interdisciplinary Research on South Florida's Built and Natural Environments","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","09/01/2023","08/29/2023","Leonardo Bobadilla","FL","Florida International University","Standard Grant","Kevin Thompson","08/31/2025","$500,000.00","Julio Ibarra, Michael Kirgan, Jayantha Obeysekera, Jason Liu","bobadilla@cs.fiu.edu","11200 SW 8TH ST","MIAMI","FL","331992516","3053482494","CSE","723100, 808000","","$0.00","The South Florida region is home to nearly 10 million people, and the population is growing.  The region faces several challenges, such as rising sea levels and flooding, harmful algae blooms, water contamination, and wildlife habit loss, which affects the economy and the welfare of its population. Florida International University (FIU) will build EnviStor, which aims to be a centrally managed Petabyte scale storage system that is also a clearing house for supporting interdisciplinary research and modeling involving both built and natural environments in South Florida. EnviStor provides opportunities for students (75% of whom coming from underrepresented groups in STEM) and faculty to enhance their knowledge of database management, focusing on interoperability. <br/> <br/>EnviStor facilitates inter and intra-campus capabilities, augmenting the current FIU storage capabilities from the Terabyte to the Petabyte scale through a low-latency, high-performance, and cost-effective architecture to facilitate the creation of science data products. Data backups will be sent to the cloud, ensuring data preservation. In addition, the architecture will include interfaces that facilitate the sharing of datasets at intra- and inter-campus levels. EnviStor will share 20% of its storage resources by federating with the OSDF (Open Science Data Federation) and leveraging FIU?s Dataverse repository.  This effort is also supported by National Discovery Cloud for Climate (NDC-C) resources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2233808","CCF Core: Small: Hardware/Software Co-Design for Sustainability at the Edge","CNS","Information Technology Researc","11/01/2022","08/03/2022","Hai Li","NC","Duke University","Standard Grant","Daniel Andresen","10/31/2025","$600,000.00","","hai.li@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","CSE","164000","7923, 9102","$0.00","Due to the wide proliferation of cloud and now edge servers, the development of sustainable acceleration methods for edge and cloud computing and artificial intelligence (AI) acceleration is beneficial to society broadly and the computer science community, in particular. Reducing carbon and methane emissions is important to reaching environmental goals for reducing warming for which targets of 2030 and 2050 have been set. This project tackles the broader implications of the growing popularity of edge and cloud computing for executing AI to establish a sustainable computing system design methodology beyond pure energy efficiency. Unlike solely energy-efficient designs that focus on reducing power consumption and carbon emissions as a by-product, this project will build a hardware/software co-design framework to generate the optimal hardware and algorithm designs that meet specific constraints of functionality, performance, sustainability, and other system requirements. The framework will guide future sustainable hardware design. Towards this goal, the proposed framework engages holistic co-design efforts across four levels - modeling, algorithm, scheduling, and hardware.<br/><br/>By introducing new synergies between computing paradigms and emerging AI applications, the outcomes of this research will benefit the entire AI industry, from hardware development to algorithm design and the applications for the end-users. The proposed co-design framework will be the first research in the community to build holistically sustainable AI systems which are recently recognized as emerging in importance. The success of this project will pave the road for future sustainable computing system design. The educational efforts aim at cultivating students' interests in the study of sustainable computing, contemporary computer architecture, and artificial intelligence. The existing curricula of computer organization, algorithms, and computing systems will be enhanced by the interdisciplinary research topics on sustainable computing, edge computing, and machine learning, as well as the hands-on experiences in building the simulation prototypes. Special attention will be given to recruiting underrepresented groups and enriching students study experiences through new education forums.<br/><br/>This project is funded by funds allocated to Design for Sustainable Computing (NSF 22-060)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1954591","CHS: Medium: Collaborative Research: Fabric-Embedded Dynamic Sensing for Adaptive Exoskeleton Assistance","IIS","CISE Education and Workforce, Information Technology Researc, HCC-Human-Centered Computing","10/01/2020","06/12/2023","Rebecca Kramer-Bottiglio","CT","Yale University","Standard Grant","Ephraim Glinert","09/30/2024","$779,287.00","","rebecca.kramer@yale.edu","150 MUNSON ST","NEW HAVEN","CT","065113572","2037854689","CSE","055Y00, 164000, 736700","7367, 7482, 7924, 9251, CL10","$0.00","Exoskeletons can provide people with movement assistance when they become fatigued during long periods of exertion. While the focus of this award is on the use of adaptive exoskeletons by people who are able-bodied, the results could be applied to help people who have diseases such as multiple sclerosis, where people need increased assistance throughout the day as they become more tired. The interdisciplinary team will develop new human-robot interaction methods through adaptive exoskeleton control by using novel fabric-embedded sensors to measure how a person is moving and to develop a model to understand how these movements indicate when a person is becoming tired. Commercially-available exoskeletons do not explicitly address fatigue issues for enhancing endurance. Additionally, commercially-available sensors for sensing the wearer's body movement and muscle activation are rigid and can be uncomfortable when worn between the body and an exoskeleton system, which often also has rigid parts. The comfortable and breathable fabric-embedded sensors combined with an adaptive exoskeleton controller that can measure a person's fatigue in real time will allow endurance enhancement for human and exoskeleton performance. The project includes a soft-robotics design curriculum for broadening participation in computing.<br/><br/>Understanding and quantifying fatigue in human body is a complex research problem. This project will utilize a soft, breathable sensor garment between the wearer's body and the exoskeleton to sense fatigue and come up with a fatigue index. Such a fabric embedded sensing will allow for the development of exoskeletons that are more power efficient, provide assistance only when needed (at the power level that is needed based upon the wearer's fatigue level), and reduce metabolic costs for the wearer while preventing potential muscle atrophy that could arise from always-on exoskeleton assistance. An interdisciplinary approach, combining controls and robotics, human performance measurement, materials and soft robotics, and human-robot interaction, will be used to accomplish this goal. This research will establish a fatigue index that can 1) reliably and quantitatively indicate the level of physical fatigue, and 2) be easily obtained based on kinematic and kinetic data. The strain-field and fabric-embedded sensors will provide the kinematic data, which will then be used to estimate the kinetic data. Exploiting these data, the research will draw upon feedback control theory and human biomechanical modeling to create a systematic method for monitoring fatigue with provable estimation accuracy. An adaptive exoskeleton control framework will be systematically derived to explicitly address fatigue issues through three synergistically connected layers: activator, optimizer, and real-time controller. The resulting exoskeleton controller will enable adaptive assistance for endurance enhancement by delaying the onset of wearers' fatigue and allowing better usage of exoskeleton power.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2224687","IMR: RI-P: Programmable Closed-loop Measurement Platform for Last-Mile Networks","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2022","08/10/2022","Arpit Gupta","CA","University of California-Santa Barbara","Standard Grant","Deepankar Medhi","09/30/2024","$100,000.00","Nicholas Feamster","arpitgupta@cs.ucsb.edu","3227 CHEADLE HALL","SANTA BARBARA","CA","931060001","8058934188","CSE","164000, 736300","115Z, 7363","$0.00","Today's society relies heavily on various Internet-based services for work, education, health care, and entertainment, and the quality of access (or last-mile) network dictates the quality of experience for these services. The quality of access networks in the US varies across different regions, communities, and demographics, contributing to digital inequities. Policymakers are interested in bridging this gap; to do so, they require understanding the state of access quality. This collaborative project will explore the design and deployment strategies for a new network measurement and analytics platform that enables policymakers to assess access networks' quality at scale.<br/><br/>This collaborative planning project will bring together researchers from the University of California Santa Barbara and the University of Chicago to explore the feasibility of a programmable and scalable data-collection platform, demonstrate the value of closed-loop data collection with specific use-cases, and evaluate different scaling strategies using the data collected from its residential deployments in Chicago. Additionally, this project will organize a workshop inviting different stakeholders from industry, government, and academia to identify scaling strategies, new features, and concrete use cases for the proposed platform.<br/><br/>The proposed platform can answer questions ranging from the state of the Internet performance and infrastructure in specific neighborhoods to the Internet speeds and performance characteristics required to support special/new applications (e.g., video conferencing, telehealth, etc.). Such capabilities will empower policymakers and other stakeholders, such as networking researchers, network operators, and end users. It is envisioned that the open-source tools, methods, and data will become the standard that these organizations rely on to both assess the nature of problems in Internet access networks, as well as to assess the effectiveness of various policy interventions, programs, and infrastructure investments, and so forth. Project website: https://pinetrics.cs.ucsb.edu/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2223360","IMR: MT: Tools for Safe, Easy, and Reliable Active Global Internet Measurement","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2022","08/02/2023","Zakir Durumeric","CA","Stanford University","Continuing Grant","Deepankar Medhi","09/30/2025","$599,999.00","","zakir@cs.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","CSE","164000, 736300","115Z, 7363","$0.00","It is difficult to predict Internet behavior due the complexity and distributed nature of the Internet. For example, it is hard to know whether proposed Internet protocols will work when deployed. As a result, researchers turn to large-scale, measurement of real-world systems to understand how the Internet is structured, how protocols operate, and where there are security and performance failures. This proposal improves a core set of measurement tools that researchers use for performing large-scale measurement of Internet hosts, websites, names, cloud assets to prevent common measurement errors, support recently introduced protocols, and account for real-world deployment complexities.<br/><br/>This proposal will specifically make improvements to the ZMap suite of active measurement tools, including to (1) automatically detect error conditions and prevent erroneous data, (2) adapt tools to accommodate recently introduced network protocols like TLS 1.3, and (3) enable tools to find and accommodate services that appear in unpredictable locations. The challenge in these tasks lays in the fact that: (1) because there is no ground truth against which to compare results, tools need to intelligently determine whether results have become unstable and to alter their behavior while maintaining data consistency, (2) must operate with exceptional performance to accommodate the size of Internet-wide studies, and (3) must produce and structure data that researchers can use when research tasks are not known ahead of time.<br/><br/>The impact of this project lies in the research that it will enable. ZMap has been used in many  research publications over the past few years, but research has uncovered several shortcomings and error conditions that can produce inconsistent data. The Internet has also evolved over this time period, which requires software changes to accommodate. The improvements will help make ZMap accessible to additional researchers, ensure that researchers with less Internet measurement experience can reliably collect valid results, enable the study of recently introduced protocols, and enable researchers to use the suite of tools as part of broader measurement platforms.<br/><br/>The ZMap tools are documented and available for download at https://zmap.io. Any auxiliary datasets produced will be published at https://scans.io. ZMap is planned to be maintained for the foreseeable future.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2223610","IMR: MT: A Community Platform for Controlled Experiments on Internet Access Networks","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2022","07/29/2022","Nicholas Feamster","IL","University of Chicago","Standard Grant","Deepankar Medhi","09/30/2024","$600,000.00","","feamster@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","CSE","164000, 736300","115Z, 7363","$0.00","Internet measurement research has long operated on convenience samples---measurements that are derived from convenience, based on the sets of vantage points that can be obtained through access to a particular testbed, the deployment of an application, or the opportunistic recruitment of participants. Throughout the early decades of the Internet, such an approach was sensible and could often provide important insights about the design of specific protocols or applications, or even some aspect of the Internet writ large.  Over the past several years, however, the research community---and society more broadly---has begun to ask a broader set of questions, moving beyond those of technical protocol or application design, to questions of social or policy relevance. The current measurement tools and platforms are ill-equipped to address the concomitant research questions that intersect with broader societal questions.  <br/><br/>This project aims to develop a platform to change that, delivering the first-ever network measurement platform that can facilitate studying a broader set of research questions than are possible with existing platforms. Perhaps one of the most striking examples of such a set of questions relates to the Internet's digital divide.  The challenge is widely acknowledged, by researchers, advocacy organizations, and policymakers alike, as a significant challenge, and providing all Americans with affordable Internet connectivity is a national priority as well.  This project is developing a measurement platform that will enable this new family of measurements. Building on early successes in a pilot deployment in Chicago, the project aims to design, develop, and deploy a first-of-its-kind measurement infrastructure that is specifically designed to enable new classes of research questions concerning digital infrastructure across our nation's population (and ultimately, the global population). The infrastructure is open-source, and incrementally deployable and expandable, allowing the project and infrastructure to scale from pilot to national or global deployments as lessons from early deployments are applied to a larger testbed.  <br/><br/>The potential outcomes of this platform are profound and wide-ranging, catalyzing an entirely new class of Internet measurements and research studies---shifting the computer networking and Internet measurement research areas from questions that focus only on convenience samples (thus limiting our research questions to those of technology and protocols) to samples that concern infrastructure and population (thus allowing networking researchers to ask a broader set of technical questions with importance across disciplines, from economics to policy to social science).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2223556","IMR: MT: Fine-Grained Telemetry for Next-Generation Cellular Access Networks (NG-Scope)","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2022","08/01/2023","Kyle Jamieson","NJ","Princeton University","Continuing Grant","Deepankar Medhi","09/30/2024","$600,000.00","","kylej@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","CSE","164000, 736300","115Z, 7363","$0.00","The Next Generation Cellular Wireless Radio Access Network (RAN) is emerging as the dominant data network for Internet access, supporting capacity-enhanced mobile broadband, ultra low latency, and high connectivity machine-to-machine scenarios alike.  Insightful performance measurement of such applications depends on one or more of delay, jitter, and network capacity measurement data (telemetry) at millisecond-level timescales.  This necessitates a clean-slate approach that marries high resolution physical-layer telemetry with end-to-end network- and transport-layer telemetry.  This proposal develops NG-Scope, a measurement tool that provides a network telemetry stream to higher software layers.  NG-Scope's telemetry succinctly summarizes wireless capacity, RAN-added latency, and network management data.  Rather than relying on crowd-sourced data from deployment on most or all user equipment, NG-Scope provides visibility into all users' telemetry data in a given cell.  NG-Scope accounts for and leverages the latest 5G innovations, but abstracts these details behind a proposed standardized API (Application Program Interface) that measurement tools, applications, and transport layer protocols access.<br/><br/>NG-Scope gives network measurement researchers visibility into the RAN at fidelity levels and user population scales not previously possible.   Its proposed RAN-specific processing techniques bridge the gap between physical- and transport-layer telemetry data, enabling inference of the sources of network phenomena not previously observable, such as the ability to differentiate between wireless channel conditions and RAN versus core network congestion as the cause of packet latency and jitter.  NG-Scope does not require the incentivization or cooperation of any service provider, enabling unfettered access to RAN telemetry and thus broadly accelerating innovation in network measurement and design.<br/><br/>NG-Scope will benefit basic longitudinal internet measurement research, network operators' network monitoring tasks, and internet application architects' design and development efforts.  It will be developed in an open-source model so that internet measurement researchers can deploy the tool locally.  Results of data measurement campaigns will be made publicly available, to facilitate trace-driven emulation based experiments of other researchers and network protocol designers.  Researchers will be able to remotely prototype and evaluate their own applications and network protocols in a virtual programming environment with remote access.  The project will engage with under-represented groups through summer research opportunities, REU sites, and the undergraduate junior/senior thesis.  Project website: ngscope.cs.princeton.edu<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335883","Conference: NSF Workshop on Crosscutting Research Needs for Digital Twins","CNS","Information Technology Researc","10/01/2023","09/11/2023","Karen Willcox","NM","Santa Fe Institute","Standard Grant","Jason Hallstrom","09/30/2024","$98,115.00","","karen.willcox@gmail.com","1399 HYDE PARK RD","SANTA FE","NM","875018943","5059462727","CSE","164000","7556","$0.00","This two-day workshop will identify research needs for digital twins. A digital twin is a set of digital constructs that mimic the structure, context, and behavior of a physical system. Digital twins are coupled to their physical counterparts; they are characterized by a dynamic, continual, two-way flow of information between the digital representation and the physical counterpart. Data streaming from the physical system are integrated into the digital representation to reduce uncertainties and improve accuracy. The digital representation may in turn be used to control the physical system, optimize data acquisition, and prove decision support. Digital twins must execute rapidly enough to support decisions and control in time scales that are relevant to the physical system and must manage and quantify uncertainties. Of particular note is the bi-directional interaction between the virtual and the physical, which is central to distinguishing a digital twin from a conventional simulation. This bi-directional interaction brings many new challenges to modeling, data curation, and decision-making.<br/><br/>The workshop will bring together a diverse community of stakeholders to identify research gaps common across application domains and that may benefit from crosscutting interdisciplinary research efforts. The technical program is organized around four methodological themes: (1) Models; (2) Data; (3) Decisions; and (4) Verification, Validation & Uncertainty Quantification. To ensure crosscutting outcomes, these themes will be explored across four application domains: (1) Engineering, Materials & Manufacturing; (2) Smart Cities; (3) Biomedicine & Health; and (4) Climate, Natural Hazards & Environmental Sciences. Workshop speakers will be asked to identify barriers and enablers for achieving scalable digital twins in the four methodological themes, giving concrete examples drawn from the application domains. These barriers and enablers will be collected and organized during interactive workshop sessions. The objective is to identify, for each of the methodological themes, a ?top-five? list of crosscutting barriers and a ?top-five? list of potential crosscutting enablers, supported with concrete examples from the application domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319487","Collaborative Research: IMR: MM-1B: Privacy-Preserving Data Sharing for Mobile Internet Measurement and Traffic Analytics","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","07/26/2023","Rose Qingyang Hu","UT","Utah State University","Continuing Grant","Deepankar Medhi","09/30/2026","$66,145.00","","rose.hu@usu.edu","1000 OLD MAIN HL","LOGAN","UT","843221000","4357971226","CSE","164000, 736300","115Z, 7363, 9102, 9150","$0.00","Mobile Internet measurement is critical to network design, resource allocation, and troubleshooting network issues. However, sharing of mobile Internet measurement data can potentially compromise user privacy. Given the wide introduction of artificial intelligence to mobile Internet measurement and traffic analytics, there is an urgent need for data sharing solutions that provide explainability in terms of the trade-offs among data quality, utility and quantity. To close the gap, the objective of this project is to develop new methods to augment data with explainable data quality and utility, to access and share collected data in a privacy-preserving manner, and to collaboratively analyze Internet data with intelligence and autonomy.<br/><br/>This collaborative project brings together investigators from University of Nebraska-Lincoln, Utah State University, and University of Wisconsin-Madison. It aims to lay a solid foundation for mobile Internet measurement with privacy preservation, collaborative and distributed intelligence, and autonomy. Methodologies and methods will be developed for quality-explainable data synthesis and augmentation; privacy-preserving data sharing; and collaborative and privacy-preserving analysis of Internet measurement data. Moreover, a mobile Internet traffic generator will be developed for evaluating the proposed methods. This project can significantly advance the prior research in Internet traffic analytics, quality-explainable and privacy-preserving data processing, mobile Internet traffic analytics, distributed artificial intelligence and machine learning algorithms, optimizations, modeling, simulations, and testbed experiments. <br/><br/>The research efforts associated with this project will greatly advance the understandings of the critical issues in the next-generation mobile Internet measurement with distributed and collaborative intelligence to provide privacy-preserving data sharing and Internet traffic analytics. The outcomes of the project can potently foster the transition of our society into data sharing with privacy and intelligent era. Research and education will be integrated in this project by introducing emerging mobile Internet measurement and privacy-preserving data processing with advanced topics such as 6G wireless systems, data augmentation, artificial intelligence and machine learning models into the current curricula in the three collaborative institutions.<br/><br/>The project website is hosted at: cns.unl.edu/imr-ppds. The collected data, simulation codes, and publication list will be published on the project website. Copies of technical reports and accepted manuscripts will also be published on the project website. The website will be maintained during the project years, and remain accessible for least 2 years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319367","Collaborative Research: IMR: MM-1C: Methods for Active Measurement of the Domain Name System","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","07/26/2023","Paul Barford","WI","University of Wisconsin-Madison","Continuing Grant","Deepankar Medhi","09/30/2026","$63,286.00","","pb@cs.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","CSE","164000, 736300","115Z, 7363","$0.00","This project will investigate techniques for measuring the contents of the Domain Name System (DNS) toward the goal of producing a comprehensive census of all record types. The project will focus on developing methods for generating domain names that will be used in queries of the global DNS. Strategies for name generation that will be considered include making use of underutilized data sources and machine learning techniques including Large Language Models. New metrics and a system called EverDNS will be developed to evaluate the effectiveness of the name generation methods by sending high-speed queries to the DNS from different Internet vantage points.<br/><br/>The first component of this project will investigate techniques for assembling the largest-feasible collection of domain names that will be used as targets to query the global DNS. The second component will assess these techniques by developing EverDNS, which will include a prototype data collector and a centralized controller. EverDNS will build on state-of-the-art DNS measurement tools and augment their methods to capture the greatest amount of information within a limited measurement budget. This project will deliver new insights about how to assemble a comprehensive census of the DNS and new data sets for research that will be made available to the community.<br/><br/>The techniques created by this project will enable generation of data sets that will advance scientific understanding of the DNS. It is expected that the insights gained through this research will enable methods for ensuring a more robust, manageable, and better performing DNS. Given the critical role of DNS in the Internet, this project has the potential to positively impact society as a whole. New materials for networking and data science courses will be developed as a direct result of this project. Research results will be disseminated by publishing in respected academic conferences and workshops, and all software and data artifacts will be made available to the community.<br/><br/>The project webpage will be hosted at https://everdns.github.io The webpage will include an overview of the project, project participants, updates on latest results and publications. It will also include all software and data artifacts developed during the project. The webpage and associated repositories will be available on github on an on-going basis after the conclusion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319369","Collaborative Research: IMR: MM-1C: Methods for Active Measurement of the Domain Name System","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","07/26/2023","Mark Crovella","MA","Trustees of Boston University","Continuing Grant","Deepankar Medhi","09/30/2026","$51,846.00","","crovella@bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","CSE","164000, 736300","115Z, 7363","$0.00","This project will investigate techniques for measuring the contents of the Domain Name System (DNS) toward the goal of producing a comprehensive census of all record types. The project will focus on developing methods for generating domain names that will be used in queries of the global DNS. Strategies for name generation that will be considered include making use of underutilized data sources and machine learning techniques including Large Language Models. New metrics and a system called EverDNS will be developed to evaluate the effectiveness of the name generation methods by sending high-speed queries to the DNS from different Internet vantage points.<br/><br/>The first component of this project will investigate techniques for assembling the largest-feasible collection of domain names that will be used as targets to query the global DNS. The second component will assess these techniques by developing EverDNS, which will include a prototype data collector and a centralized controller. EverDNS will build on state-of-the-art DNS measurement tools and augment their methods to capture the greatest amount of information within a limited measurement budget. This project will deliver new insights about how to assemble a comprehensive census of the DNS and new data sets for research that will be made available to the community.<br/><br/>The techniques created by this project will enable generation of data sets that will advance scientific understanding of the DNS. It is expected that the insights gained through this research will enable methods for ensuring a more robust, manageable, and better performing DNS. Given the critical role of DNS in the Internet, this project has the potential to positively impact society as a whole. New materials for networking and data science courses will be developed as a direct result of this project. Research results will be disseminated by publishing in respected academic conferences and workshops, and all software and data artifacts will be made available to the community.<br/><br/>The project webpage will be hosted at https://everdns.github.io The webpage will include an overview of the project, project participants, updates on latest results and publications. It will also include all software and data artifacts developed during the project. The webpage and associated repositories will be available on github on an on-going basis after the conclusion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319368","Collaborative Research: IMR: MM-1C: Methods for Active Measurement of the Domain Name System","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","07/26/2023","Joel Sommers","NY","Colgate University","Standard Grant","Deepankar Medhi","09/30/2026","$60,000.00","","jsommers@colgate.edu","13 OAK DR","HAMILTON","NY","133461386","3152287457","CSE","164000, 736300","115Z, 7363","$0.00","This project will investigate techniques for measuring the contents of the Domain Name System (DNS) toward the goal of producing a comprehensive census of all record types. The project will focus on developing methods for generating domain names that will be used in queries of the global DNS. Strategies for name generation that will be considered include making use of underutilized data sources and machine learning techniques including Large Language Models. New metrics and a system called EverDNS will be developed to evaluate the effectiveness of the name generation methods by sending high-speed queries to the DNS from different Internet vantage points.<br/><br/>The first component of this project will investigate techniques for assembling the largest-feasible collection of domain names that will be used as targets to query the global DNS. The second component will assess these techniques by developing EverDNS, which will include a prototype data collector and a centralized controller. EverDNS will build on state-of-the-art DNS measurement tools and augment their methods to capture the greatest amount of information within a limited measurement budget. This project will deliver new insights about how to assemble a comprehensive census of the DNS and new data sets for research that will be made available to the community.<br/><br/>The techniques created by this project will enable generation of data sets that will advance scientific understanding of the DNS. It is expected that the insights gained through this research will enable methods for ensuring a more robust, manageable, and better performing DNS. Given the critical role of DNS in the Internet, this project has the potential to positively impact society as a whole. New materials for networking and data science courses will be developed as a direct result of this project. Research results will be disseminated by publishing in respected academic conferences and workshops, and all software and data artifacts will be made available to the community.<br/><br/>The project webpage will be hosted at https://everdns.github.io The webpage will include an overview of the project, project participants, updates on latest results and publications. It will also include all software and data artifacts developed during the project. The webpage and associated repositories will be available on github on an on-going basis after the conclusion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1846320","CAREER: Enabling Seamless Vision Sensing in Cloud-Edge Systems","CNS","GVF - Global Venture Fund, Information Technology Researc, CSR-Computer Systems Research","06/01/2019","06/08/2023","Karthik Dantu","NY","SUNY at Buffalo","Continuing Grant","Marilyn McClure","05/31/2024","$648,932.00","","kdantu@buffalo.edu","520 LEE ENTRANCE STE 211","AMHERST","NY","142282577","7166452634","CSE","054Y00, 164000, 735400","1045, 120Z","$0.00","Mobile devices such as smartphones have changed our daily lives. With the growth in the capabilities of smartphones as well as wearables such as ""smart glasses"", the next decade promises richer capabilities in terms of augmented reality, improved contextual sensing and more seamless computing that will integrate into our day-to-day lives. However, vision sensing applications, which are fueled by the improved understanding of user context, are highly resource and energy intensive, reducing useful operation time.<br/><br/>This project will enable the design of such applications by providing a programming framework that allows future applications to leverage cloud computation seamlessly, to provide the user with a good quality of service for the intended application while removing from programmers the burden of reasoning about where the computation should happen.  The project includes plans for integration of research in education, broadening participation, and educational outreach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2318844","Collaborative Research: SaTC: CORE: Small: Towards a Privacy-Preserving Framework for Research on Private, Encrypted Social Networks","CNS","Human Networks & Data Sci Infr, Information Technology Researc, Special Projects - CNS, Special Projects - CCF, IIS Special Projects, Secure &Trustworthy Cyberspace","10/01/2023","09/25/2023","Mohammad Amin Rahimian","PA","University of Pittsburgh","Continuing Grant","Sara Kiesler","09/30/2026","$93,143.00","","rahimian@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","CSE","130y00, 164000, 171400, 287800, 748400, 806000","025Z, 107Z, 7434, 7923","$0.00","This research aims to explore the role of private, encrypted social networks (ESNs) such as WhatsApp and iMessage in the spread of information and rumors. It seeks to understand how manipulated information is exposed, believed, and shared on ESNs compared to public social media platforms. By utilizing a privacy-preserving data donation framework, the project will develop tools to uncover and analyze the spread of manipulated information on encrypted platforms. The resulting data and software will be shared while ensuring data privacy and confidentiality. The research will shed light on how bad actors exploit unmoderated spaces to disseminate disinformation, particularly targeting vulnerable communities providing insights into the role of different social networks (public, semi-public, private) in spreading and influencing beliefs. This research will benefit various disciplines such as data science, journalism, cybersecurity, demography, social psychology, behavioral science, communications, and epidemiology. The tools developed can assist domain experts, policymakers, journalists, security operators, and NGOs in supporting at-risk populations and devising effective solutions. Ultimately, the research aims to empower ESN users by providing information about their information consumption and identifying instances of rumor and inauthentic information.<br/><br/>In this work, using data collected with a novel privacy-preserving data donation framework, the project team studies the exposure, belief in and sharing of information on encrypted social networks versus public social media platforms. The technical contributions would enable: a  large-scale, anonymized data collection framework to identify viral content spreading on encrypted platforms in a privacy-preserving manner; and  statistical measures of the prevalence and incidence of viral rumor and hearsay in the target population using the collected data, while controlling the privacy risks to our users from publishing statistics based on their donated data and survey responses. The project team is developing novel algorithms that work on private and public data and come with tunable parameters that allow the researchers to balance the loss in statistical efficiency from randomization against a differential privacy budget. The statistical measures proposed allow working with private ESN data and sharing those answers without undue risk to the privacy of data donors. Simulation studies with calibrated measures of spread allow the team to evaluate the large-scale impact of various platform strategies across public and private social networks, and experimental designs with human subjects are used to evaluate the impact of potential interventions on individuals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319409","Collaborative Research: IMR:MM-1B: Privacy in Internet Measurements Applied To WAN and Telematics","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2023","07/28/2023","John Heidemann","CA","University of Southern California","Continuing Grant","Deepankar Medhi","03/31/2026","$107,054.00","","johnh@isi.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900890701","2137407762","CSE","164000, 736300","115Z, 7363","$0.00","The PIMAWAT (Privacy in Internet Measurements Applied to WAN And Telematics) project will demonstrate new methods to provide data networking datasets that respect end-user privacy, while still being able to support new research in network protocols, security, privacy, and machine learning. The main insight is that *most data today sent over the wide-area network (WAN) is encrypted*; thus, the challenge is to demonstrate what data is encrypted, detect and scrub any remaining leaks, and finally anonymize the metadata (who talks to whom) before sharing data.<br/><br/>The intellectual merit of PIMAWAT will be to develop new methods to anonymize network traffic at scale, then use those new algorithms to evaluate potential data leakage, and demonstrate that real-world data sources can be scrubbed for sharing while respecting privacy.  PIMAWAT plans to focus the investigator?s prior work on wide-area network data traffic. As possible, it will also explore vehicle telematics as a recently developing dataset that poses unique privacy opportunities and challenges, with a device (not person) focus, yet with geolocation and application details.<br/><br/>The broader impacts of PIMAWAT will be to democratize the potential to collect and share network data through new tools and best-practices for privacy-respecting data scrubbing.  Data from this project will enable new approaches in computer science for protocol design and cyber-security, applying AI and machine learning, and will provide early results in the rapidly evolving field of vehicle telematics. PIMAWAT will provide new tools, data, and practices, and encourage use of these methods by other researchers, in classrooms, and by industry.<br/><br/>The PIMAWAT project website will be https://ant.isi.edu/pimawat and its tools and datasets will be provided through https://ant.isi.edu/datasets/ as they are developed during project and after it completes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322110","CC* Networking Infrastructure: Improving Science Data Flows with Advanced Networking and Cyberinfrastructure at Manhattan College","OAC","Campus Cyberinfrastructure","08/01/2023","02/28/2024","Wyatt Madej","NY","Manhattan College","Standard Grant","Kevin Thompson","07/31/2025","$639,778.00","Magdalen Michalczyk, Zahra Shahbazi, Luis Salazar, Wyatt Madej","wmadej01@manhattan.edu","4513 MANHATTAN COLLEGE PKWY","BRONX","NY","104714004","7188627160","CSE","808000","","$0.00","This project re-architects the campus network at Manhattan College to drive, enable, and advance scientific data flows and discovery across multiple disciplines, including engineering, mathematics, chemistry, and sociology utilizing advanced cyberinfrastructure to eliminate barriers to research and STEM education currently encountered by faculty, researchers, and students in respect to the flow of scientific data. <br/><br/>The campus network is rearchitected to optimize scientific data flows via the construction of a campus-wide IPv6-native Science DMZ, enabling frictionless paths for science data accessible across campus. This Science DMZ utilizes recognized best practices for research cybersecurity, balancing network performance and security, and establishes data transfer nodes to facilitate data transfers between Manhattan College researchers and their collaborators via Globus. It connects the College to external research and education (R&E) networks statewide via NYSERNet and nationally through Internet2, and monitors network performance using perfSONAR to tune the Science DMZ to ensure optimal performance. The re-architected network, designed and tuned for scientific data flows, advances and drives research, scientific discovery, and education. Additionally, this project is the foundation for future cyberinfrastructure improvements, including high performance computing and storage infrastructures to further R&E capabilities of the College?s researchers and students. This network will support and grow these activities to advance research and STEM education programs that educate students, contribute to new scientific discoveries, inform public policy, facilitate institutional partnerships, and provide a better STEM education to train the next generation of scientists and engineers to be capable of tackling complex challenges in the world ahead.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018299","CC* Team: Research Innovation with Scientists and Engineers (RISE)","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","08/25/2022","Chad Hanna","PA","Pennsylvania State Univ University Park","Continuing Grant","Kevin Thompson","06/30/2024","$1,396,556.00","Edward O'Brien, Jenni Evans","crh184@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","CSE","723100, 808000","","$0.00","The pace of scientific discovery and the dissemination of scientific knowledge are increasingly being driven by computation through modeling, data science, and digital communication platforms. Not all researchers are equally positioned to leverage this computational revolution due to having inadequate expertise in their groups or insufficient funding to hire computational experts into full time positions.  Penn State is working to ensure that researchers and educators across the 24-campus Penn State system have access to the cutting-edge cyberinfrastructure and computational expertise that they need to conduct the highest quality research and education.  Penn State's approach is to build a team of cyberinfrastructure facilitators who are shared across investigators and who consult on projects at various scales to bring shared knowledge and the best-practices of modern computational techniques and tools to the broadest possible Penn State community.  These facilitators, known as the ""Research Innovation with Scientists and Engineers"" (RISE) team, are experts in databases, visualization, code optimization, application development, web services, and cloud computing. They have broad knowledge of research cyberinfrastructure and are able to architect, design, and develop new cyberinfrastructure.  They will also have deep knowledge of various scientific domains and will enable computational discovery. Investing in such a team will pay substantial dividends through increased productivity of faculty, more efficient use of research and education funding, and ultimately new discoveries across a broad swath of scientific domains including Physics, Astronomy, Bioinformatics, Chemistry, Energy, and Climate Modeling.<br/><br/>This project builds a cyber-team for Research Innovation with Scientists and Engineers (RISE) who will partner with campus-level CI experts, domain scientists, research groups, and educators to drive new approaches that support scientific discovery across the state-wide Pennsylvania State University system including 24 campuses serving more than 100,000 students.  The RISE team will directly facilitate the usage and creation of research cyberinfrastructure across domains including Astronomy, Biology, Chemistry, Meteorology, Physics and more through consulting and providing direct services to faculty. The RISE team will partner with the Open Science Grid to establish Penn State as an OSG site, develop replica-exchange molecular dynamics software, apply machine learning to molecular biophysics, build digital signal processing software for radio astronomy,  collaborate on feature development and testing with HTCondor, onboard new climate modeling tools and software in a sustainable and maintainable ecosystem, develop a gene sequencing management platform, deploy and maintain infrastructure to support real-time gravitational wave analysis with LIGO, and build a science gateway for stellar astrophysics simulations. Through the RISE team's shared knowledge, they will elevate the productivity of researchers who use and develop cyberinfrastructure allowing them to accomplish far more than they could in isolation.  In order to broaden participation, the investigators will develop a seed grant program whereby faculty can apply to receive extended engagement with the RISE team where members would be embedded into research groups.  RISE members will also regularly conduct training workshops and seminars in response to the needs of faculty across all Penn State campuses.  Participation broadening and coordination activities will involve regular travel among the branch campuses by RISE team members and project investigators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201558","CC* Compute: Accelerating Compute Driven Science Through a Sharable High Performance Computing Cluster in Kent State Multi-Campus System","OAC","Campus Cyberinfrastructure","05/01/2022","04/07/2022","Javed Khan","OH","Kent State University","Standard Grant","Amy Apon","04/30/2025","$400,000.00","Philip Thomas","javed@kent.edu","1500 HORNING RD","KENT","OH","442420001","3306722070","CSE","808000","","$0.00","This project will add an agile locally and globally sharable HPCC (High-Performance Computing Cluster) hosted in a ScienceDMZ enclave, integrated with national science computing facilities, including the Open Science Grid (OSG), by creatively using recent advances in federated science networking and distributed systems? virtualization open to regional faculty. The system is composed of 18 nodes with dual Intel Xeon Gold 6242R class CPUs (20 core), 192GB RAM, and an NVIDIA A30 class GPUs. Storage is spread across the nodes using CEPH<br/><br/>The project supports several interesting newly emerging collaborative HPCC workflows- scienceware as-a-service (SAS) and science-data-lakes (SDL), and intense real-time-computing (iRTC) besides supporting the HPC and HTC workflows. NSF-funded resources in this project are open to all faculty researchers in northeast Ohio colleges and their collaborators, including the faculty of all eight campuses of Kent who are in the network?s latency proximity and engaged in data-intensive collaborative workflows. In order to support high throughput and collaborative computing, the ScienceDMZ exercises a new model of unimpeded host-centric cauterized and federated security, as opposed to the traditional perimeter focused security approach. It is already fronted by a 100-Gbps Data Transfer Node (DTN) capable of ?friction-free? long-haul transferring massive datasets. <br/><br/>The project directly contributes to NSF?s goals to foster innovation, integration, and engineering of new campus-level networking and cyberinfrastructure that can assertively support widely collaborative, multi-campus distributed massive-data driven research and harness largely untapped potential to share unused compute cycles and resources across the entire academic fabric, while leveraging a compelling set of science projects from a wide variety of disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2006679","FET: Small: Quantum-secure quantum-enhanced covert networks over generalized bosonic channels","CCF","FET-Fndtns of Emerging Tech, Information Technology Researc","10/01/2020","07/29/2021","Boulat Bash","AZ","University of Arizona","Standard Grant","Elizabeth Behrman","09/30/2024","$557,999.00","Saikat Guha","boulat@email.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","CSE","089Y00, 164000","7923, 7928, 9251","$0.00","Security and privacy are of utmost importance to communications, with standard cryptography-based approaches well studied in the literature.  However, these approaches are limited to protecting the content of transmissions, which is insufficient when the detection of the transmission must be prevented in the first place. Covert, or low probability of detection/intercept (LPD/LPI) communication is thus necessary in many settings, including military operations.  At the same time, quantum methodology has been demonstrated to benefit substantially the performance of non-covert communications.  This project focuses on quantifying the fundamental improvements to security and throughput of covert communication networks offered by quantum information processing, as well as characterizing the systems for achieving these gains. Additionally, the research team mentors graduate and undergraduate students involved in this project, and engages in collaborative efforts to validate the results experimentally.<br/> <br/>Covert communication has thus far been studied largely from the classical viewpoint.  This includes both the practical approaches to system design such as spread spectrum as well as the corresponding information-theoretic limits.  However, quantum, not classical, mechanics govern the fundamental laws of physics.  Leveraging quantum methodology has yielded encryption systems that are provably secure against the adversary restricted only by the laws of physics. Quantum phenomena such as entanglement have been shown to substantially improve the non-covert communication bitrate.  This project takes the quantum perspective to covertness.  It looks to determine the fundamental limits of covert communication in a networked setting that is secure against the adversary who is only subject to the laws of physics.  This assumption yields mathematically provable covertness against the most powerful adversary possible.  These limits are sought for communication under different channel conditions (including dynamic links due to platform mobility and time-varying channel conditions) and resources available to the communicating parties (including quantum processing of light).  Furthermore, transmitter and receiver structures and algorithms are being devised to achieve these limits.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232860","CC* Regional Computing: Taylor Geospatial Institute Regional AI Learning System","OAC","Campus Cyberinfrastructure","09/01/2022","08/30/2022","William Kramer","IL","University of Illinois at Urbana-Champaign","Standard Grant","Amy Apon","08/31/2024","$1,000,000.00","Shaowen Wang, Vasit Sagan","wtkramer@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","808000","","$0.00","The Taylor Geospatial Institute Regional AI Learning System (TGI RAILS) project provides a high-performance computing and data analysis system for use by the regional consortium of academic institutions that is centered around the St. Louis area and extends to states throughout the Midwest. The unifying focus of this consortium is the Taylor Geospatial Institute (TGI), a comprehensive and holistic geospatial research community of initially eight outstanding research institutions in the Midwest central region. TGI comprises partnerships with industry, government agencies, and research and educational entities to advance multiple disciplines including AI, data analytics and geospatial science with the targeted goals of producing significant societal benefits in the areas of food systems, health and social equality, smart and resilient communities, workforce development, national security, and economic development.<br/><br/>TGI RAILS provides critical, shared computing infrastructure that is otherwise lacking for the research and educational work of the consortium, dramatically reducing time to insight for challenging research investigations and providing staff, researchers, and students experience with the latest AI and data analytics technologies. TGI RAILS experience supports researchers in obtaining allocations on national computing resources such as ACCESS systems.<br/><br/>TGI RAILS addresses significant computational resource gaps at key under-resourced institutions, particularly those serving minoritized or non-traditional students and investigators. It prepares students with data science, computer science, statistics, research, and scientific communication skills to design experiments and communicate their findings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2117941","MRI: Acquisition of a GPU Cluster for Multi-Disciplinary Research and Education at University of Nevada, Las Vegas","OAC","Major Research Instrumentation, Information Technology Researc, EPSCoR Co-Funding","09/01/2021","09/07/2022","Mingon Kang","NV","University of Nevada Las Vegas","Standard Grant","Alejandro Suarez","08/31/2024","$432,269.00","","mingon.kang@unlv.edu","4505 S MARYLAND PKWY","LAS VEGAS","NV","891549900","7028951357","CSE","118900, 164000, 915000","1189, 9150","$0.00","The project funds the purchase and commissioning of a high-performance graphical processing unit (GPU) cluster at the University of Nevada, Las Vegas (UNLV). The project will address emergent and longer-term needs, challenges, and opportunities in research and educational efforts across multiple disciplines: biomedical research, intelligent transportation systems and automated vehicles, genomics, astronomy, and physics. The project will help advance national initiatives in big data, strategic computing, artificial intelligence, and smart infrastructure systems. These will integrate, synthesize, model, and visualize large volumes of data from various sources as well as develop and apply Artificial Intelligence (AI) techniques to assist decision making. For applied and basic research aspects of the project, the GPU cluster will leverage advances in computing hardware, software, sensor networks, and communications systems. Some elements of the project will address near-term societal needs to preserve and enhance the quality of living of individuals and families, support economic competitiveness and growth of businesses, and foster the vitality of communities. These include topics related to public health, transportation, environment, and energy. The project?s longer-term initiatives will address explorations and innovations in basic research in these domains as well as in astronomy, physics, and genomics. These activities will include partnerships with academia, government entities, and private sector organizations. The project will support curricular and co-curricular activities for undergraduate and graduate students to increase their interests in related education, research, and career opportunities. As a Minority-Serving Institution and Hispanic Serving Institution, this grant will help UNLV to significantly expand such opportunities for students from varied socio-economic and socio-demographic communities. Thus, an outcome of the project will be to help develop skilled work-forces from diverse backgrounds. <br/><br/>The GPU cluster will support basic and applied research, as well as educational programs across multiple disciplines. Common elements for the research efforts include integrating, synthesizing, modeling, and visualizing large volumes of data along with the development and application of various AI techniques to support decision making. Efforts in Biomedicine will be to stratify individuals at risk for benzodiazepine and opioid overdose using interpretable deep learning techniques using publicly available pluripotency transcription factors datasets. Activities related to intelligent transportation systems and automated vehicles will address comprehensive trajectory prediction challenges for near real-time applications on transportation networks to help accelerate the deployment of Connected Automated Vehicles and Infrastructure Systems; they will use data from various in-vehicle, on-roadway, and roadside sensors. Research in Genomics will be to better understand the evolution of novel transcription factor (TF) binding sites originating from endogenous retrovirus (ERV) integration. Astronomy related endeavors will be to estimate planet mass from protoplanetary disk images using Convolutional Neural Networks (CNN). Efforts in Physics will be to develop rotationally equivariant CNN to simulate and evaluate force fields at atomistic scales of materials. Educational aspects of the project will include curricular and co-curricular initiatives at the undergraduate and graduate levels to help alert, engage, excite, and motivate students to pursue education, research, and career opportunities in related fields. The project will include partnerships with public and private sector organizations and academia.<br/><br/>This project is jointly funded by the Major Research Instrumentation (MRI) program, the Established Program to Stimulate Competitive Research (EPSCoR), and the Computer & Information Science & Engineering (CISE) Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210344","Collaborative Research: NeTS: JUNO3: End-to-end network slicing and orchestration in future programmable converged wireless-optical networks","CNS","Information Technology Researc, Special Projects - CNS","09/01/2022","09/14/2023","Shih-Chun Lin","NC","North Carolina State University","Standard Grant","Ann Von Lehmen","08/31/2025","$256,207.00","","slin23@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","276950001","9195152444","CSE","164000, 171400","044Z, 7071","$0.00","Beyond 5G/6G and next-generation core networks promise a wide range of new applications and industrial verticals such as digital enterprise, Industry 4.0, and smart cities. Revolutionary technological breakthroughs in all domains, including access fronthaul, edge clouds, and core networks, are necessary to realize this unprecedented opportunity through timely end-to-end performance guarantees. Several challenges must be overcome in order to achieve this. Firstly, emerging applications often demand high availability and reliability for mission-critical purposes. Secondly, edge-to-edge traffic flows gradually dominate conventional cloud-to-user flows, transforming traffic patterns and profiles. Thirdly, the explosive growth of the Internet and core network traffic due to cloud services and the ?new normal? following the COVID-19 pandemic propel new programmable broadband data delivery. Lastly, seamless end-to-end networking and management are necessary to support data-intensive, latency-sensitive applications such as metaverse, digital twins, and remote surgery. This project seeks to address these challenges. <br/><br/>The objective of this joint US-Japan project is to develop critical enablers to realize the full spectrum of beyond 5G/6G applications. The specific aims are to: a) Design resilient fronthaul architectures based on novel power-over-fiber technology to provide wireless access survivability; b(Design automatic edge cloud management and service chain deployment by leveraging machine learning techniques and Markov decision processes; c)Design multi-petabit optical core networks and efficient access-edge-core orchestration and slicing for stringent service-level agreements; d) Prototype a wireless-optical network testbed and evaluate end-to-end harmonization performance.  This project brings unique research synergy of US and Japanese researchers and will lay the theoretical, algorithmic, and experimental foundation of end-to-end network slicing and orchestration to enable future programmable converged wireless-optical networks. The research outcomes will have overarching and transformative broader impacts on technology and society through the harmonization of optical networking, edge, and wireless domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2219736","IMR: MM-1C: Effective Geolocation of Internet Hosts","CNS","Information Technology Researc, Networking Technology and Syst","10/01/2022","07/29/2022","Michael Rabinovich","OH","Case Western Reserve University","Standard Grant","Deepankar Medhi","09/30/2025","$599,185.00","","michael.rabinovich@case.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","CSE","164000, 736300","115Z, 7363","$0.00","This project tackles the problem of determining the geographic location of Internet hosts from their Internet addresses (the ?geolocation problem?).  Geolocation is a fundamental problem in computer networking that affects numerous aspects of Internet operation, research, and development. Unfortunately, ample evidence shows that existing geolocation services are prone to errors and inaccuracies, especially when it comes to mapping content provider platforms. Beyond the obvious impact on applications relying on geolocation, these inaccuracies undermine the rigor of any research that utilizes geolocation.  This research will develop novel techniques that improve the efficiency and accuracy of geolocation and leverage these techniques to systematically assess the accuracy of influential existing geolocation services.  <br/><br/>Geolocation commonly involves measuring network delays from a set of ?landmarks? (the hosts with known locations) to the geolocation target.  By selecting ? differently for different targets -- a small number of landmarks from a large pool of candidates, this project attempts to break the dilemma where a large number of landmarks is desired for higher accuracy but leads to scalability limits due to high measurement volumes.  Through developing a new geolocation methodology, assessing if its new capabilities could make provably correct geolocation feasible for a substantial fraction of Internet hosts, and evaluating the accuracy of existing geolocation services, this activity will advance our knowledge in the area of Internet host geolocation, a fundamental problem in networking affecting numerous aspects of Internet development.<br/><br/>Advancing the state of the art in Internet host geolocation has broad impact through benefiting any applications and research that relies on geolocation. To the extent that it will show that provable geolocation is practical, this project will transform how networking research relying on geolocation is done, by allowing rigorous reasoning about host locations. Datasets containing active measurements, as well as software artifacts, produced in the course of this project will be made publicly available. The project will significantly enhance undergraduate and graduate education at CWRU and facilitate its ongoing efforts in broadening participation of underrepresented sections of the population in computer science.<br/><br/>All the data (anonymized when needed) and relevant software artifacts the project produces will be made available through the project website at http://engr.case.edu/rabinovich_michael/geolocation. The content itself will be stored at external public repositories such as github (github.com) and OSF (osf.io), while the project website will provide links to the content along with its description.  This will be kept available for at least 5 years after the project completion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2224565","SaTC: CORE: Small: When and from Whom Reminder-based Corrections of Everyday Misinformation Improve Memory and Belief Accuracy","CNS","Information Technology Researc, Special Projects - CNS, Secure &Trustworthy Cyberspace","10/01/2022","04/05/2023","Chris Wahlheim","NC","University of North Carolina Greensboro","Standard Grant","Sara Kiesler","09/30/2025","$458,000.00","","cnwahlhe@uncg.edu","1000 SPRING GARDEN ST","GREENSBORO","NC","274125068","3363345878","CSE","164000, 171400, 806000","025Z, 065Z, 7434, 7923, 9178, 9251","$0.00","Misinformation shared via news and social media platforms has negative consequences. For example, misinformation about safe and effective COVID vaccines has reduced intent to receive them. Websites have been designed to counteract misinformation by indicating contradictory information. But this approach may not always be effective because reminding people of misinformation can make it more believable. The project team is evaluating a reminding technique that recently improved memory and belief accuracy in internet news headlines in college students who were immediately tested. The project advances areas of social and behavioral sciences by revealing the mental processes that improve memory and belief accuracy for everyday news. This could benefit public health and national defense by establishing whether this implementable method counteracts health-based and other misinformation that can be propagated by foreign sources. This project supports the education of a diverse student body by being conducted at a university that emphasizes student research training and attracts students from many backgrounds. Both undergraduate and graduate students are training in theoretical and practical approaches to research and analysis that will prepare them for STEM careers.<br/><br/>The current project is examining whether benefits of misinformation reminders generalize to various situation in representative US samples. Specifically, the project is testing if the benefits extend from: presentation and test delays of minutes to days, news headlines with text and images to podcasts and news videos, and partisan-neutral to partisan-specific information sources varying in perceived credibility. Participants are first being exposed to real and fake internet news story headlines and then learning real news corrections that sometimes appear after participants are reminded of fake news. Participants? recall accuracy and beliefs in recalled details are then being tested. The findings will illuminate when, how, and from whom reminders of everyday internet-based misinformation effectively promote accurate memory and beliefs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2128594","Collaborative Research: SWIFT: Intelligent Dynamic Spectrum Access (IDEA): An Efficient Learning Approach to Enhancing Spectrum Utilization and Coexistence","ECCS","SWIFT-Spectrum Innov Futr Tech, Information Technology Researc","09/15/2022","09/18/2023","Lingjia Liu","VA","Virginia Polytechnic Institute and State University","Standard Grant","Huaiyu Dai","08/31/2025","$525,000.00","Yang Yi","ljliu@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","ENG","140Y00, 164000","044Z, 086Z, 103E, 153E, 7936","$0.00","With the growing importance of wireless connectivity for social and economic interaction, there have been rising demands for greater spectrum use by both primary and secondary active radios, amidst the critical requirement of quiet spectrum for scientific exploration by receiver-only passive systems. To improve the overall spectrum utilization of the wireless ecosystem, this project develops a holistic Intelligent Dynamic spEctrum Access (IDEA) framework that can substantially enhance the spectrum utilization, energy-efficiency, and coexistence capability of spectrum sharing networks. In IDEA, enabling technical innovations across multiple disciplines are synergistically developed, including neuromorphic design of energy-efficient computing hardware at the device and circuit level, and artificial intelligence for spectrum sensing and dynamic access at the network level. The spectrum and interference management in IDEA conscientiously treats the coexistence constraints imposed by passive services, in support of scientific and societal returns from remote sensing investments. The outcomes of this research are expected to broadly impact next-generation wireless networks and Internet of Things applications with high traffic demands, such as autonomous driving, smart cities and remote sensing.<br/><br/>The goal of this project is to develop an intelligent dynamic spectrum access framework with unprecedented spectrum utilization efficiency and agility to support spectrum coexistence. The developed IDEA network platform supports heterogeneous devices from both primary and secondary active radios as well as passive radios. Key technical innovations are developed across the network to substantially enhance system-level spectrum utilization and active-passive radio coexistence. Specifically, analog/mixed-signal spiking neural network (SNN)-based neuromorphic computing hardware is designed to provide on-board intelligence at ultra-low power for resource-constrained secondary active radios. Model-free deep reinforcement learning is integrated with wireless domain knowledge and the SNN platform to accelerate learning-based spectrum access and coexistence. Advanced spectrum monitoring techniques are developed to quickly detect and characterize various signal emitters in both active and passive services. Finally, software and hardware testbeds are developed for system level evaluation and tradeoff optimization. The IDEA framework not only empowers efficient spectrum access in highly dynamic wireless environments, but also facilitates holistic system design and optimization across devices and circuits, sensing and communications, and networking.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2319486","Collaborative Research: IMR: MM-1B: Privacy-Preserving Data Sharing for Mobile Internet Measurement and Traffic Analytics","CNS","Information Technology Researc, CISE Research Resources, Networking Technology and Syst","10/01/2023","09/06/2023","Yi Qian","NE","University of Nebraska-Lincoln","Continuing Grant","Marilyn McClure","09/30/2026","$170,000.00","","yqian2@unl.edu","2200 VINE ST","LINCOLN","NE","685032427","4024723171","CSE","164000, 289000, 736300","115Z, 7363, 9150","$0.00","Mobile Internet measurement is critical to network design, resource allocation, and troubleshooting network issues. However, sharing of mobile Internet measurement data can potentially compromise user privacy. Given the wide introduction of artificial intelligence to mobile Internet measurement and traffic analytics, there is an urgent need for data sharing solutions that provide explainability in terms of the trade-offs among data quality, utility and quantity. To close the gap, the objective of this project is to develop new methods to augment data with explainable data quality and utility, to access and share collected data in a privacy-preserving manner, and to collaboratively analyze Internet data with intelligence and autonomy.<br/><br/>This collaborative project brings together investigators from University of Nebraska-Lincoln, Utah State University, and University of Wisconsin-Madison. It aims to lay a solid foundation for mobile Internet measurement with privacy preservation, collaborative and distributed intelligence, and autonomy. Methodologies and methods will be developed for quality-explainable data synthesis and augmentation; privacy-preserving data sharing; and collaborative and privacy-preserving analysis of Internet measurement data. Moreover, a mobile Internet traffic generator will be developed for evaluating the proposed methods. This project can significantly advance the prior research in Internet traffic analytics, quality-explainable and privacy-preserving data processing, mobile Internet traffic analytics, distributed artificial intelligence and machine learning algorithms, optimizations, modeling, simulations, and testbed experiments. <br/><br/>The research efforts associated with this project will greatly advance the understandings of the critical issues in the next-generation mobile Internet measurement with distributed and collaborative intelligence to provide privacy-preserving data sharing and Internet traffic analytics. The outcomes of the project can potently foster the transition of our society into data sharing with privacy and intelligent era. Research and education will be integrated in this project by introducing emerging mobile Internet measurement and privacy-preserving data processing with advanced topics such as 6G wireless systems, data augmentation, artificial intelligence and machine learning models into the current curricula in the three collaborative institutions.<br/><br/>The project website is hosted at: cns.unl.edu/imr-ppds. The collected data, simulation codes, and publication list will be published on the project website. Copies of technical reports and accepted manuscripts will also be published on the project website. The website will be maintained during the project years, and remain accessible for least 2 years after the completion of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1647182","US Ignite: Collaborative Research: Focus Area 2: Resilient Virtual Path Management for Scalable Data-intensive Computing at Network-Edges","CNS","Information Technology Researc, CISE Research Resources","01/01/2017","05/31/2023","Prasad Calyam","MO","University of Missouri-Columbia","Standard Grant","Deepankar Medhi","09/30/2024","$800,053.00","Kannappan Palaniappan","calyamp@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","CSE","164000, 289000","015Z, 044Z, 9150","$0.00","This project addresses bootstrapping and managing virtual path networks hosting Visual Cloud Computing (VCC) applications for public safety during disaster incidents.  This project meets the needs of incident-supporting VCC applications that are essential for first responders to gain rapid visual situational awareness through: (a) scalable processing of imagery/video data collected at incident scenes, and (b) use of thin-client technologies for collaboration. Targeted demonstrations will also be conducted in collaboration with first-responder agencies such as trauma care hospitals (with theater-scale use cases) and a county fire protection district (with regional-scale use cases).  Specifically, the project objectives are to: (i) investigate virtual path management algorithms and large-scale simulations with the aim of providing resilient VCC applications with guaranteed performance even in the presence of host/link failures, host mobility and network partitioning; and (ii) demonstrate resilient resource provisioning for such path management algorithms within hybrid fog-cloud infrastructures on experimental testbeds such as GENI and CloudLab.<br/><br/>The salient theoretical contribution of this project lays in the design of novel distributed and federated algorithms for the virtual path embedding and computation placement problems, for real-time VCC applications. In particular, the project seeks a solution improvement for constrained path finders schemes, e.g., virtual network embedding or traffic steering, in presence of severe host and link failures, host mobility and network partitioning. The work also advances knowledge of application-aware network design through a scheme that allows fog-cloud compute location selection tradeoffs. In particular, the approach is based on decoupling small/large instances of visual data processing in the context of latency-critical VCC applications at:  (a) ""theater-scale"" for first-responder and incident commander personnel co-ordination at an incident scene, and (b) ""regional-scale"" for tracking objects of interest across different geographical scales in wide-area motion imagery from airborne platforms."
"2331094","Travel: WINS Travel Funds in Support of SCinet at SC23","OAC","Campus Cyberinfrastructure","07/01/2023","06/05/2023","Marla Meehl","CO","University Corporation For Atmospheric Res","Standard Grant","Amy Apon","06/30/2024","$49,830.00","Brenna Meade","marla@ucar.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","CSE","808000","7556, 9102","$0.00","This project supports six WINS volunteers to participate in SCinet activities in support of the 2023 Supercomputing Conference (SC23) scheduled November 12 - 17, 2023 in Denver, CO.   Volunteers will gain hands on experience in the ground-up construction of the conference network. WINS strives to have a diverse representation of organizations and applicant backgrounds in the group selected each year, with the 2023 group expected to be the most diverse since the start of the program, including many participants from underrepresented groups.<br/><br/>WINS volunteers will participate in SCinet, one of the fastest and most advanced computer networks in the world which supports the annual Supercomputing Conference (SC).  Expected outcomes include WINS SC23 volunteers developing new technical skills, gaining exposure to technologies or equipment outside their standard work environment, adding to their professional network, improving collaboration and communication skills, applying the skills developed during SCinet at their home institution, and access to future professional development opportunities.   The travel funded by the proposal will continue the broadening and expansion of knowledge and training for network engineers and information technology professionals who desire to build expertise and continue their education and careers in network and computer systems.  Through this program, the National Science Foundation?s mission is strengthened by opening more training and career opportunities for a diverse workforce.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2228983","Collaborative Research: HCC: Small: Toolkits for Creating Interaction-powered Energy-aware Computing Systems","IIS","Information Technology Researc","01/01/2023","09/08/2022","Josiah Hester","GA","Georgia Tech Research Corporation","Standard Grant","Dan Cosley","12/31/2025","$250,000.00","","josiah@gatech.edu","926 DALNEY ST NW","ATLANTA","GA","303186395","4048944819","CSE","164000","7367, 7923","$0.00","The explosion in the number of ?smart? computing devices has led to the need to design power solutions that reduce their ecological footprint and electronic waste. One possible answer is ?self-sustaining? systems that harvest power from electromagnetic, solar, and other sources, reducing the need for batteries and external charging. This project looks at a novel kind of self-sustaining power source, one where systems draw energy from how people use them, for instance through button clicks or motion of the device. These user interactions are a promising source of power, but require careful management on the part of device designers since both the timing and types of interactions can be unpredictable and must be designed primarily to meet users? needs rather than power goals. The research team seeks to deepen the understanding of information and resource needs of novice designers of self-sustaining systems and to create tools that help developers manage those needs. This in turn will improve sustainability in the design of computing systems, with the goal of developing power sources and development methods that generalize to a wide set of device design contexts. <br/><br/>The project is structured around three research thrusts. The first is to better understand user needs, through a combination of observations of novices and experts programming self-sustaining systems, interviews, and surveys. The second is to create modules to harvest energy from interactions, providing developers with profiling tools and composable mechanisms enabled by a generic backbone system. The third is to develop toolkits that link the first two thrusts: supporting the development, deployment, and assessment of self-sustaining computing systems by providing guidance to developers on harvester selection, mechanism design, energy profiling, debugging, and beyond. These three research thrusts will be complemented by a comprehensive evaluation with a series of technical validations and user studies. This project will adopt common qualitative and quantitative evaluation techniques including A/B tests, Likert-scale questionnaires, semi-structured interviews, experimenters? observations, walk-through demonstrations, evaluation through demonstrations, and pair programming. Overall, this research effort will guide the creation of tools to support future developments of self-sustaining computing systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835661","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","XC-Crosscutting Activities Pro, Data Cyberinfrastructure","01/01/2019","08/27/2018","Diego Melgar","OR","University of Oregon Eugene","Standard Grant","Marlon Pierce","09/30/2024","$405,494.00","","dmelgarm@uoregon.edu","1776 E 13TH AVE","EUGENE","OR","974031905","5413465131","CSE","722200, 772600","062Z, 077Z, 7925","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835677","Collaborative Research: Framework: Data: HDR: Nanocomposites to Metamaterials: A Knowledge Graph Framework","OAC","DMR SHORT TERM SUPPORT, Data Cyberinfrastructure, Software Institutes","11/01/2018","05/07/2021","Lynda Brinson","NC","Duke University","Standard Grant","Alejandro Suarez","10/31/2024","$2,606,810.00","Cynthia Rudin","cate.brinson@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","CSE","171200, 772600, 800400","026Z, 054Z, 062Z, 077Z, 7925, 9102, 9251","$0.00","A team of experts from four universities (Duke, RPI, Caltech and Northwestern) creates an open source data resource for the polymer nanocomposites and metamaterials communities.  A broad spectrum of users will be able to query the system, identify materials that may have certain characteristics, and automatically produce information about these materials.  The new capability (MetaMine) is based on previous work by the research team in nanomaterials (NanoMine).  The effort focuses upon two significant domain problems: discovery of factors controlling the dissipation peak in nanocomposites, and tailored mechanical response in metamaterials motivated by an application to personalize running shoes.  The project will significantly improve the representation of data and the robustness with which user communities can identify promising materials applications.   By expanding interaction of the nanocomposite and metamaterials communities with curated data resources, the project enables new collaborations in materials discovery and design.  Strong connections with the National Institute of Standards and Technology (NIST), the Air Force Research Laboratory (AFRL), and Lockheed Martin facilitate industry and government use of the resulting knowledge base. <br/><br/>The project develops an open source Materials Knowledge Graph (MKG) framework.  The framework for materials includes extensible semantic infrastructure, customizable user templates, semi-automatic curation tools, ontology-enabled design tools and custom user dashboards.  The work generalizes a prototype data resource (NanoMine) previously developed by the researchers, and demonstrates the extensibility of this framework to metamaterials.  NanoMine enables annotation, organization and data storage on a wide variety of nanocomposite samples, including information on composition, processing, microstructure and properties.  The extensibility will be demonstrated through creation of a MetaMine module for metamaterials, parallel to the NanoMine module for nanocomposites.  The frameworks will allow for curation of data sets and end-user discovery of processing-structure-property relationships.  The work supports the Materials Genome Initiative by creating an extensible data ecosystem to share and re-use materials data, enabling faster development of materials via robust testing of models and application of analysis tools.  The capability will be compatible with the NIST Material Data Curator System, and the team also engages both AFRL and Lockheed Martin to facilitate industry and government use of the resulting knowledge base. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Materials Research within the NSF Directorate for Mathematical and Physical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322274","CC* Regional Networking: Regional Collaboration - Precision Agriculture - Yuma AZ","OAC","Campus Cyberinfrastructure","09/15/2023","11/14/2023","Derek Masseth","AZ","Arizona State University","Standard Grant","Kevin Thompson","08/31/2025","$1,135,788.00","Matthew Rahr, Stephanie Slinski, Paul Brierley","dmasseth@suncorridor.org","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","CSE","808000","","$0.00","Yuma agriculture uses 10,000 acres of irrigated farmland, water diverted from the Colorado River, and a year-round growing season to produce over 90% of North America?s winter vegetables. The use of information technologies in farming and agribusiness is growing and expanding the necessary discovery to improve yields and align production with changing environmental conditions.  The use of field data collected, monitored, and analyzed allows producers to plan and assess crop stability and growth.  The expansion of cyberinfrastructure (CI) systems and resources enables modern farming to collect, analyze, and share soil, crop, and environmental data.  The CC* Regional Network project delivers high speed external connectivity, expands wireless networks and internet of things (IOT) systems to research farms in one of the most important agricultural communities in the country.<br/><br/>The introduction of specialized and novel networking in new collaborations with higher education research in the region improves the opportunity to plan and implement broadband and networking aligned with the science of agriculture. The use of technology to improve crop yields and minimize natural resource impacts, or precision agriculture, is a necessary step forward that requires the deployment of cyberinfrastructure on research farms, in rural communities, and in an extended collaboration between higher education, the Yuma agricultural research community with Yuma broadband providers and farmers.<br/><br/>The project enables participation and informs regional cyberinfrastructure capacity building by Arizona?s research and education network, the Sun Corridor Network (SCN), Arizona State University, Tempe, Arizona as a research service facilitator in this community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835785","Framework: Software: Collaborative Research: CyberWater--An open and sustainable framework for diverse data and model integration with provenance and access to HPC","OAC","XC-Crosscutting Activities Pro, Data Cyberinfrastructure, Software Institutes, EarthCube","01/01/2019","05/19/2021","Xu Liang","PA","University of Pittsburgh","Standard Grant","Ashok Srinivasan","12/31/2024","$453,232.00","","xuliang@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","CSE","722200, 772600, 800400, 807400","062Z, 077Z, 7925, 8004, 9251","$0.00","This project addresses a high priority need for water research communities: interoperability among a wide variety of data sources and models, and integration of different computational models into water research communities.  The project will develop an open and sustainable software framework enabling integration of hydrologic data and models for interdisciplinary teamwork and discovery.   The models and datasets cover fields such as hydrology, biology, environmental engineering and climate.  The project also addresses one of the key issues for extreme-scale computing:  scalable file systems.  The collaboration draws upon computing, modeling, and hydrology expertise at six institutions: University of Pittsburgh, University of Iowa, Ball State University, North Carolina State University, Indiana University, and the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI).  <br/><br/>The project develops CyberWater, a community-driven software framework that integrates a wide range of models and datasets across disparate temporal and spatial scales. The CyberWater framework allows scientists to bypass challenges associated with model and dataset complexity.  The project designs a model agent tool enabling users to generate model agents for common model types without coding, and integrates multiple existing software codes/elements that provide for broad-scale use.  To develop such a diverse modeling framework, the project brings together hydrologists, climate experts, meteorologists, computer scientists and cyberinfrastructure experts.  The project builds upon an existing prototype developed by the lead investigator;  basic elements for the system were developed, consisting of plugged-in models and data sources with corresponding agents and a workflow engine allowing user workflow control.  The prototype was successfully demonstrated for two models, making use of datasets plugged in from NASA, USGS and CUAHSI.  For the current project, new models and datasets are added to the framework; the ability to use high performance computing resources is also incorporated.  The team will use the CUAHSI HydroShare System to distribute CyberWater software and its associate model agents, including instructions on how to establish a local CyberWater environment, models and model agents. The project will enable substantial scientific advances for water related issues, and the solution can be applied to other research disciplines. <br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the NSF Directorate for Geosciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2219975","Collaborative Research: Framework: Data: NSCI: HDR: GeoSCIFramework: Scalable Real-Time Streaming Analytics and Machine Learning for Geoscience and Hazards Research","OAC","Data Cyberinfrastructure, EarthCube, Big Data Science &Engineering","10/01/2021","03/15/2022","Ivan Rodero","UT","University of Utah","Standard Grant","Marlon Pierce","08/31/2024","$803,971.00","","ivan.rodero@utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","CSE","772600, 807400, 808300","062Z, 077Z, 7925, 8083","$0.00","This project develops a real-time processing system capable of handling a large mix of sensor observations. The focus of this system is automation of the detection of natural hazard events using machine learning, as the events are occurring.  A four-organization collaboration (UNAVCO, University of Colorado, University of Oregon, and Rutgers University) develops a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  This work will support rapid analysis and understanding of data associated with hazardous events (earthquakes, volcanic eruptions, tsunamis).  <br/><br/>This project uses a collaboration between computer scientists and geoscientists to develop a data framework for generalized real-time streaming analytics and machine learning for geoscience and hazards research.  It focuses on the aggregation and integration of a large number of data streams into a coherent system that supports analysis of the data streams in real-time. The framework will offer machine-learning-based tools designed to detect signals of events, such as earthquakes and tsunamis, that might only be detectable when looking at a broad selection of observational inputs.  The architecture sets up a fast data pipeline by combining a group of open source components that make big data applications viable and easier to develop. Data sources for the project draw primarily upon the 1500+ sensors from the EarthScope networks currently managed by UNAVCO and the Incorporated Research Institutions for Seismology (IRIS), as well as the Ocean Observatories Initiative (OOI) cabled array data managed by Rutgers University.  Machine learning (ML) algorithms will be researched and applied to the tsunami and earthquake use cases.  Initially, the project plans to employ an advanced convolutional neural network method in a multi-data environment.  The method has only been applied to seismic waveforms, so the project will explore extending the method to a multi-data environment.  The approach is expected to be extensible beyond detection and characterization of earthquakes to include the onset of other geophysical signals such as slow-slip events or magmatic intrusion, expanding the potential for new scientific discoveries.  The framework is applied to use cases in the Cascadia subduction zone and Yellowstone: these locations combine the expertise of the science team with locations where EarthScope and OOI have the greatest concentration of instruments.  The architecture will be transportable and scalable, running in a Docker environment on laptops, local clusters and the cloud.  Integral to the project will be development, documentation and training using collaborative online resources such as GitLab and Jupyter Notebooks, and utilizing NSF XSEDE resources to make larger datasets and computational resources more widely available.<br/><br/>This award by the NSF Office of Advanced Cyberinfrastructure is jointly supported by the Cross-Cutting Program and Division of Earth Sciences within the NSF Directorate for Geosciences, the Big Data Science and Engineering Program within the Directorate for Computer and Information Science and Engineering, and the EarthCube Program jointly sponsored by the NSF Directorate for Geosciences and the Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126319","CC* Regional: A Purpose-built SoCal Science DMZ for Catalyzing Scientific Research Collaborations","OAC","Campus Cyberinfrastructure","10/01/2021","08/19/2021","Carl Kesselman","CA","University of Southern California","Standard Grant","Kevin Thompson","09/30/2024","$999,777.00","Byoung-Do Kim, Yul Pyun","carl@isi.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900890701","2137407762","CSE","808000","","$0.00","The Los Nettos Regional Network is a long-standing regional research and education (R&E) network with a history of supporting science and engineering research for its more than 30 members and associates in the greater Los Angeles area. This project builds a friction-free regional Science DMZ network across multiple Southern California college campuses, catalyzing collaborative research capabilities at the institutions. The project establishes the network infrastructure and software necessary to facilitate high speed transfers of large-scale research data for regional and national scientific collaborations. The campuses included in the network are Loyola Marymount University, Occidental College, and The Claremont Colleges consortium, which consists of Claremont Graduate University, Claremont McKenna College, Harvey Mudd College, Keck Graduate Institute, Pitzer College, Pomona College, and Scripps College. <br/><br/>This purpose-built science network is specially customized for each institution?s unique needs and follows the well-known Science DMZ guidelines established by ESnet. The new network interconnects with state, national, and international networks, such as CENIC?s California Research and Education Network (CalREN), Internet2, and Pacific Wave. Many projects in various science domains benefit from the significant network capacity increase that this project supports. Coordinated activities at the regional level, including technical training for administrators and researchers at each campus, ensure uniform standards are maintained. This scalable R&E network can be expanded in the future for researchers and students at other smaller regional institutions (e.g., Charles R. Drew University of Medicine and Science, ArtCenter College of Design) as their need for collaboration widens.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019220","CC* Compute: A Customizable, Reproducible, and Secure Cloud Infrastructure as a Service for Scientific Research in Southern California","OAC","Campus Cyberinfrastructure","09/01/2020","10/20/2020","Carl Kesselman","CA","University of Southern California","Standard Grant","Amy Apon","08/31/2024","$399,800.00","Byoung-Do Kim","carl@isi.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900890701","2137407762","CSE","808000","","$0.00","________________________________________________________________________________________________________________<br/><br/><br/>This project creates a hybrid cloud infrastructure as a scientific computing gateway that promotes and supports inter-disciplinary, multi-institutional research in science, engineering, biomedicine, and the social sciences. The hybrid cloud platform also promotes regional and national research collaboration, as a portion of the resources is integrated into the Open Science Grid (OSG). Many institutes with multi-institutional research projects headquartered at University of Southern California (USC), along with their regional and national collaborators, benefit from use of the OSG. It also extends the impact of their research outcomes and the projects themselves, as the system offers various ways to share research outputs and knowledge with external collaborators. The planned support for regional universities and integration with OSG increases opportunities to serve a broader community.<br/><br/>A broad research community is supported by this system by providing access to public and private cloud services as well as local high performance computing (HPC) and data resources. The design of the hybrid cloud system facilitates the creation of customizable, virtualized platforms and reproducible, container-based application services that enable multi-dimensional computing and data solutions. Researchers are able to pick and choose from a standard service catalogue to build pre-defined virtual machines and containerized applications, or, if necessary, create their own specialized environments. Along with built-in security, reproducible service modules, and the capability of creating and sharing customized environments, the hybrid cloud system bridges multi-disciplinary research domains and enhances the usability of advanced cyberinfrastructure for improved research productivity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2211982","Collaborative Research: SHF: MEDIUM: Smart Integrated Tuning of Parallel Code for Multicore and Manycore Systems","CCF","Information Technology Researc, Software & Hardware Foundation","10/01/2022","09/13/2023","Ali Jannesari","IA","Iowa State University","Continuing Grant","Almadena Chtchelkanova","09/30/2025","$502,318.00","","jannesar@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","CSE","164000, 779800","7924, 7942, 9150","$0.00","High Performance Computing (HPC) entails executing code on multicore and manycore architectures. To better utilize multicore/manycore architectures, parallel programming models have emerged. But often using these parallel models naively will not be able to scratch the surface of the potential performance gains such systems can provide. A common technique for improving performance is to add more hardware resources. However, this is expensive and system integration is usually an onerous task. To this end, the investigators propose a framework of improving performance by better utilization of the available resource and identifying near-optimal configuration. These configurations can take the form of code optimizations, as well as intelligent resource mapping and utilization. Specifically, this project is concerned with identifying code optimizations and runtime configurations that can potentially speed up executions manifold. Faster executions can also implicitly lead to reduced power consumption. Additionally, for situations where existing execution performance is acceptable, the proposed approach can also be extended to optimize for other performance metrics such as power. Power consumption is usually a huge bottleneck for HPC systems, and is a source of concern for organizations that deploy such systems; these concerns are both fiscal and environmental. The investigators posit that the framework outlined in this project can also be extended to optimize for power consumption without compromising execution performance.<br/><br/><br/>The investigators? aim is to provide such an AI-assisted framework that can automatically configure parallel code considering the underlying hardware architecture. The steps necessary to build such a framework lie at the convergence of compiler technologies, performance analysis and modeling, and deep learning. A primary driver of this project will be developing a program representation technique targeted towards parallel code. Existing representations target mostly serial code and cannot fully encapsulate the interactions and complexities of parallel code. Such a code representation technique is highly suited to analyses using deep learning. A means of representing parallel code in a machine learning friendly format will be very beneficial to the overall program analysis community. The proposed code representation will take the form of a graph, in order to correctly typify the inherent structure present in code. The investigators propose modeling this code representation using state-of-the-art Graph Neural Network (GNN) techniques. The modeled embeddings will be used in conjunction with task specific features in order to identify near optimum configurations for improved performance. The overall scale of this project will span the entire ?source code to execution? pipeline that most HPC workloads follow. The aim of this project is to optimize each optimizable step in the pipeline. A sample optimization pipeline can take the following form: given a parallel code, our GNN-based code optimization model will predict the best optimizations for the given code, followed by identifying the best device (CPU, GPU, and others) for executing the optimized code. Further downstream, our framework will identify the optimum runtime configurations appropriate for the device under consideration. The ideas presented in this project can have the potential effect of increased hardware utilization and reduced future hardware commissioning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2139091","Science for Judges - Development of the Reference Manual on Scientific Evidence, 4th Edition","SES","Law & Science, EngEd-Engineering Education, Information Technology Researc, PROJECTS, IIS Special Projects, Discovery Research K-12, Systems and Synthetic Biology, Secure &Trustworthy Cyberspace","09/01/2021","09/16/2022","Anne-Marie Mazza","DC","National Academy of Sciences","Standard Grant","Reginald Sheehan","08/31/2024","$874,752.00","","amazza@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","SBE","128Y00, 134000, 164000, 197800, 748400, 764500, 801100, 806000","025Z, 098Z, 7465, 8819","$0.00","The Reference Manual on Scientific Evidence is a primary reference source for federal judges on questions of <br/>science in litigation. It is used not only by federal judges, but by state judges, attorneys, legal scholars, and <br/>law students. Its utility and widespread use is the result of the fair and balanced presentation it provides of <br/>fundamental principles of scientific methodology in areas likely to arise in expert testimony. Ten years have <br/>passed since the publication of the last (third) edition of the Reference Manual, and many of the chapters need <br/>updating to account for recent developments in science and law. For example, the chapter on DNA <br/>identification needs updating to take into account new genetic testing techniques and methodologies, andthe <br/>chapter on neuroscience must take account of new approaches for detecting neural activity. A new edition will <br/>also need to address emerging areas of science that have become important in litigation in recent years. The <br/>Committee on Science, Technology, and Law of the National Academies of Sciences, Engineering, and <br/>Medicine, in collaboration with the Federal Judicial Center (FJC) ? the research and education agency of the <br/>judicial branch of the U.S. government ? have agreed to pursue the development of a fourth edition of the <br/>Reference Manual. The manual ultimately benefits society by increasing the rigor with which the judiciary <br/>evaluates scientific evidence.<br/><br/>A committee of 10-12 members representing both the legal and scientific/ engineering/ medical communities <br/>will be convened to develop new edition of the manual with input from the FJC. The new edition will include <br/>updates of chapters from the previous edition of the Reference Manual as well as new chapters that reflect <br/>emerging areas of science, technology, and medicine relevant to the courts. The committee will select the <br/>topics to be included in the manual, commission expert authors to revise the current chapters or draft new <br/>ones, review all draft chapters, approve the chapters for external review, submit the manual for external <br/>review, and oversee chapter revisions as necessitated by external review comments.<br/><br/>This project was co-funded by the following NSF Divisions: Social and Economic Sciences; Chemistry; Information and Intelligent Systems; Computer and Network Systems; ITR/CISE Information Technology Research; Engineering Education and Centers; Molecular and Cellular Biology; Research in Learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835543","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","Data Cyberinfrastructure","09/01/2018","08/09/2018","Noah Fahlgren","MO","Donald Danforth Plant Science Center","Standard Grant","Alejandro Suarez","08/31/2024","$592,999.00","","nfahlgren@danforthcenter.org","975 N WARSON RD","SAINT LOUIS","MO","631322918","3145871285","CSE","772600","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126308","CC* CIRA: Southwest Higher Education Knowledge and Technology Exchange (SHEKATE)","OAC","Campus Cyberinfrastructure","10/01/2021","07/09/2021","Lev Gonick","AZ","Arizona State University","Standard Grant","Kevin Thompson","09/30/2024","$197,470.00","Derek Masseth","lev.gonick@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","CSE","808000","","$0.00","The COVID-19 pandemic exposed the deficiencies of long haul, middle mile, and last mile broadband connectivity in the West.  Long distances, sparse populations, and isolated tribal communities characterized new challenges for the higher education community.  An influx of new federal funding and state allocations from the CARES Act released new opportunities to plan, build, and deliver a new foundation for cyberinfrastructure.  Whereas planning and collaboration were needed and required in a pre-pandemic environment, the demand for regional-scale thinking and planning by leadership organizations is now an essential element in rebuilding the mountain west.  As a regional, multi-state, multi-institution collaboration, SHEKATE is prepared to engage, plan, and implement the new cyberinfrastructure that supports and improves science in a place where connectivity is expensive and isolated from researchers, and where regional CIOs and state research and networking collaborations work collectively to solve regional-scale research problems.<br/> <br/>SHEKATE?s collaborative planning events introduce and connect researchers from the region?s research universities to cross state and international boundaries and to work with federal and state broadband initiatives to establish a new research core across the region. Led by Arizona State University, the Sun Corridor Network, and the Utah Education and Telehealth Network as planning leads, SHEKATE conference events focus on researcher enabled urban biometrics, artificial intelligence, broadband in the West, cyberinfrastructure sustainability, and the underlying organizational structures that inform a new approach to building CI in one of the most complex urban and rural regions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2008116","CHS: Small: Collaborative Research: Learning Maker Skills By Building Game Props","IIS","Information Technology Researc, HCC-Human-Centered Computing, IIS Special Projects","10/01/2020","03/20/2023","Stefanie Mueller","MA","Massachusetts Institute of Technology","Standard Grant","Dan Cosley","09/30/2024","$417,923.00","","stefanie.mueller@MIT.EDU","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","CSE","164000, 736700, 748400","7367, 7923, 9251, CL10","$0.00","Fabrication devices, such as laser cutters and 3D printers, enable ordinary people to fabricate their own physical objects. These tools are driving innovation and entrepreneurship. However, rising availability of fabrication devices does not correlate to a rise in education. Many designs that makers are creating suffer from practicality, durability, and reliability issues. One promising group for addressing this problem is young learners who have the potential to become the next generation of engineers and innovators. This project explores teaching skills for fabrication to high school and undergraduate learners through games. Recent advances in virtual-physical game play enable players to use physical props in a game. For instance, a physical fishing rod made from cardboard and sensors is cast by the player to acquire virtual fish in the game. While today these physical props are used to increase immersion, they are not yet used for teaching. With the funding from this award, the team of researchers investigate how the act of building a physical game prop can be used to teach players fabrication, electronics, and programming skills. This research will provide insights into what skills can be effectively taught through games and how designers can leverage game mechanisms to create immersive game play to facilitate learning. The project will develop and release as open source a suite of games for acquiring fabrication skills. It will engage diverse high school students, undergraduate students, and fabrication lab and makerspace communities as co-developers and users of the educational games. As games are developed, the researchers will use their network within the fabrication and educational communities to publicize and integrate the games into additional environments, from schools to summer camps to community organizations. <br/> <br/>By combining methods from the learning sciences, educational game design, human-computer interaction, and design-based research literature, this research will conduct an iterative design process to investigate: (1) knowledge maps connecting skills and concepts that can be integrated with game mechanics to teach new fabrication tasks, through conducting interviews, contextual inquiries, and landscape analyses; (2) theory and principles for designing and evaluating maker-learning games, including mechanisms for recording user interaction with the game, such as click analysis, time between tasks, and in-game user feedback; (3) implications for evaluating learners? increases in knowledge and experiences, through mixed methods user studies that compare pre- and post-training learning gain and result in qualitative and quantitative findings; and (4) a game-fabrication infrastructure to facilitate the transfer of activities from one game to another and broaden the applicability of the maker-learning concept.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107313","Collaborative Research: RI: Medium: Thermal Computational Imaging","IIS","Information Technology Researc, Robust Intelligence","10/01/2021","07/28/2022","Ashok Veeraraghavan","TX","William Marsh Rice University","Standard Grant","Jie Yang","09/30/2024","$615,994.00","","Ashok.Veeraraghavan@gmail.com","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","164000, 749500","120Z, 7495, 7924, 9251","$0.00","Image sensors for thermal wavebands of light are usually noisy and subject to nuisance variations. Yet, it is indisputable that thermal imaging has the potential to spur numerous scientific and engineering applications that can fundamentally transform our society. For example, autonomous vehicles equipped with thermal cameras can navigate in the dark and through fog and rain. The widespread availability of thermal imaging technology could usher in methods for non-intrusive vital sign monitoring in public spaces, thereby providing a new tool in our public health arsenal, as well as enable environmental and ecosystem monitoring at both local and global scales. This project seeks to enable such applications with inexpensive but noisy thermal sensors. The research advances made in this progress will be integrated with an educational and outreach program that includes creating new undergraduate and graduate courses, engaging undergraduates in research, and engaging with K-12 communities.<br/><br/>The focus of this research is to advance thermal scene understanding by developing foundational tools for rendering, modeling and imaging at thermal wavebands. The project will develop a physically accurate rendering pipeline for thermal wavebands, that incorporates wavelength-dependent thermal emissivity of complex surfaces, thermal propagation in atmosphere, and accurate sensor modeling and noise characterization. This modeling will be used to design and develop novel computational imaging systems that allow capture of multi-dimensional thermal signals, such as multi-spectral and polarization measurements. The research will also develop a differentiable renderer and exploit it within an end-to-end learning framework for inverse graphics. This will allow for significant improvements in problems such as denoising, super-resolution, stereo, object detection and others. The advances in research will be used to explore and advance autonomous navigation, remote health monitoring and remote environmental monitoring.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2129515","RAPID: Leading Practices for Improving Accessibility and Inclusion in Field and Laboratory Science ? A Conversation Series","RISE","RSCH EXPER FOR UNDERGRAD SITES, Cross-Directorate  Activities, Information Technology Researc, GOLD-GEO Opps LeadersDiversity","07/01/2021","06/28/2021","Deborah Glickson","DC","National Academy of Sciences","Standard Grant","Brandon Jones","06/30/2024","$187,214.00","","dglickson@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","GEO","113900, 139700, 164000, 178Y00","096Z, 7914","$0.00","The National Academies of Sciences, Engineering, and Medicine (NASEM) will convene a series of webinars that explore leading practices for improving accessibility for people with disabilities in scientific disciplines that conduct field and laboratory research. The COVID-19 pandemic has provided a valuable perspective to discuss the changing landscape of opportunity in this area. The webinars will explore strategies for increasing accessibility for field and laboratory research in the physical, social, and life sciences, including: current barriers to full participation, accommodations that might assist with accessibility and inclusion, similarities and differences between fieldwork and laboratory research solutions, the role of mentoring and other support networks, how individual successes could translate into more generalizable practices, and the institutional policies and cultural change needed to ensure long-term change. This activity will encourage greater collaboration and coordination among members of the scientific community, with the goal of institutional and cultural change, and build upon the knowledge base that has been acquired since the beginning of the pandemic. <br/><br/>The National Academies of Sciences, Engineering, and Medicine (NASEM) will organize and convene a series of virtual conversation-style webinars that explore leading practices for improving accessibility for people with disabilities in disciplines that conduct field and laboratory research. The COVID-19 pandemic provides a valuable perspective to discuss the changing landscape of opportunity for improved inclusion and accessibility. In this series, NASEM will explore the opportunities related to increasing accessibility for field and laboratory research in the physical, social, and life sciences, including current barriers to full participation; the potential for accommodations including, but not limited to, assistive technologies, support, or modification of duties to increase accessibility and inclusion; commonalities and differences in solutions for different types of fieldwork and lab science; the role of mentoring and other support networks in improving inclusion; how one might scale up individual successes into more generalizable practices; and the institutional policies and cultural change needed to ensure sustainability. This project builds upon the knowledge base that has been acquired since the beginning of the pandemic, when laboratories were closed and field work delayed or cancelled. Many of these actions have had a side effect of allowing increased inclusion of people with disabilities in activities. However, without a commitment to sustained change, practices explored during the pandemic might not be disseminated, scaled up, or continued. The conversations will be planned by a volunteer committee that will engage a broad range of perspectives and experience in the physical, social, and life sciences and will include several scientists with disabilities. The conversations will be scheduled approximately monthly over a six-month period. The format(s) will be determined by the planning committee, and may include talks that present information, panelists that delve deeply into different aspects of the topics, time for broad discussions with audience members, and appropriate break times. The conversation series will be held on a virtual platform, and will be livestreamed, recorded, and captioned to broaden the audience reach.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2321684","CC* CIRA: Mid-TN AI for Interdisciplinary Imaging Interpretation Alliance","OAC","Campus Cyberinfrastructure","08/01/2023","07/18/2023","Daniel Moyer","TN","Vanderbilt University","Standard Grant","Kevin Thompson","07/31/2025","$200,000.00","Robert Keever, Paul Sheldon, Bennett Landman","daniel.moyer@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","CSE","808000","","$0.00","This project facilitates new regional collaborations in middle Tennessee centered on artificial intelligence (AI) for imaging. A primary goal of these planning and coordination project is to facilitate connections between research-focused institutions and minority serving institutions by hosting community events, training and tutorial workshops for students, and sharing computational resources for both pedagogy and research activities. Through the Mid-TN AI for Interdisciplinary Imaging Interpretation Alliance (AI4A), the project identifies, characterizes, and enables drivers of imaging AI science and the capacity for such science across the AI4A.<br/><br/>A critical component of this project is the development of human infrastructure and interconnections. AI4A hosts summer faculty fellowships to seed collaborations, and summer student opportunities to engage with pre-doctoral trainees in collaboration with existing REU programs such as the Fisk Bridge NSF REU. AI4A develops training materials for the technical skills required in AI-imaging, including GPU computing, deep learning concepts and practice, and current computing environments for high performance computing. AI4A also provides access to these devices and environments to support research collaborations and to provide practical experience for students in the development and validation of large models.  The project facilitates the emergence of de novo research directions, but based on the current research foci of participants, and based around imaging problems in radiology, neurology and neuroscience, cell microscopy, ecology and remote sensing, transportation, materials engineering, archeology, digital anthropology, and deep networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2334855","Collaborative Research: Conference: NSF Workshop Sustainable Computing for Sustainability","CCF","Information Technology Researc","09/01/2023","08/25/2023","Amy McGovern","OK","University of Oklahoma Norman Campus","Standard Grant","Anindya Banerjee","08/31/2024","$1,413.00","","amcgovern@ou.edu","660 PARRINGTON OVAL RM 301","NORMAN","OK","730193003","4053254757","CSE","164000","7556, 9102, 9150","$0.00","The motivations behind this project are two-fold.  It is driven by the growing role of computing as an essential tool in devising solutions to the many major societal problems created by climate change.  At the same time, it is an acknowledgment of the fact that, in spite of its positive role, computing is itself facing sustainability challenges as well as contributing to some of the factors behind climate change.  Fostering the development of new initiatives addressing those problems and challenges is the main goal of this project. The project?s novelties lie in jointly addressing the dual role of computing as an enabler of solutions to climate change as well as a contributor to climate change itself, and in exploring the challenges faced by the interdisciplinary teams needed to develop effective approaches.  The project?s impacts are similarly two-fold:  It will help prioritize new research directions where computing can have a transformative role in developing solutions to climate change challenges and it will increase awareness of the need for computing to account for sustainability in its own development.<br/><br/>Towards realizing the project?s goals, the investigators rely on a two-prong approach.  The project?s first phase consists of an open information gathering effort conducted virtually (online) towards identifying critical, yet poorly explored sustainability problems that computing can help address. The second phase seeks to develop a concrete agenda for not only addressing those challenges, but equally importantly help build and sustain the interdisciplinary scientific community that successfully tackling them requires.  This second phase is in the form of a workshop bringing together not only researchers across the required disciplines, but also researchers with expertise in building interdisciplinary teams and team science towards informing the creation and sustainability of successful teams involving participants across multiple disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322201","Research Infrastructure: CC* Data Storage: 20 Petabyte Campus Research Storage Facility at Johns Hopkins University","OAC","Campus Cyberinfrastructure","08/15/2023","08/07/2023","Charles Meneveau","MD","Johns Hopkins University","Standard Grant","Kevin Thompson","07/31/2025","$500,000.00","Dean Zarriello, Alexander Szalay","meneveau@jhu.edu","3400 N CHARLES ST","BALTIMORE","MD","212182608","4439971898","CSE","808000","","$0.00","There is a wide spectrum of ongoing research projects at Johns Hopkins University (JHU) that generate and analyze many petabytes of data. These include both large numerical simulations (turbulence, cosmology, ocean circulation models) and large microscopy data from a broad range of areas (cryoEM, fluorescent microscopy, material science projects, NG sequencing) and various large-scale data fusion projects from digital social science. The JHU Institute for Data Intensive Engineering and Science has been supporting these activities for the last decade by providing a platform to store and serve petabytes at high speeds and at excellent economies of scale across all disciplines at JHU and beyond. After a workshop held at JHU in the summer of 2022 the need for a new larger-scale, campus-wide research storage facility became clear.<br/><br/>This CC* project builds a 30PB data facility for Big Data projects, in a very cost-effective way, building on previous NSF investments. The system is based on upgrading the 90-node Data-Scope cluster whose Supermicro chassis and power supplies are standardized and compatible with the latest motherboards and CPUs. These nodes are upgraded at a low per node cost to a AMD Ryzen system and turned into a high-performance Ceph cluster with 30PB net storage capacity. The new storage serves a very wide range of scientific research, including offering many petabytes of open scientific data for global science including SDSS, turbulence, ocean circulation, and cosmology simulations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1909700","CHS: Small: Collaborative Research: Shared Mobility Systems to Address Transportation Barriers of Underserved Urban and Rural Communities","IIS","Information Technology Researc, HCC-Human-Centered Computing, IIS Special Projects","01/01/2020","06/07/2022","Patrick Shih","IN","Indiana University","Continuing Grant","Dan Cosley","12/31/2024","$297,768.00","","patshih@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","164000, 736700, 748400","1640, 7367, 7923, 9251, CL10","$0.00","This project will lead to a novel understanding of effective design strategies for shared mobility systems for underserved rural and urban communities.  Due to advances in networked information systems and ubiquitous connectivity, use of shared mobility systems is increasingly common. Underserved communities typically have the greatest needs for low-cost and high-quality shared transportation services. Yet, these communities experience financial, technical, skill-based and social barriers to their use. The study will result in a shared mobility system that leverages community strengths, addresses potential obstacles to use, appeals to community members, and uses a service model that is sustainable after the grant period.  Furthermore, the study will advance the design of timebanking systems by accounting for the unique properties of transportation as a service and of rural settings. This work could lead to a broader set of participants in timebanks, and an expansion of the timebanking model to include new approaches involving nonprofit and healthcare organizations.  Results can also inform policy efforts to promote transportation models that are equitable and inclusive for residents of underserved communities.<br/> <br/>This project involves collaborations among two academic universities and 14 representatives of non-profit organizations in two geographic areas, and it has three primary aims.  Aim 1 is to assess transportation needs and barriers and to generate participatory design ideas for a shared mobility system using the timebanking concept. Sessions will focus on: trust, reciprocity, design for skill development and access for people with limited computer skills and Internet access. Patients at partnering healthcare agencies and potential drivers will be interviewed, and participatory design sessions will be conducted with these groups and third-party intermediaries. Aim 2 is to develop and implement a shared mobility system for healthcare transportation; new features based on Aim 1 will be integrated into an existing timebank platform. This aim will involve a pre-pilot field study. Aim 3 is evaluation of the feasibility and preliminary impact of the system on the primary outcome of missed appointments at healthcare organizations. This will involve a four-month pilot study using an interrupted time series quasi-experimental design. Analyses will leverage electronic health record data, timebank platform server log data, user surveys, and observation at healthcare organizations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2324673","Collaborative Research: EAGER: A High Throughput Science Gateway for the Event Horizon Telescope","OAC","Campus Cyberinfrastructure","07/01/2023","05/04/2023","Chi-kwan Chan","AZ","University of Arizona","Standard Grant","Kevin Thompson","06/30/2024","$98,957.00","","chanc@arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","CSE","808000","7916","$0.00","The Event Horizon Telescope Science Gateway project is a collaborative effort to develop an accessible web-based portal for analyzing black hole images. Bringing together astrophysics researchers and students from the University of Arizona with technology designers, cyberinfrastructure experts, and user experience specialists from the Cyberinfrastructure Integration Research Center at the Indiana University?s Pervasive Technology Institute, the project aims to create an intuitive gateway that harnesses the computational power provided by the Partnership to Advance Throughput Computing and the Open Science Grid. The gateway is built on the Apache Airavata Science Gateway framework. The user-centric design of the portal allows both experienced and novice scientists to access essential analysis tools and benefit from millions of freely available computing cycles. Beyond providing a valuable compute environment and access to untapped resources for the astrophysics community, the project serves as a case study for using science gateways to facilitate access to high-throughput computing resources, demonstrating the effectiveness of integrating human-computer interaction design with science gateway technologies to simplify access to advanced computational resources.<br/><br/>Science gateways are effective tools for expanding access to scientific cyberinfrastructure by creating user environments customized for specific scientific communities. An emerging area of research in this field involves integrating science gateway technologies with human-computer interaction and user experience design methodologies. The Event Horizon Telescope community's utilization of the Open Science Grid exemplifies this approach. This project focuses on combining human-centered design principles with science gateway technologies to address key challenges in resource provisioning, meta-scheduling, data management, and complex computing workflows in high-throughput settings. The objective is to enhance the Event Horizon Telescope community's capacity to analyze extensive astronomical datasets by providing a more user-friendly and efficient gateway. This innovative approach introduces a new perspective to astrophysics and high-throughput computing, with the potential to drive transformative advancements in both fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2104104","Elements: Spatial Ecology Gateway","OAC","Capacity: Cyberinfrastructure, Data Cyberinfrastructure, Software Institutes","07/01/2021","05/21/2021","Robert Sinkovits","CA","University of California-San Diego","Standard Grant","Varun Chandola","06/30/2024","$600,000.00","James Sheppard, Jesse Lewis","rssinkovits@ucsd.edu","9500 GILMAN DR","LA JOLLA","CA","920930021","8585344896","CSE","168Y00, 772600, 800400","077Z, 7923","$0.00","Spatial ecology is the study of how landscape characteristics influence the distribution and movement of organisms within their environment. The Spatial Ecology Gateway (SEG) enables researchers, students and wildlife managers to upload biotelemetry data, typically GPS readings, and construct home ranges that allow them to interpret animal space use. Applications of the SEG can include classroom projects, basic research into problems in wildlife ecology, environmental impact studies and mitigation of adverse outcomes such as habitat fragmentation or increased human-wildlife interaction resulting from new development. The SEG insulates users from the underlying computational details so that they can focus on their science rather than mastering the technology. Users of the SEG have the option to generate two-dimensional (2D) or, where applicable, three-dimensional (3D) home ranges. Users are also provided with less computationally intensive tools to perform exploratory analyses.<br/><br/>The SEG is built using the HubZero platform, an open-source software platform for building websites that support scientific activities and leverages the Extreme Science and Engineering Discovery Environment (XSEDE) for more computationally demanding tasks. Data can be pulled directly into the SEG from the online community platform Movebank using their REST API. The SEG deploys the Brownian Bridge Movement Model (BBMM), Potential Path Volumes (PPV) and Continuous-Time Movement Modeling (ctmm) methods for home range construction. Tools for exploratory analysis of animal trajectories include net squared displacement, movement path tortuosity and relocation sampling rates versus deployment time. Users can apply basic filtering to their data sets to restrict analyses to specified time ranges or particular animals. The SEG insulates users from having to decide computational details such as choice of resource, number of compute cores or wall clock time. Rather, these decisions are made using logic built into the gateway and based on extensive benchmarking studies.<br/><br/>This award by the Office of Advanced Cyberinfrastructure is jointly supported by the Division of Biological Infrastructure within the NSF Biosciences Directorate, and by the Division of Information and Intelligent Systems within the NSF Computer and Information Science and Engineering Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2018373","CC* Networking Infrastructure: Creation of a Science DMZ and 10Gb/s Connection to Internet2 for Tennessee Tech University","OAC","Campus Cyberinfrastructure","07/01/2020","11/16/2023","Susmit Shannigrahi","TN","Tennessee Technological University","Standard Grant","Kevin Thompson","06/30/2024","$292,322.00","Michael Rogers","sshannigrahi@tntech.edu","1 WILLIAM L JONES DR","COOKEVILLE","TN","385050001","9313723374","CSE","808000","9251","$0.00","This project facilitates national and international research collaborations at Tennessee Technological University by establishing a dedicated 10 Gbps connection to Internet2 and creating a high-speed Science DMZ. <br/><br/>The new cyberinfrastructure benefits a large number of researchers across multiple colleges and research centers. It enables data-driven research in areas including next-generation Networking, Cybersecurity, High-performance Computing, Chemical Engineering, Biology, High-energy Physics, Earth Sciences, and Civil & Environmental Engineering. The upgraded connectivity enables researchers to share data both internally and externally in a fast, secure, and reliable manner over the dedicated research-only connection. The Science DMZ equips researchers with use-case specific segmented resources (network, disk space, and compute nodes) as well as increased autonomy and flexibility to deploy isolated research prototypes.<br/><br/>The new infrastructure bypasses Tennessee Tech's perimeter firewall while integrating with the existing infrastructure. High-speed data transfer nodes (DTNs) provides faster data delivery to and from the campus. The network design provides dedicated connectivity for research traffic to most buildings.  Several PerfSonar nodes deployed at strategic network locations report real-time network and application performance, allowing the operators to optimize network flows. Finally, this new infrastructure brings IPv6 deployment to the campus. A study of network and application performance before and after the deployment can demonstrate the impact of a science DMZ on science workflows in terms of performance, latency, and flexibility. The experience in implementing this project at Tennessee Tech can provide guidance to other universities pursuing similar cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2324672","Collaborative Research: EAGER: A High Throughput Science Gateway for the Event Horizon Telescope","OAC","Campus Cyberinfrastructure","07/01/2023","05/04/2023","Robert Quick","IN","Indiana University","Standard Grant","Kevin Thompson","06/30/2024","$201,041.00","","rquick@iu.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","CSE","808000","7916","$0.00","The Event Horizon Telescope Science Gateway project is a collaborative effort to develop an accessible web-based portal for analyzing black hole images. Bringing together astrophysics researchers and students from the University of Arizona with technology designers, cyberinfrastructure experts, and user experience specialists from the Cyberinfrastructure Integration Research Center at the Indiana University?s Pervasive Technology Institute, the project aims to create an intuitive gateway that harnesses the computational power provided by the Partnership to Advance Throughput Computing and the Open Science Grid. The gateway is built on the Apache Airavata Science Gateway framework. The user-centric design of the portal allows both experienced and novice scientists to access essential analysis tools and benefit from millions of freely available computing cycles. Beyond providing a valuable compute environment and access to untapped resources for the astrophysics community, the project serves as a case study for using science gateways to facilitate access to high-throughput computing resources, demonstrating the effectiveness of integrating human-computer interaction design with science gateway technologies to simplify access to advanced computational resources.<br/><br/>Science gateways are effective tools for expanding access to scientific cyberinfrastructure by creating user environments customized for specific scientific communities. An emerging area of research in this field involves integrating science gateway technologies with human-computer interaction and user experience design methodologies. The Event Horizon Telescope community's utilization of the Open Science Grid exemplifies this approach. This project focuses on combining human-centered design principles with science gateway technologies to address key challenges in resource provisioning, meta-scheduling, data management, and complex computing workflows in high-throughput settings. The objective is to enhance the Event Horizon Telescope community's capacity to analyze extensive astronomical datasets by providing a more user-friendly and efficient gateway. This innovative approach introduces a new perspective to astrophysics and high-throughput computing, with the potential to drive transformative advancements in both fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2212346","Collaborative Research: SHF: Medium: Automated energy-efficient sensor data winnowing using native analog processing","CCF","Information Technology Researc, Software & Hardware Foundation","10/01/2022","09/12/2023","Jiang Hu","TX","Texas A&M Engineering Experiment Station","Continuing Grant","Sankar Basu","09/30/2026","$220,420.00","","jianghu@ece.tamu.edu","3124 TAMU","COLLEGE STATION","TX","778433124","9798626777","CSE","164000, 779800","1640, 7924, 7945","$0.00","As computing becomes pervasive in all aspects of daily life, computing hardware must allow for increasing interaction with the physical world. This interaction takes the form of sensed signals that are analog in nature, e.g., a sensor may output a voltage that can take on a continuous range of values. Traditional mainstream computing digitizes this data, converting it to digital 0s and 1s for efficient analysis and processing. However, as the amount of sensed analog data is growing exponentially, digital processors will be faced with a data deluge from external sensors.  For these vast volumes of data, even the cost of converting analog input data to digital signals, prior to any processing, can be prohibitively expensive.  Native analog processing (NAP) negates the need for analog-to-digital conversion by working in the analog domain. NAP can be used to implement data processing functions inexpensively, but can achieve only limited accuracy; on the other hand, digital processing can achieve high accuracy, but requires the overhead of analog-to-digital conversion. This project presents a methodology for mixed-signal processing that hybridizes digital and analog subcircuit implementations to achieve the best of both worlds. The effort intends to actively engage with the semiconductor industry, and will train graduate and undergraduate students in the area of semiconductor design, thus alleviating the national skills shortage in this area.<br/><br/>In the first step, computing tasks are automatically partitioned into hybrid analog/digital segments, with the goal of meeting system-level constraints on throughput, power, and noise/error. The computations associated with a task are abstractly represented by a dataflow graph (DFG). This representation is widely used to model a variety of tasks, including those commonly used in digital signal processing and machine learning. The nodes in the DFG are mapped to analog or digital implementations, using cost functions that represent the cost of implementation, as well as the cost of any required analog-to-digital or digital-to-analog conversion. Next, the analog and digital circuitry is optimized to build a silicon implementation at the layout level, based on cutting-edge transistor technologies, which involve restrictive design rules that impose limitations such as unidirectional routing and gridded layout. The optimizations are facilitated by novel techniques for back-end analysis, synthesis, and implementation developed in this project, including transistor and interconnect optimizations, placement and routing techniques that are specifically targeted to mixed-signal designs, and compact performance machine-learning-based model generation that efficiently predicts circuit performance. The project thus automatically translates the system-level DFG specification of a computing task to an optimized mixed-signal silicon implementation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2126248","CC* Regional: LEARN Extending & Accelerating Participation in Science (Texas LEAPS)","OAC","Campus Cyberinfrastructure","10/01/2021","09/25/2023","Akbar Kara","TX","LEARN: Lonestar Education and Research Network","Standard Grant","Kevin Thompson","09/30/2024","$890,486.00","Akbar Kara, Amy Schultz, Lonie Packer, catherine howard","ak@zettafiber.org","3726 20TH ST","LUBBOCK","TX","794101208","8067437878","CSE","808000","102Z, 9102","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).<br/><br/>Higher education institutions at all levels rely on Cyberinfrastructure (CI) to provide environments for teaching and learning. For smaller institutions, having access to advanced CI can enrich the opportunities for teaching and learning, while presenting new opportunities for programs that rely on external information and service resources. The counterpoint is that sophisticated technology and the expertise to manage that technology are challenges for small colleges; however, these challenges in part can be met by state, regional and national organizations specializing in these technologies that can assist small colleges to access state-of-the-art technology and expertise.   <br/><br/>Lonestar Education and Research Network (LEARN) is further developing and expanding its model program (LEARN Smaller Institution CI Program) for regional network connectivity for small campuses in Texas.  A set of three campuses ? Texarkana College, Trinity Valley Community College, and Texas Lutheran University -  are taking part in this project to receive:  <br/><br/> ? Advanced network services and gigabit connectivity,  <br/> ? Expertise to manage these technologies, <br/> ? Training and assistance to effectively adopt and use the technologies, and <br/> ? A community of similarly engaged scholars and administrators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925687","CC* Team:  KyRC - A Kentucky Research Computing Team","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/15/2019","10/15/2020","Brian Nichols","KY","University of Kentucky Research Foundation","Continuing Grant","Kevin Thompson","06/30/2024","$1,399,638.00","Doyle Friskney, James Griffioen","bnichols@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","CSE","723100, 808000","9150","$0.00","High performance computing centers now serve a broad set of user interests with researchers from all disciplines now leveraging computation and/or big data in interesting and novel ways. This change is not confined to top tier universities, but rather impacts researchers at all institutions of higher learning.  In short, the need to support researchers who span all academic areas and require a diverse set of cyberinfrastructure (CI) has become a key challenge for research centers and IT organizations nationally. Today, scientific discovery is enabled through compute-intensive and data-intensive research that would not be possible without advanced CI.  This project forms a collaborative, state-wide support team, the Kentucky Research Computing team (KyRC), that provides researchers with direct access to expert CI engineers to assist in applying, consulting, and supporting a wide-range of computational systems and research disciplines.  <br/><br/>The goal of KyRC is to serve a broad range of institutions across the state of Kentucky in higher education, smart cities, and community education and training efforts. KyRC provides support to a variety of emerging research areas in need of CI support that heretofore were not part of the computational community. In addition to state-wide support, KyRC collaborates with leading regional and national CI entities to facilitate a community of expertise. By providing access, education, and exposure to advanced CI, KyRC enables research programs to recruit more students (e.g., groups underrepresented in STEM) to computational research while enhancing the training of undergraduates, graduate students, and postdocs in Kentucky higher education.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2019135","CC* Team: Texas Education and Research Cybertraining Center (TERCC)","OAC","CYBERINFRASTRUCTURE, Campus Cyberinfrastructure","07/01/2020","08/24/2023","Frank Feagans","TX","University of Texas at Dallas","Continuing Grant","Kevin Thompson","06/30/2024","$1,399,502.00","Christopher Simmons, Amy Schultz, Akbar Kara, Jeffery Neyland, Frank Feagans, Edward Gonzales, Kendra Ketchum","Frank.Feagans@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","CSE","723100, 808000","","$0.00","The Texas Research and Education Cyberinfrastructure Services (TRECIS) center establishes a regional hub that advances collaborative support for research computing and expands the use of advanced cyberinfrastructure (CI) and expertise throughout the University of Texas system. It effectively integrates previously siloed CI and computing support across three UT campuses and the Lonestar Education and Research Network (LEARN) and builds a unique model of embedded<br/>collaborative facilitation with scientific projects. TRECIS trains postdoctoral researchers with domain-specific expertise in high performance computing and other CI-related services who are then available for consultations and support. TRECIS creates new career paths in research facilitation, broadens CI access for research and education and engages students and underrepresented groups, including historically black colleges, other minority institutions, and<br/>organizations that promote gender equality in STEM.<br/><br/>TRECIS builds upon the model of shared CI and support developed at UT Dallas, extends fast connectivity and networking to other campuses, provides outreach and develops a model of research facilitation through training and collaboration. It initially engages seven scientific projects from the fields of biomedicine, chemistry, geosciences, and others. Most are multi-institution collaborations and have multiple CI needs, including parallel / distributed computing, data modeling, and storage and transfer of data. TRECIS helps these projects and other users of research computing in allocating CI resources and developing effective workflows, algorithms and pipelines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2233738","EAGER: Curating and representing mental health data to support therapists in personalized care","IIS","Information Technology Researc, HCC-Human-Centered Computing, IIS Special Projects","10/01/2022","04/17/2023","Elena Agapie","CA","University of California-Irvine","Standard Grant","Dan Cosley","12/31/2024","$188,498.00","","eagapie@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","CSE","164000, 736700, 748400","7367, 7916, CL10","$0.00","This research addresses the growing need to support mental health therapists in managing their workload and client support through technology. Every year, 20% of the US adult population experiences depression and anxiety, these numbers increased during COVID-19 to over 40% of the population. As practices of mental health providers have moved online during COVID-19 through telehealth, providers took on more care responsibilities related to clients. A key aspect of mental health treatment is managing care outside of therapy, which is achieved through personalized care plans. This relies on the therapist developing deep understanding of the client?s personal data and context, which is cognitively challenging, and difficult to achieve due to limited tools to support therapists? data gathering. The proposed research investigates the role of visual curation in supporting reflection and tailoring of care plans. This research contributes novel representations of care plans and client data for supporting reflection. It investigates how visual curation and synthesis, data organization, and annotation can support therapists in representing subjective goals, behaviors, and thoughts over time, to gain insights and identify future plans of action. The research has the potential to make clinical care more effective by supporting therapists in better understanding their clients and managing client care, providing more tailored care, and building stronger relationships with clients. This is expected to help therapists better manage their caseloads, maintain client engagement, and provide successful care. By supporting therapists, it has the potential to enable citizens at large to receive better mental health care, addressing the immediate mental health crisis and the nation?s long-term needs.<br/><br/>The exploratory research grant will contribute to the fields of human-computer interaction and psychology by investigating new techniques for how technology can facilitate therapists in capturing, representing, and comprehending evolving, multifaceted, and complex goals and experiences related to mental health. The research involves two phases to develop and evaluate approaches for personalizing care through reflection on visual data. In Phase 1, the research team establishes visual representations beneficial for reflection and future planning in mental health. This phase outlines data representation techniques for mental health data and establishes design principles for visual data curation of multifaceted, subjective, and difficult to quantify data. In Phase 2, the research team develops and evaluates a prototype for visual curation and representation of longitudinal mental health data to support reflection. This phase establishes initial contributions of approaching therapy data curation using visual representations of mental health data, which include longitudinal information about client multiple goals, behaviors, thoughts, and progress, to support clients and therapists in reflective activities, and inform personalization of mental health care plans. This exploratory research contributes design guidelines and frameworks  addressing how visual forms can be used to represent psychological and environmental client data and support mental health therapists reflecting on the data while providing personalized care.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209726","Elements: Making Ice Penetrating Radar More Accessible: A tool for finding, downloading and visualizing georeferenced radargrams within the QGIS ecosystem","OAC","Polar Cyberinfrastructure, Data Cyberinfrastructure, Software Institutes, EarthCube","07/15/2022","07/08/2022","Laura Lindzey","WA","University of Washington","Standard Grant","Marlon Pierce","06/30/2025","$330,713.00","","lindzey@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","CSE","540700, 772600, 800400, 807400","077Z, 1094, 4444, 5294, 8004, 9102","$0.00","Ice penetrating radar is one of the primary tools that researchers use to study ice sheets and glaciers. With radar, it is possible to see a cross-section of the ice, revealing internal layers and the shape of the rocks under the ice.  Among other things, this is important for calculating how much potential sea level change is locked up in the polar ice sheets, and how stable the ice sheets are likely to be in a warming world. This type of data is logistically challenging and expensive to collect. Historically, individual research groups have obtained funding to collect these data sets, and then the data largely stayed within that institution. There has been a recent push to make more and more data openly available, enabling the same datasets to be used by multiple research groups. However, it is still difficult to figure out what data is available because there is no centralized index. Additionally, each group releases data in a different format, which creates an additional hurdle to its use. This project addresses both of those challenges to data reuse by providing a unified tool for discovering where ice penetrating radar data already exists, then allowing the researcher to download and visualize the data. It is integrated into open-source mapping software that many in the research community already use, and makes it possible for non-experts to explore these datasets. This is particularly valuable for early-career researchers and for enabling interdisciplinary work.<br/><br/><br/>The US alone has spent many tens of millions of dollars on direct grants to enable the acquisition and analysis of polar ice penetrating radar data, and even more on the associated infrastructure and support costs. Unfortunately, much of these data is not publicly released, and even the data that has been released is not easily accessible. There is significant technical work involved in figuring out how to locate, download and view the data. This project is developing a tool that will both lower the barrier to entry for using this data and improve the workflows of existing users. <br/>Quantarctica and QGreenland have rapidly become indispensable tools for the polar research community, making diverse data sets readily available to researchers. However, ice penetrating radar is a major category of data that is not currently supported ? it is possible to see the locations of existing survey lines, and the ice thickness maps that have been interpreted from their data, but it is not readily possible to see the radargrams themselves in context with all of the other information. This capability is important because there is far more visual information contained in a radargram than simply its interpreted basal elevation or ice thickness. This project is developing software that will enable researchers to to view radargram images and interpreted surface and basal horizons in context with the existing map-view datasets in Quantarctica and QGreenland. A data layer shows the locations of all known ice penetrating radar surveys, color-coded based on availability. This layer enables data discovery and browsing. The plugin itself interacts with the data layer, first to download selected data, then to visualize the radargrams along with a cursor that moves simultaneously along the radargram and along the map view, making it straightforward to determine the precise geolocation of radar features.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2107135","Collaborative Research: SHF: Medium: Co-Optimizing Computation and Data Transformations for Sparse Tensors","CCF","Information Technology Researc, Software & Hardware Foundation","01/01/2022","01/22/2024","Catherine Olschanowsky","ID","Boise State University","Continuing Grant","Almadena Chtchelkanova","12/31/2025","$329,963.00","","cathie@cs.boisestate.edu","1910 UNIVERSITY DR","BOISE","ID","837250001","2084261574","CSE","164000, 779800","7924, 7942, 9102, 9150","$0.00","Sparse tensor computations are central to important applications including computer-assisted drug design, fraud detection, and national security. Timely execution of these applications improves user productivity and reduces the energy consumption associated with each execution. Sparse computations are characterized as having inputs where many or most values are zero.  To avoid the inefficiency of storing and computing on zero-valued data, applications only store the nonzeros, with auxiliary data structures to recover their locations.  As a result, sparse tensor computations exhibit unpredictable memory-access patterns that include indirection through the auxiliary data structures.  Consequently, on today?s computer architectures, performance of sparse tensor computations is completely dominated by the movement of data, through the memory system and across nodes. Data movement is expensive both in terms of execution time and energy expenditure. Optimizing data movement of sparse tensor computations as high-performance architectures have become increasingly diverse ? conventional parallel architectures, graphics processors used as parallel accelerators and complex memory systems ? creates a performance and productivity challenge for software developers who end up writing low-level architecture-specific code for each platform. The proposed approach simultaneously optimizes how data is organized in memory, how the computation is structured to access the data in a way that reduces data movement, and how the computation and data movement make best use of features of the hardware architectures. Since the nonzero structure of the data is unknown until program execution, the approach also examines runtime information in its decisions.  The resulting co-optimization strategy enables a cohesive approach for iteratively making scheduling and data representation transformation decisions for a wide range of sparse computations and incorporating runtime adaptations.<br/><br/>This project is developing a programming framework that permits high-level specification of a sparse computation and optimizes it to reduce data movement.  It composes data representations, data layouts and storage mappings, and parallel schedules for sparse computations.  It employs data dependencies, runtime information, and architecture features to fully bind the final generated code. This approach is intended to enable handling sparse tensor computations with dependences such as sparse triangular solve and many other solvers for systems of linear equations, applying reorderings such as Morton ordering on sparse tensors, and late binding of sparse tensor data representations.  The novel and most significant aspects of the research include: (1) composable schedule and data transformations, including data layout transformations and storage mapping; (2) inspector synthesis for runtime data transformations between data representations, layouts, and storage mappings, which are composed with external functions; (3) support for data-dependent tensor computations; and, (4) framework abstractions deployed in the MLIR/LLVM compiler.<br/><br/>The researchers are strongly committed to broadening participation in computing and have comprehensive plans to engage the underrepresented groups.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209768","Collaborative Research: Elements: Towards A Scalable Infrastructure for Archival and Reproducible Scientific Visualizations","IIS","OAC-Advanced Cyberinfrast Core, Data Cyberinfrastructure","09/15/2022","09/06/2022","Roberto Sisneros","IL","University of Illinois at Urbana-Champaign","Standard Grant","Hector Munoz-Avila","08/31/2025","$192,864.00","","sisneros@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","090Y00, 772600","077Z, 8004","$0.00","Today?s science revolves around leading edge datasets ? data that scientists need to carefully analyze so that they can draw reliable scientific conclusions. The rate at which these leading-edge datasets are becoming larger and more complex is accelerating every day. In many ways, having access to a dataset does not equal to, or even come close to, having access to the insights in the dataset. This nuanced but crucial difference in accessibility creates a deep barrier to making scientific results reproducible. To this end, ?Accessible Reproducible Research?, published by Science in 2010, presented a system for reproducible research.  A decade later, unfortunately, accessible reproducible research is still in its infancy. It turns out that this barrier is much more fundamental than previously believed, even though on the surface it seems solvable by investing resources and setting guidelines and policies. The real challenge is that the computing toolsets, the working environments, and the work processes of the original team of scientists are very difficult for a different team of scientists to recreate with precision. Such difficulty stems from the rapid speed at which computing technology is advancing; so that freezing a computing environment in a practical manner is nearly impossible. In addition, scientific intuition is difficult to codify, simply documenting a new idea is not enough to communicate what a scientist saw before pursuing that idea. From that respect, making accessible reproducible research a reality requires better methods and tools. In this project, the investigators will focus on the visualization step of data analysis, which is a central component of scientific discovery. This project?s aim is to develop an Archiving Infrastructure for Reproducible Interactive Visualization (AIRIV). Through this infrastructure, the investigators will demonstrate how visual explorations of large and complex data can be reliably captured, efficiently stored, easily shared, and freely reused by any user. This project will improve accessibility of reproducible research and promote the progress of science. For areas such as medicine and pharmaceutical research, this project will provide an unprecedented channel to accelerate translational research and advance the national health.<br/><br/>This project will build upon research funded by a prior NSF CISE Research Infrastructure award. In that previous project, the investigators found a method to capture interactive user experience of visualization tools, and to share the captured experience without the need to share the original software or the original data. Furthermore, during the reuse of a captured experience, the user has freedom to explore beyond the exact sequence of how the previous user has used the tool with a method called Loom. In this new project to create AIRIV, the investigators will focus on web-based visualization dashboards, which represent the standard way for scientists around the world to interact with their data and derive insights. This project will first build a general AIRIV Javascript library that can be imported by any web browser-based application. Using the AIRIV library, developers of web-based visual dashboards can easily implement automatic generation of Loom objects into their dashboards. Developers will be able to instrument their applications to store new provenance information with Loom objects as well. The investigators will then conduct performance and scaling tests to understand the tradeoffs between hosting choices under settings of local, institutional clusters, and community shared data infrastructures. Operators of scientific facilities can use the findings to help science communities make informed choices as to where and how to host scientific visualization archives for better share-ability and cost efficiency. The investigators will also develop machine learning methods that can compare Loom objects and externalize commonalities and patterns in an entire archive of Loom objects. Such new methods will lead to creating a search by example functionality for AIRIV archives. For requirements collection, continuous improvement, and deployment testing, the investigators will engage the Mayo Clinic & Illinois Alliance, which serves as a framework for several technologies in healthcare, many of which center around the research and development of dashboard/analytical tools. We target two such analytics efforts, OmiX and KnowEnG, both of which are developed at National Center for Supercomputing Applications (NCSA).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2105169","CRII: HCC: Temporal dynamics of team emotion and cognition in AI-supported decision-making","IIS","Information Technology Researc, HCC-Human-Centered Computing, IIS Special Projects","10/01/2021","06/05/2023","Bei Yan","NJ","Stevens Institute of Technology","Standard Grant","Dan Cosley","09/30/2024","$220,734.00","","byan7@stevens.edu","1 CASTLEPOINT ON HUDSON","HOBOKEN","NJ","07030","2012168762","CSE","164000, 736700, 748400","102Z, 7367, 8228, CL10","$0.00","Artificial intelligence (AI) is becoming increasingly present and powerful in systems that affect people?s lives, helping people make decisions in contexts ranging from market decisions, military task support, and medical diagnosis. It is thus essential to understand how adding AI aspects to the tools teams use influences team collaboration, and its implications for organizational practices. This project will address those questions through studying how affective and cognitive processes in teams with AI support intertwine to shape team decision-making performance over time. Insights produced by this research will improve existing knowledge about how AI can be applied to facilitate human collaboration. It will help organizations in various industries make informed decisions about whether and when to adopt AI tools, and develop training and interventions to improve AI-supported team decision-making. The research will also help AI designers better understand AI?s impact on team decision-making and to design more effective AI tools to support collaboration.   <br/><br/>Bridging research perspectives and practices from human-computer interaction (HCI) and team research, this project seeks to develop a human-centered theory on how AI may shape team decision-making processes and outcomes through a set of lab experiments. In the experiments, teams will perform a series of decision-making tasks with AI assistance. The first experiment will vary the communication features of an AI system that provides information and ideas to a team decision-making task, examining how both the presence and the communication style of the AI system affects team interaction processes and decision-making outcomes. The second experiment will vary when AI assistance is employed in different team collaboration stages ? for example, whether AI assistance was introduced at the beginning of the team task or in the middle of the team task. The third experiment will investigate how team turnover during a decision-making task interacts with the use of AI-based tools. Team information sharing, affective status, learning, and performance during the tasks will be observed and analyzed using computational methods. Understanding these processes will contribute to theory on AI-supported team decision-making and in particular, the emergence of team emotion, cognition, and their inter-relations under different conditions of using AI-based tools. The analytical approach can help develop unobtrusive measures of team emotion and cognition and create new research paradigms, measures, and materials (such as task designs and customized chatbots) that can be used by the broader scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1835834","Collaborative Research: CSSI: Framework: Data: Clowder Open Source Customizable Research Data Management, Plus-Plus","OAC","Data Cyberinfrastructure","09/01/2018","08/09/2018","Kenton McHenry","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alejandro Suarez","08/31/2024","$3,752,045.00","Praveen Kumar, Klara Nahrstedt","kmchenry@ncsa.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","772600","062Z, 077Z, 7925","$0.00","Preserving, sharing, navigating, and reusing large and diverse collections of data is now essential to scientific discoveries in areas such as phenomics, materials science, geoscience, and urban science. These data navigation needs are also important when addressing the growing number of research areas where data and tools must span multiple domains. To support these needs effectively, new methods are required that simplify and reduce the amount of effort needed by researchers to find and utilize data, support community accepted data practices, and bring together the breadth of standards, tools, and resources utilized by a community. Clowder, an active curation based data management system, addresses these needs and challenges by distributing much of the data curation overhead throughout the lifecycle of the data, augmenting this with social curation and automated analysis tools, and providing extensible community-dependent means of viewing and navigating data. As an open source framework, built to be extensible at every level, Clowder is capable of interacting with and utilizing a variety of community tools while also supporting different data governance and ownership requirements.<br/><br/>The project enhances Clowder's core systems for the benefit of a larger group of users. It increases the level of interoperability with community resources, hardens the core software, and distributes core software development, while continuing to expand usage.  Governance mechanisms and a business model are established to make Clowder sustainable, creating an appropriate governance structure to ensure that the software continues to be available, supportable, and usable.  The effort engages a number of stakeholders, taking data from diverse but converging scientific domains already using the Clowder framework, to address broad interoperability and cross domain data sharing. The overall effort will transition the grassroots Clowder user community and Clowder's other stakeholders (such as current and potential developers) into a larger organized community, with a sustainable software resource supporting convergent research data needs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1856641","III: Large: Collaborative Research: Analysis Engineering for Robust End-to-End Data Science","IIS","Information Technology Researc, Special Projects - CCF, Info Integration & Informatics","10/01/2019","07/16/2020","Brad Myers","PA","Carnegie-Mellon University","Continuing Grant","Wendy Nilsen","09/30/2024","$728,500.00","","bam@cs.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","CSE","164000, 287800, 736400","026Z, 077Z, 7364, 7925, 9251","$0.00","From poor statistical practices leading to retractions of scientific ""discoveries"" to low-level spreadsheet errors subverting high-stakes analyses, failures of data analysis can have catastrophic consequences. The rapid growth of data science practice in the last decade has led to large collaborative efforts to develop new data processing, machine learning, and analytics tools that put more advanced data analysis into the hands of a wider audience of practitioners, from students to scientists to designers. The most dominant tool for data science is code, where cutting-edge algorithms can be applied from an existing libraries. However, as this democratization of data science has lowered the barrier to using advanced methods, safely using these tools under sound statistical practice remains as difficult as ever. To facilitate more robust data science, this project investigates models and tools for analysis engineering by data scientists who write programs. The focus is on the complete end-to-end process of data analysis performed with code: the iterative, and often exploratory, steps that analysts go through to turn data into This project will contribute insights and characterizations of analytic work, novel methods for capturing and analyzing data science activities, and develop new programming tools and visualization methods for authoring and validating analyses. If successful, this project will augment people's ability to conduct and assess data analyses, promoting more robust results and reducing the gap between novice and expert analysts. The findings and tools from the project will be incorporated into educational efforts, including classroom teaching and tutorials and available as open source software integrated into popular analytical environments (e.g., Jupyter).<br/><br/>Data analysis is a central activity to scientific research, yet is too often conducted in an undisciplined fashion. This project treats the entire analytic process as our central phenomenon of study. The project will employ mixed methods to study and characterize common analysis practices and pitfalls, including direct observations of data analysts, large-scale analysis of computational notebooks, and instrumentation of analytic programming environments like JupyterLab. The project will contribute new methods for specifying and safeguarding analyses, including domain-specific languages and program synthesis methods to guide users to preferred next steps. It will also explore ""multiverse"" workflows to manage and assess a diversity of analysis decisions. Analogues of debugging and testing tools will be developed to flag problems and perform error analysis, while the capture and visualization of analytic provenance to aid reproducibility, verification, and collaborative review. The work will be evaluated through controlled studies, classroom use, and open-source deployment for wide-scale field use.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2334853","Collaborative Research: Conference: NSF Workshop Sustainable Computing for Sustainability","CCF","Information Technology Researc","09/01/2023","08/25/2023","Roch Guerin","MO","Washington University","Standard Grant","Damian Dechev","08/31/2024","$185,078.00","","guerin@wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","63110","3147474134","CSE","164000","7556","$0.00","The motivations behind this project are two-fold.  It is driven by the growing role of computing as an essential tool in devising solutions to the many major societal problems created by climate change.  At the same time, it is an acknowledgment of the fact that, in spite of its positive role, computing is itself facing sustainability challenges as well as contributing to some of the factors behind climate change.  Fostering the development of new initiatives addressing those problems and challenges is the main goal of this project. The project?s novelties lie in jointly addressing the dual role of computing as an enabler of solutions to climate change as well as a contributor to climate change itself, and in exploring the challenges faced by the interdisciplinary teams needed to develop effective approaches.  The project?s impacts are similarly two-fold:  It will help prioritize new research directions where computing can have a transformative role in developing solutions to climate change challenges and it will increase awareness of the need for computing to account for sustainability in its own development.<br/><br/>Towards realizing the project?s goals, the investigators rely on a two-prong approach.  The project?s first phase consists of an open information gathering effort conducted virtually (online) towards identifying critical, yet poorly explored sustainability problems that computing can help address. The second phase seeks to develop a concrete agenda for not only addressing those challenges, but equally importantly help build and sustain the interdisciplinary scientific community that successfully tackling them requires.  This second phase is in the form of a workshop bringing together not only researchers across the required disciplines, but also researchers with expertise in building interdisciplinary teams and team science towards informing the creation and sustainability of successful teams involving participants across multiple disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2334854","Collaborative Research: Conference: NSF Workshop Sustainable Computing for Sustainability","CCF","Information Technology Researc","09/01/2023","08/25/2023","Klara Nahrstedt","IL","University of Illinois at Urbana-Champaign","Standard Grant","Anindya Banerjee","08/31/2024","$1,753.00","","klara@cs.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","CSE","164000","7556, 9102","$0.00","The motivations behind this project are two-fold.  It is driven by the growing role of computing as an essential tool in devising solutions to the many major societal problems created by climate change.  At the same time, it is an acknowledgment of the fact that, in spite of its positive role, computing is itself facing sustainability challenges as well as contributing to some of the factors behind climate change.  Fostering the development of new initiatives addressing those problems and challenges is the main goal of this project. The project?s novelties lie in jointly addressing the dual role of computing as an enabler of solutions to climate change as well as a contributor to climate change itself, and in exploring the challenges faced by the interdisciplinary teams needed to develop effective approaches.  The project?s impacts are similarly two-fold:  It will help prioritize new research directions where computing can have a transformative role in developing solutions to climate change challenges and it will increase awareness of the need for computing to account for sustainability in its own development.<br/><br/>Towards realizing the project?s goals, the investigators rely on a two-prong approach.  The project?s first phase consists of an open information gathering effort conducted virtually (online) towards identifying critical, yet poorly explored sustainability problems that computing can help address. The second phase seeks to develop a concrete agenda for not only addressing those challenges, but equally importantly help build and sustain the interdisciplinary scientific community that successfully tackling them requires.  This second phase is in the form of a workshop bringing together not only researchers across the required disciplines, but also researchers with expertise in building interdisciplinary teams and team science towards informing the creation and sustainability of successful teams involving participants across multiple disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2322420","CC* Data Storage: Closing Caltech's data storage gap: from ad-hoc to well-managed stewardship of large-scale datasets","OAC","Campus Cyberinfrastructure","08/01/2023","06/28/2023","Thomas Morrell","CA","California Institute of Technology","Standard Grant","Kevin Thompson","07/31/2025","$500,000.00","Jin Chang, Kara Whatley","tmorrell@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","CSE","808000","","$0.00","Caltech Library and the campus IT group, IMSS (Information Management Systems and Services) are collaborating to build research data storage and sharing infrastructure for a sustainable future. Two Open Storage Network (OSN) pods provide 2.2 TB of long-term storage for both Caltech researchers and the wider research community through the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program. OSN storage enables the existing CaltechDATA institutional repository to share larger files and empowers Caltech researchers to generate Findable, Accessible, Interoperable, and Reusable (FAIR) data. 15 campus research groups are committed to openly sharing over 600 TB of data, spanning a wide range of research areas including biological sequencing and imaging, astronomy, geosciences, and engineering.<br/><br/>This rich data resource broadens participation in science by providing global access to raw data from cutting-edge microscopes, telescopes, and supercomputers that are currently limited to Caltech researchers. Training-based workforce development components include OSN instruction through basic cyberinfrastructure/programming workshops, in-class instruction, and training sessions that are open to all students, postdocs, faculty, staff, and collaborators. Support for a diverse group of student workers will engage them in the management of CaltechDATA and OSN storage. The open-source storage architecture serves as a model that other universities can deploy through open documentation. The combination of OSN and an open-source data repository such as InvenioRDM serves as a model of using open-source tools for institutional research data management and broadening participation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2321565","Equipment: CC Campus Compute: Expansion of GPU Compute Capacity for NC State University HPC to  Support Research and Education","OAC","Campus Cyberinfrastructure","08/01/2023","06/05/2023","Andrew Petersen","NC","North Carolina State University","Standard Grant","Amy Apon","07/31/2025","$467,607.00","Eric Sills, Marc Hoit","aapeters@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","276950001","9195152444","CSE","808000","","$0.00","This project expands advanced computing capacity at North Carolina State University (NC State). The resources enable larger models, faster simulations, larger datasets for more accurate Artificial Intelligence models, and faster processing of genome assemblies. The resource is deployed into a supportive environment - the High Performance Computing (HPC) facility - which provides outreach, training, documentation, tutorials, support, and tools for access, visualization, job execution, and an onboarding process for beginning users.<br/><br/>The project installs six Lenovo SR670 nodes with the latest NVIDA Hopper architecture (with 12x H100 and 12x L40 NVIDIA GPUs) so that teaching, classroom use, and research (including research from the science drivers) can expand, relieving the compute congestion on the current GPU cards. More than twelve science applications will be supported on the system. Applications include ab-initio methods such as Density Functional Theory and Molecular Dynamics, neural network training, cryo-EM, genome assembly, and fluid simulation. More than 200 projects are supported by the HPC facility. Access to this system will be provided for all students, including undergraduates. Educational impacts include access to the system for teaching, classroom use, and research capability for HPC users. A significant portion of the resource will be made available to Open Science Grid, providing increased computing capacity for scientific users nationwide.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
